{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature rich dataset ``data/dataset.pkl`` out of corenlp annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://s3.amazonaws.com/dl4j-distribution/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                       binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from iteration_utilities import unique_everseen\n",
    "\n",
    "\n",
    "def _extract_plain_features(sentence):\n",
    "    \n",
    "    # postag sequences occuring in dataset more than 20 times\n",
    "    cols_postag_subj = ['VBN~IN~NNP~IN~CD', 'VBN~IN~NNP', 'VBN~IN~CD', 'NN~NN~NN~VBN~IN~NNP', 'NNP', 'NNP~NNP~NNP~NNP', 'NN', 'JJ~NN~NN', 'JJ~CD', 'NN~CD', 'NNP~NNPS', 'NNP~NNP~NNS', 'JJ~NNP~NNP', 'RB~VBN', 'PRP', 'CD', 'NNP~NNP~IN~NNP~NNP', 'JJ~NN', 'NN~IN~NN~NN', 'CD~NNS', 'NN~IN~NNP', 'NN~NN', 'NN~NN~NN', 'JJ~NN~NN~NN', 'JJ~NN~VBN~IN~NNP', 'NN~VBN~IN~NNP', 'NNP~CD', 'NN~NN~VBN~IN~NN~NNP', 'NN~NN~VBN~IN~JJ~NN~NNP', 'NNP~NNP~NNP', 'JJ~NNS', 'NNP~NNP', 'NN~NN~CD', 'NNP~NN', 'NN~IN~CD', 'JJ~NN~NN~VBN~IN~CD', 'NN~NN~VBN~IN~CD', 'CD~NNP~CD', 'NN~NN~VBN~IN~NNP~NNP~NNP', 'NN~IN~CD~NNS', 'NNP~NNP~NN~NN', 'JJ', 'NNP~POS~NN', 'PRP$~NN~NN', 'JJ~NN~NN~NN~VBN', 'NN~NN~NN~VBN', 'NNP~NN~NN', 'JJ~NNP', 'NNP~NNP~IN~NNP', 'PRP$~JJ~NN', 'PRP$~NN', 'VBN~IN~CD~IN~NNP~NNP', 'JJ~JJ', 'PRP$~CD~NN', 'NNP~NN~VBN', 'NN~IN~NNP~NNPS', 'NN~IN~NNP~NNP', 'CD~CD~NNS', 'JJ~NN~IN~NN', 'NN~NN~NN~CD', 'RB~NN', 'VBN~IN~NNP~NNP~IN~CD', 'CD~NN', 'NN~IN~NNP~NNP~NNP', 'JJ~NN~VBN~IN~NN', 'JJ~NN~VBN', 'JJ~JJ~NN', 'JJ~JJ~NN~VBN', 'PRP$~NNS', 'CD~NNS~IN~NN', 'CD~JJ~NNS', 'NN~VBN~IN~NNP~NNP~NNP', 'JJ~NN~IN~NNP~NNP', 'NN~NNP~NNP', 'JJ~NN~NNP~NNP', 'NNS', 'NN~NN~NN~NN', 'NN~NN~IN~NN', 'NNP~IN~NNP', 'NNP~NNS', 'JJ~JJ~NN~NN', 'CD~NN~IN~NN', 'NN~CD~CD', 'NN~NN~VBN~IN~NNP', 'JJ~NN~NN~VBN~IN~NNP', 'NNP~NNP~NN', 'JJ~NN~NN~NN~CD', 'NNP~CD~NN', 'JJ~NN~NN~NN~VBN~IN~NNP', 'JJ~JJ~NN~VBN~IN~NNP', 'JJ~NN~NNS', 'NN~VBN~IN~NNP~NNP', 'JJ~NN~VBN~IN~NNP~NNP', 'RB~NNP', 'NNP~NNP~CD', 'NNS~IN~NNP', 'NN~NN~VBN', 'CD~NNS~IN~NNS', 'CD~NN~NNS', 'NN~JJ~NN', 'NNS~IN~NNP~NNP', 'NN~IN~JJ~NN', 'NNP~NNP~NNP~IN~NNP', 'NN~NNS', 'JJ~NNP~NN', 'JJ~NN~IN~NNS', 'NN~NN~IN~NNP', 'NN~IN~PRP$~NN', 'NN~IN~NN', 'JJ~VBN', 'CD~NN~NN', 'PRP$~NN~IN~NN', 'PRP$~NN~JJ~NN', 'JJ~NN~NN~VBN', 'JJ~JJ~JJ~NN', 'VBN~IN~CD~IN~NN', 'CD~TO~CD', 'NNS~NN', 'CD~NNS~VBN', 'NN~VBN', 'NNP~IN~NNP~NNP', 'NN~NNP', 'JJ~NN~NNP', 'VBN~IN~CD~TO~CD', 'JJ~NN~VBN~IN~CD', 'NN~VBN~IN~CD', 'JJ~NN~IN~NNP', 'JJ~NN~IN~JJ~NN', 'JJ~JJ~NN~NN~VBN', 'VBN~IN~NNP~NNP', 'NN~NN~IN~NNP~NNP', 'NNS~IN~NN', 'JJ~NNS~IN~NNP', 'NN~IN~NN~IN~NNP', 'NN~NN~VBN~IN~NNP~NNP', 'JJ~NN~NN~VBN~IN~NNP~NNP', 'NN~IN~JJ~NNS', 'NN~NN~CD~NN', 'NN~IN~CD~JJ~NNS', 'JJ~JJ~NNS', 'CD~JJ~NN', 'PRP$~JJ~NN~IN~NN', 'PRP$~NN~NNS', 'VBN~IN~NNP~IN~CD~TO~CD']\n",
    "    cols_postag_obj = ['VBN~IN~NNP~IN~CD', 'VBN~IN~NNP', 'VBN~IN~CD', 'NN~NN~NN~VBN~IN~NNP', 'NNP', 'NNP~NNP~NNP~NNP', 'NN', 'JJ~NN~NN', 'JJ~CD', 'NN~CD', 'NNP~NNPS', 'NNP~NNP~NNS', 'JJ~NNP~NNP', 'RB~VBN', 'PRP', 'CD', 'NNP~NNP~IN~NNP~NNP', 'JJ~NN', 'NN~IN~NN~NN', 'CD~NNS', 'NN~IN~NNP', 'NN~NN', 'NN~NN~NN', 'JJ~NN~NN~NN', 'JJ~NN~VBN~IN~NNP', 'NN~VBN~IN~NNP', 'NNP~CD', 'NN~NN~VBN~IN~NN~NNP', 'NN~NN~VBN~IN~JJ~NN~NNP', 'NNP~NNP~NNP', 'JJ~NNS', 'NNP~NNP', 'NN~NN~CD', 'NNP~NN', 'NN~IN~CD', 'JJ~NN~NN~VBN~IN~CD', 'NN~NN~VBN~IN~CD', 'CD~NNP~CD', 'NN~NN~VBN~IN~NNP~NNP~NNP', 'NN~IN~CD~NNS', 'NNP~NNP~NN~NN', 'JJ', 'NNP~POS~NN', 'PRP$~NN~NN', 'JJ~NN~NN~NN~VBN', 'NN~NN~NN~VBN', 'NNP~NN~NN', 'JJ~NNP', 'NNP~NNP~IN~NNP', 'PRP$~JJ~NN', 'PRP$~NN', 'VBN~IN~CD~IN~NNP~NNP', 'JJ~JJ', 'PRP$~CD~NN', 'NNP~NN~VBN', 'NN~IN~NNP~NNPS', 'NN~IN~NNP~NNP', 'CD~CD~NNS', 'JJ~NN~IN~NN', 'NN~NN~NN~CD', 'RB~NN', 'VBN~IN~NNP~NNP~IN~CD', 'CD~NN', 'NN~IN~NNP~NNP~NNP', 'JJ~NN~VBN~IN~NN', 'JJ~NN~VBN', 'JJ~JJ~NN', 'JJ~JJ~NN~VBN', 'PRP$~NNS', 'CD~NNS~IN~NN', 'CD~JJ~NNS', 'NN~VBN~IN~NNP~NNP~NNP', 'JJ~NN~IN~NNP~NNP', 'NN~NNP~NNP', 'JJ~NN~NNP~NNP', 'NNS', 'NN~NN~NN~NN', 'NN~NN~IN~NN', 'NNP~IN~NNP', 'NNP~NNS', 'JJ~JJ~NN~NN', 'CD~NN~IN~NN', 'NN~CD~CD', 'NN~NN~VBN~IN~NNP', 'JJ~NN~NN~VBN~IN~NNP', 'NNP~NNP~NN', 'JJ~NN~NN~NN~CD', 'NNP~CD~NN', 'JJ~NN~NN~NN~VBN~IN~NNP', 'JJ~JJ~NN~VBN~IN~NNP', 'JJ~NN~NNS', 'NN~VBN~IN~NNP~NNP', 'JJ~NN~VBN~IN~NNP~NNP', 'RB~NNP', 'NNP~NNP~CD', 'NNS~IN~NNP', 'NN~NN~VBN', 'CD~NNS~IN~NNS', 'CD~NN~NNS', 'NN~JJ~NN', 'NNS~IN~NNP~NNP', 'NN~IN~JJ~NN', 'NNP~NNP~NNP~IN~NNP', 'NN~NNS', 'JJ~NNP~NN', 'JJ~NN~IN~NNS', 'NN~NN~IN~NNP', 'NN~IN~PRP$~NN', 'NN~IN~NN', 'JJ~VBN', 'CD~NN~NN', 'PRP$~NN~IN~NN', 'PRP$~NN~JJ~NN', 'JJ~NN~NN~VBN', 'JJ~JJ~JJ~NN', 'VBN~IN~CD~IN~NN', 'CD~TO~CD', 'NNS~NN', 'CD~NNS~VBN', 'NN~VBN', 'NNP~IN~NNP~NNP', 'NN~NNP', 'JJ~NN~NNP', 'VBN~IN~CD~TO~CD', 'JJ~NN~VBN~IN~CD', 'NN~VBN~IN~CD', 'JJ~NN~IN~NNP', 'JJ~NN~IN~JJ~NN', 'JJ~JJ~NN~NN~VBN', 'VBN~IN~NNP~NNP', 'NN~NN~IN~NNP~NNP', 'NNS~IN~NN', 'JJ~NNS~IN~NNP', 'NN~IN~NN~IN~NNP', 'NN~NN~VBN~IN~NNP~NNP', 'JJ~NN~NN~VBN~IN~NNP~NNP', 'NN~IN~JJ~NNS', 'NN~NN~CD~NN', 'NN~IN~CD~JJ~NNS', 'JJ~JJ~NNS', 'CD~JJ~NN', 'PRP$~JJ~NN~IN~NN', 'PRP$~NN~NNS', 'VBN~IN~NNP~IN~CD~TO~CD']\n",
    "    cols_postag_rel = ['VBZ', 'VBZ~DT~JJ~NN', 'VBD~RB~VBN~IN', 'VBD~VBN~IN', 'MD~VB', 'MD~RB~VB', 'VBD~NNS~IN', 'VBD', 'NN~IN', 'VBN', 'VBN~IN', 'IN', 'VB', 'VBZ~RB~TO', 'VBZ~TO', 'RB~VBN~IN', 'VBD~RB~VBN', 'VBD~VBN', 'VBG~IN', 'VB~NN', 'VBD~RB~VBN~TO', 'VB~NN~IN', 'VBZ~VBN', 'VBD~TO', 'VBZ~IN', 'VBD~IN', 'VBZ~NN~NN', 'VBG', 'POS~NNP', 'NN~NN', 'VBZ~NN', 'VBZ~NN~IN', '', 'VBZ~VBN~IN', 'VBZ~JJ~TO', 'POS', 'VBD~NN', 'VBD~NN~IN', 'RB', 'VBP', 'VBZ~DT~NN~IN', 'VBD~JJ~NN~IN', 'VBZ~JJ~NN', 'VBZ~JJ~NN~IN', 'POS~NN', 'MD', 'CD~IN', 'VB~NNS~IN', 'NN', 'NNP~IN', 'VBD~VBN~TO', 'VBG~NN', 'VBD~NNS', 'RB~VBD', 'VBP~RB~VBN', 'VBP~VBN', 'RB~VBZ', 'NNS~IN', 'JJ~NNS~IN', 'RB~VBZ~IN', 'VBP~JJ~NNS', 'NNS', 'VBZ~RB~VBN', 'VBN~TO', 'JJ~NN', 'VBD~DT~JJ~NN', 'JJ~IN', 'RB~VBN', 'VB~NNS', 'VBZ~NN~NNS', 'VBZ~RB~VBN~IN', 'VBG~TO', 'NN~NN~NN', 'VBZ~JJ~IN', 'VBD~JJ~IN', 'VBD~JJ', 'VB~JJ', 'VBZ~VBG', 'NN~TO', 'MD~VB~IN', 'VBD~DT~NN', 'VBP~IN', 'VBZ~NNS~IN', 'RB~VBP', 'VBD~VBG', 'RB~VBG', 'JJ~NN~IN', 'JJ~JJ~NN~IN', 'VBD~JJ~TO', 'VBN~NN~IN', 'VBD~JJ~NN', 'NN~NNS', 'VBD~NN~NN', 'VBG~NNS~IN', 'VBZ~DT~NN', 'VBD~NN~TO', 'VBD~JJ~VBN~IN', 'VBP~VBN~IN', 'VBP~NNS', 'RB~VB~NNS~IN', 'VBN~NN', 'RB~VB', 'VB~IN', 'VBZ~DT~NN~TO', 'VBN~VBN', 'VBZ~JJ', 'VBZ~NN~TO', 'VBP~VBN~TO', 'TO', 'VBD~DT~NN~NN', 'VB~TO', 'VBD~NN~NNS~IN', 'VBD~DT~NN~IN', 'VBD~NNP~IN', 'VBZ~VBN~TO', 'RB~VBD~NNS~IN', 'RB~VBD~IN', 'VBG~NN~IN', 'VBD~VBN~RP~IN', 'VBD~VBN~RP', 'VBP~JJ', 'VBD~RP', 'VBD~NN~NN~IN', 'VBZ~NN~NNS~IN', 'RB~VBZ~VBN', 'VBD~NNP', 'VBP~RB~VBN~IN', 'VBP~RB', 'VBZ~NNS', 'MD~VB~RP', 'VBP~DT~NN~IN', 'VBD~DT~NN~TO', 'JJ', 'VBD~RB', 'RB~VBP~IN', 'NNP', 'RB~VBG~IN', 'VBD~NNP~NNP', 'VBG~VBN', 'VBZ~NNP~IN', 'RB~VBD~TO', 'VBZ~RB~JJ~IN', 'VBD~NNP~NNP~NNP', 'RB~VBD~NN', 'VBZ~DT~NNP', 'VBZ~JJ~JJ~NN', 'VBP~JJ~IN', 'VBZ~VBN~NN~IN', 'RBS~VBN~IN', 'VBZ~VBN~NN', 'VBZ~NNP', 'VBG~VBN~IN', 'RBS~JJ~NN~IN', 'VBD~RB~TO', 'JJ~JJ~NN', 'MD~VB~NN', 'JJ~VBN~IN', 'VB~VBN~IN', 'VBD~RP~IN', 'VBG~NNS', 'RB~VBD~NN~NN', 'VBP~NN', 'VBZ~JJS~NN', 'VBN~VBN~IN', 'VBG~DT~NN', 'IN~IN', 'RB~IN', 'VB~VBN', 'VBP~VBN~NN', 'JJ~TO', 'VBP~TO', 'VBD~JJ~NNS', 'VBD~RB~IN', 'DT~IN', 'VBD~NN~NNS', 'JJ~CC~JJ~NNS', 'JJR~NN', 'VBD~DT~JJ', 'VBD~RBS~JJ~NN', 'VBP~JJ~NNS~IN', 'VBZ~CD', 'VBZ~CD~IN', 'NN~NN~IN', 'VBD~RP~NN', 'VBP~NNS~IN', 'RB~VBD~RB', 'VBD~JJ~VBN', 'MD~NN~IN', 'RB~VBD~NN~IN', 'RB~NNP', 'VBG~RB~TO', 'VBZ~RBS~JJ~IN', 'VBG~NNP~NNS', 'VBZ~RB~JJ~NN', 'VBP~NN~IN', 'VBP~NNP', 'RB~VBG~RB', 'VBG~RB', 'VBP~RP~TO', 'VB~RB~RBR', 'VB~RBR', 'VBD~NN~NN~NNS', 'VBZ~VB~NN~IN', 'VBZ~VBG~NN~IN', 'RBR', 'NNS~NNP~IN', 'JJ~NNP', 'RB~JJ~NNP', 'NNP~NN~NN~IN', 'VBZ~NNS~RB~IN', 'VBG~CD~NN~IN', 'VBG~PRP$~NNS~TO', 'RBS~RB']\n",
    "    \n",
    "    def get_postags_sequence(span, words, columns):\n",
    "        sequence = '~'.join([token['pos'] for token in sentence[0]['tokens'][span[0]:span[1]] \n",
    "                     if token['originalText'] in words])\n",
    "    \n",
    "        result = tuple(int(sequence == column) for column in columns)\n",
    "        return result\n",
    "    \n",
    "    def get_before(span):\n",
    "        return sentence[0]['tokens'][span[0]]['before']\n",
    "    \n",
    "    def get_after(span):\n",
    "        return sentence[0]['tokens'][span[-1] - 1]['after']\n",
    "    \n",
    "    _ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "       'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "       'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "       'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "       'MONEY', 'HANDLE']\n",
    "    \n",
    "    def get_ner_occurrences(span, words):\n",
    "        mentions = [token['ner'] for token in sentence[0]['tokens'][span[0]:span[1]] \n",
    "                             if token['originalText'] in words]\n",
    "        result = tuple(int(_ner_kind in mentions) for _ner_kind in _ner_kinds)\n",
    "        return result\n",
    "    \n",
    "    def tag_lemma(span, words, tag=False):\n",
    "        if tag:\n",
    "            return [token['lemma'].lower() + '_' + _penn_tagset[token['pos']]['fPOS'] for token in sentence[0]['tokens'][span[0]:span[1]] \n",
    "                             if token['originalText'] in words]\n",
    "        else:\n",
    "            return [token['lemma'].lower() for token in sentence[0]['tokens'][span[0]:span[1]] \n",
    "                             if token['originalText'] in words]\n",
    "    \n",
    "    def remove_repetition(words):\n",
    "        if words[:len(words)//2].strip() == words[len(words)//2:].strip():\n",
    "            return words[:len(words)//2].strip()\n",
    "        return words\n",
    "    \n",
    "    def _build_dep_path(dependencies, tokens, start: int, end: int):\n",
    "        edges = []\n",
    "        deps = {}\n",
    "\n",
    "        for edge in dependencies:\n",
    "            edges.append((edge['governor'], edge['dependent']))\n",
    "            deps[(min(edge['governor'], edge['dependent']),\n",
    "                  max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "        graph = nx.Graph(edges)\n",
    "        path = nx.shortest_path(graph, source=start, target=end)\n",
    "        return path[:-1]  # exclude right end\n",
    "\n",
    "    def _tokens_by_index(indexes, tokens):\n",
    "        return [token['originalText'] for token in tokens if token['index'] in indexes]\n",
    "    \n",
    "    def _lemmas_by_index(indexes, tokens):\n",
    "        return [token['lemma'].lower() for token in tokens if token['index'] in indexes]\n",
    "\n",
    "    result = []\n",
    "    header = ['subject', 'relation', 'object', \n",
    "              'dep_path',\n",
    "              'distance_0', 'distance_1', \n",
    "              'rel_pos_subj', 'rel_pos_rel', 'rel_pos_obj',\n",
    "              #'postags_subj', 'postags_rel', 'postags_obj',\n",
    "              'lemma_subj', 'lemma_rel', 'lemma_obj'] +\\\n",
    "            [ner + '_subj' for ner in _ner_kinds] +\\\n",
    "            [ner + '_rel' for ner in _ner_kinds] +\\\n",
    "            [ner + '_obj' for ner in _ner_kinds] +\\\n",
    "            [col + '_subj' for col in cols_postag_subj] +\\\n",
    "            [col + '_obj' for col in cols_postag_obj] +\\\n",
    "            [col + '_rel' for col in cols_postag_rel]\n",
    "\n",
    "    for triplet in sentence[0]['openie']:\n",
    "        result.append((\n",
    "            remove_repetition(triplet['subject']), \n",
    "            remove_repetition(triplet['relation']), \n",
    "            remove_repetition(triplet['object']),\n",
    "            ' '.join(_lemmas_by_index(_build_dep_path(sentence[0]['basicDependencies'], \n",
    "                                                      sentence[0]['tokens'], \n",
    "                                                      triplet['subjectSpan'][0], \n",
    "                                                      triplet['objectSpan'][-1]), sentence[0]['tokens'])),\n",
    "            triplet['relationSpan'][0] - triplet['subjectSpan'][0],\n",
    "            triplet['objectSpan'][0] - triplet['relationSpan'][0],\n",
    "            triplet['subjectSpan'][0] / len(sentence[0]['tokens']),\n",
    "            triplet['relationSpan'][0] / len(sentence[0]['tokens']),\n",
    "            triplet['objectSpan'][0] / len(sentence[0]['tokens']),\n",
    "            tag_lemma(triplet['subjectSpan'], triplet['subject']),\n",
    "            tag_lemma(triplet['relationSpan'], triplet['relation']),\n",
    "            tag_lemma(triplet['objectSpan'], triplet['object']),\n",
    "        ) +\\\n",
    "            get_ner_occurrences(triplet['subjectSpan'], triplet['subject']) +\\\n",
    "            get_ner_occurrences(triplet['relationSpan'], triplet['relation']) +\\\n",
    "            get_ner_occurrences(triplet['objectSpan'], triplet['object']) +\\\n",
    "            get_postags_sequence(triplet['subjectSpan'], triplet['subject'], cols_postag_subj) +\\\n",
    "            get_postags_sequence(triplet['objectSpan'], triplet['relation'], cols_postag_obj) +\\\n",
    "            get_postags_sequence(triplet['relationSpan'], triplet['object'], cols_postag_rel)\n",
    "        ) \n",
    "        \n",
    "    return pd.DataFrame(result, columns=header)\n",
    "\n",
    "def _embed(placeholder, words):\n",
    "    for j in range(len(words)):\n",
    "        if j == len(placeholder):\n",
    "            break\n",
    "            \n",
    "        word = words[j]\n",
    "        if word and word in word2vec_model:\n",
    "            placeholder[j, :] = word2vec_model[word]\n",
    "    return placeholder\n",
    "    \n",
    "def _extract_features(sentence):\n",
    "    features = _extract_plain_features(sentence[1])\n",
    "    features.insert(loc=0, column='docid', value=sentence[0])\n",
    "    max_len = {'obj': 25, 'rel': 10, 'subj': 10}\n",
    "    features['w2v_subj'] = features['lemma_subj'].map(lambda words: _embed(np.zeros((max_len['subj'], word2vec_vector_length)), words))\n",
    "    features['w2v_rel'] = features['lemma_rel'].map(lambda words: _embed(np.zeros((max_len['rel'], word2vec_vector_length)), words))\n",
    "    features['w2v_obj'] = features['lemma_obj'].map(lambda words: _embed(np.zeros((max_len['obj'], word2vec_vector_length)), words))\n",
    "                                                                  \n",
    "    return features\n",
    "\n",
    "def remove_repetitions(annot):\n",
    "    for i in range(len(annot)):\n",
    "        for j in range(len(annot[i])):\n",
    "            annot[i][j]['openie'] = list(unique_everseen(annot[i][j]['openie']))\n",
    "    return annot\n",
    "\n",
    "                                           \n",
    "class FeaturesProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pool = multiprocessing.Pool(processes=4)\n",
    "                 \n",
    "    def __call__(self, data):\n",
    "        features = pd.concat(self.pool.map(_extract_features, data))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/corenlp_annotations_ner_pairs'  #'data/filtered_annotations'\n",
    "RESULT_PATH = 'data/processed'\n",
    "! mkdir $RESULT_PATH \n",
    "result = []\n",
    "extr = FeaturesProcessor()\n",
    "\n",
    "for file in tqdm(glob(DATA_PATH + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp = tmp[tmp.loc[:, 1].map(len) > 0]\n",
    "    tmp[1] = remove_repetitions(tmp[1].values)\n",
    "    result = extr(tmp[[0, 1]].values)\n",
    "    result.to_pickle(file.replace(DATA_PATH, RESULT_PATH).replace('.json', '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>dep_path</th>\n",
       "      <th>lemma_subj</th>\n",
       "      <th>lemma_rel</th>\n",
       "      <th>lemma_obj</th>\n",
       "      <th>w2v_subj</th>\n",
       "      <th>w2v_rel</th>\n",
       "      <th>w2v_obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Houthi movement</td>\n",
       "      <td>is</td>\n",
       "      <td>Islamic religious-political-armed movement</td>\n",
       "      <td></td>\n",
       "      <td>[houthi, movement]</td>\n",
       "      <td>[be]</td>\n",
       "      <td>[islamic, religious-political-armed, movement]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.228515625, -0.08837890625, 0.1279296875, ...</td>\n",
       "      <td>[[-0.158203125, 0.0361328125, 0.3515625, 0.437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Houthi movement</td>\n",
       "      <td>is</td>\n",
       "      <td>Islamic movement</td>\n",
       "      <td></td>\n",
       "      <td>[houthi, movement]</td>\n",
       "      <td>[be]</td>\n",
       "      <td>[islamic, movement]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.228515625, -0.08837890625, 0.1279296875, ...</td>\n",
       "      <td>[[-0.158203125, 0.0361328125, 0.3515625, 0.437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUBIT Cubit</td>\n",
       "      <td>is</td>\n",
       "      <td>computer user interface system for multi-touch...</td>\n",
       "      <td>system device design</td>\n",
       "      <td>[cubit, cubit]</td>\n",
       "      <td>[be]</td>\n",
       "      <td>[computer, user, interface, system, for, multi...</td>\n",
       "      <td>[[0.322265625, 0.1259765625, 0.142578125, -0.0...</td>\n",
       "      <td>[[-0.228515625, -0.08837890625, 0.1279296875, ...</td>\n",
       "      <td>[[0.107421875, -0.201171875, 0.123046875, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUBIT Cubit</td>\n",
       "      <td>is</td>\n",
       "      <td>computer user interface system for multi-touch...</td>\n",
       "      <td>system device design</td>\n",
       "      <td>[cubit, cubit]</td>\n",
       "      <td>[be]</td>\n",
       "      <td>[computer, user, interface, system, for, multi...</td>\n",
       "      <td>[[0.322265625, 0.1259765625, 0.142578125, -0.0...</td>\n",
       "      <td>[[-0.228515625, -0.08837890625, 0.1279296875, ...</td>\n",
       "      <td>[[0.107421875, -0.201171875, 0.123046875, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUBIT Cubit</td>\n",
       "      <td>is</td>\n",
       "      <td>computer user interface system for devices des...</td>\n",
       "      <td>system device design</td>\n",
       "      <td>[cubit, cubit]</td>\n",
       "      <td>[be]</td>\n",
       "      <td>[computer, user, interface, system, for, devic...</td>\n",
       "      <td>[[0.322265625, 0.1259765625, 0.142578125, -0.0...</td>\n",
       "      <td>[[-0.228515625, -0.08837890625, 0.1279296875, ...</td>\n",
       "      <td>[[0.107421875, -0.201171875, 0.123046875, 0.21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject relation  \\\n",
       "0  Houthi movement       is   \n",
       "1  Houthi movement       is   \n",
       "0      CUBIT Cubit       is   \n",
       "1      CUBIT Cubit       is   \n",
       "2      CUBIT Cubit       is   \n",
       "\n",
       "                                              object              dep_path  \\\n",
       "0         Islamic religious-political-armed movement                         \n",
       "1                                   Islamic movement                         \n",
       "0  computer user interface system for multi-touch...  system device design   \n",
       "1  computer user interface system for multi-touch...  system device design   \n",
       "2  computer user interface system for devices des...  system device design   \n",
       "\n",
       "           lemma_subj lemma_rel  \\\n",
       "0  [houthi, movement]      [be]   \n",
       "1  [houthi, movement]      [be]   \n",
       "0      [cubit, cubit]      [be]   \n",
       "1      [cubit, cubit]      [be]   \n",
       "2      [cubit, cubit]      [be]   \n",
       "\n",
       "                                           lemma_obj  \\\n",
       "0     [islamic, religious-political-armed, movement]   \n",
       "1                                [islamic, movement]   \n",
       "0  [computer, user, interface, system, for, multi...   \n",
       "1  [computer, user, interface, system, for, multi...   \n",
       "2  [computer, user, interface, system, for, devic...   \n",
       "\n",
       "                                            w2v_subj  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "0  [[0.322265625, 0.1259765625, 0.142578125, -0.0...   \n",
       "1  [[0.322265625, 0.1259765625, 0.142578125, -0.0...   \n",
       "2  [[0.322265625, 0.1259765625, 0.142578125, -0.0...   \n",
       "\n",
       "                                             w2v_rel  \\\n",
       "0  [[-0.228515625, -0.08837890625, 0.1279296875, ...   \n",
       "1  [[-0.228515625, -0.08837890625, 0.1279296875, ...   \n",
       "0  [[-0.228515625, -0.08837890625, 0.1279296875, ...   \n",
       "1  [[-0.228515625, -0.08837890625, 0.1279296875, ...   \n",
       "2  [[-0.228515625, -0.08837890625, 0.1279296875, ...   \n",
       "\n",
       "                                             w2v_obj  \n",
       "0  [[-0.158203125, 0.0361328125, 0.3515625, 0.437...  \n",
       "1  [[-0.158203125, 0.0361328125, 0.3515625, 0.437...  \n",
       "0  [[0.107421875, -0.201171875, 0.123046875, 0.21...  \n",
       "1  [[0.107421875, -0.201171875, 0.123046875, 0.21...  \n",
       "2  [[0.107421875, -0.201171875, 0.123046875, 0.21...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect to one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv data/processed/processed_ processed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv processed_ data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations\t\t\t      enwiki-latest-pages-articles.xml.bz2.1\r\n",
      "categories.filter\t\t      filtered_annotations\r\n",
      "corenlp_annotations_filtered\t      it_wiki_articles.json\r\n",
      "corenlp_annotations_ner_pairs\t      prepare_wiki_dataset.ipynb\r\n",
      "corenlp_annotations_new\t\t      processed\r\n",
      "dataset.pkl\t\t\t      sample_dataset.pkl\r\n",
      "enwiki-latest-pages-articles.xml.bz2  stopwords.filter\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292cd4ac70be46b894f0427332da917f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=132), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/processed'\n",
    "\n",
    "result = []\n",
    "for file in tqdm(glob(DATA_PATH + '/*.pkl')):\n",
    "    result.append(pd.read_pickle(file))\n",
    "    \n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48983, 586)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pickle('data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW for relation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_rel = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "bow_rel = vectorizer_rel.fit_transform(result['relation'].values).toarray()\n",
    "columns = [column + '_bow_rel' for column in vectorizer_rel.get_feature_names()]\n",
    "features = pd.concat([result.reset_index(drop=True), pd.DataFrame(bow_rel, columns=columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer_rel, open('models/relation_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW for synt dependency path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_path = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "bow_path = vectorizer_path.fit_transform(features['dep_path'].values).toarray()\n",
    "columns = [column + '_bow_path' for column in vectorizer_path.get_feature_names()]\n",
    "features = pd.concat([features.reset_index(drop=True), pd.DataFrame(bow_path, columns=columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer_path, open('models/path_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48983, 2586)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple clustering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "excluding_cols = ['docid', 'subject', 'relation', 'object', 'dep_path', 'lemma_subj', 'lemma_rel', 'lemma_obj']\n",
    "embedding_cols = ['w2v_subj', 'w2v_rel', 'w2v_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=100, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans=KMeans(n_clusters=100)\n",
    "kmeans.fit(X.drop(columns=embedding_cols+excluding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return X[kmeans.labels_ == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>37976837</td>\n",
       "      <td>He</td>\n",
       "      <td>is known as</td>\n",
       "      <td>founder director of Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21530</th>\n",
       "      <td>1281824</td>\n",
       "      <td>Koster</td>\n",
       "      <td>is recognized for</td>\n",
       "      <td>his work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19517</th>\n",
       "      <td>14581096</td>\n",
       "      <td>model</td>\n",
       "      <td>is</td>\n",
       "      <td>three-layer model for network design proposed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34127</th>\n",
       "      <td>142597</td>\n",
       "      <td>He</td>\n",
       "      <td>is best known for</td>\n",
       "      <td>his work in fields of operational research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47459</th>\n",
       "      <td>48327467</td>\n",
       "      <td>Spindel</td>\n",
       "      <td>is known for</td>\n",
       "      <td>his involvement in union leader Jimmy Hoffa 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17933</th>\n",
       "      <td>16064919</td>\n",
       "      <td>Baer</td>\n",
       "      <td>is known for</td>\n",
       "      <td>his contributions at University of Kansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19251</th>\n",
       "      <td>27717386</td>\n",
       "      <td>He</td>\n",
       "      <td>is son of</td>\n",
       "      <td>Zhiuli Shartava killed by Abkhaz militias duri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25902</th>\n",
       "      <td>8207891</td>\n",
       "      <td>Vaynerchuk</td>\n",
       "      <td>is best known for</td>\n",
       "      <td>his work as chairman as CEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>37976837</td>\n",
       "      <td>He</td>\n",
       "      <td>is best known as</td>\n",
       "      <td>founder director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42085</th>\n",
       "      <td>1628191</td>\n",
       "      <td>He</td>\n",
       "      <td>is leader</td>\n",
       "      <td>cares about his people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          docid     subject           relation  \\\n",
       "5820   37976837          He        is known as   \n",
       "21530   1281824      Koster  is recognized for   \n",
       "19517  14581096       model                 is   \n",
       "34127    142597          He  is best known for   \n",
       "47459  48327467     Spindel       is known for   \n",
       "17933  16064919        Baer       is known for   \n",
       "19251  27717386          He          is son of   \n",
       "25902   8207891  Vaynerchuk  is best known for   \n",
       "5822   37976837          He   is best known as   \n",
       "42085   1628191          He          is leader   \n",
       "\n",
       "                                                  object  \n",
       "5820                         founder director of Defense  \n",
       "21530                                           his work  \n",
       "19517  three-layer model for network design proposed ...  \n",
       "34127         his work in fields of operational research  \n",
       "47459  his involvement in union leader Jimmy Hoffa 's...  \n",
       "17933          his contributions at University of Kansas  \n",
       "19251  Zhiuli Shartava killed by Abkhaz militias duri...  \n",
       "25902                        his work as chairman as CEO  \n",
       "5822                                    founder director  \n",
       "42085                             cares about his people  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_cluster_sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12358</th>\n",
       "      <td>8783360</td>\n",
       "      <td>Chevrolet Volt</td>\n",
       "      <td>is</td>\n",
       "      <td>plug-in hybrid car also marketed as Holden Volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35592</th>\n",
       "      <td>576403</td>\n",
       "      <td>Renault Mégane</td>\n",
       "      <td>is</td>\n",
       "      <td>small family car produced by car manufacturer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37976</th>\n",
       "      <td>1822962</td>\n",
       "      <td>Daewoo Lanos</td>\n",
       "      <td>is</td>\n",
       "      <td>subcompact car produced by manufacturer Daewoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44577</th>\n",
       "      <td>373793</td>\n",
       "      <td>Volkswagen Polo</td>\n",
       "      <td>is</td>\n",
       "      <td>car produced by manufacturer Volkswagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44837</th>\n",
       "      <td>409837</td>\n",
       "      <td>Mercedes-Benz C-Class</td>\n",
       "      <td>is line of</td>\n",
       "      <td>compact executive cars produced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10492</th>\n",
       "      <td>43797566</td>\n",
       "      <td>Opel Karl</td>\n",
       "      <td>is</td>\n",
       "      <td>city car designated as their entry model for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41739</th>\n",
       "      <td>10523335</td>\n",
       "      <td>MG 3</td>\n",
       "      <td>is</td>\n",
       "      <td>subcompact car produced by giant SAIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19421</th>\n",
       "      <td>14507998</td>\n",
       "      <td>Elegia</td>\n",
       "      <td>is</td>\n",
       "      <td>genus of grass-like plants described as genus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12371</th>\n",
       "      <td>8783360</td>\n",
       "      <td>Chevrolet Volt</td>\n",
       "      <td>is</td>\n",
       "      <td>plug-in hybrid car marketed in rebadged varian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>9557050</td>\n",
       "      <td>Suzuki Lapin</td>\n",
       "      <td>is</td>\n",
       "      <td>kei car with five-door hatchback body manufact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          docid                subject    relation  \\\n",
       "12358   8783360         Chevrolet Volt          is   \n",
       "35592    576403         Renault Mégane          is   \n",
       "37976   1822962           Daewoo Lanos          is   \n",
       "44577    373793        Volkswagen Polo          is   \n",
       "44837    409837  Mercedes-Benz C-Class  is line of   \n",
       "10492  43797566              Opel Karl          is   \n",
       "41739  10523335                   MG 3          is   \n",
       "19421  14507998                 Elegia          is   \n",
       "12371   8783360         Chevrolet Volt          is   \n",
       "2811    9557050           Suzuki Lapin          is   \n",
       "\n",
       "                                                  object  \n",
       "12358    plug-in hybrid car also marketed as Holden Volt  \n",
       "35592  small family car produced by car manufacturer ...  \n",
       "37976  subcompact car produced by manufacturer Daewoo...  \n",
       "44577            car produced by manufacturer Volkswagen  \n",
       "44837                    compact executive cars produced  \n",
       "10492  city car designated as their entry model for l...  \n",
       "41739              subcompact car produced by giant SAIC  \n",
       "19421  genus of grass-like plants described as genus ...  \n",
       "12371  plug-in hybrid car marketed in rebadged varian...  \n",
       "2811   kei car with five-door hatchback body manufact...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_cluster_sample(90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

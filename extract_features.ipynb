{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature rich dataset ``data/dataset.pkl`` out of corenlp annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'GoogleNews-vectors-negative300.bin.gz'  # 1.6G\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://s3.amazonaws.com/dl4j-distribution/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                       binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install iteration_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "       'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "       'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "       'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "       'MONEY', 'HANDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ner_kinds_obj = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "       'LOCATION', 'NATIONALITY', 'ORDINAL',\n",
    "       'DURATION', 'CITY']\n",
    "    \n",
    "_ner_kinds_subj = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "   'LOCATION', 'URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_ner_kinds_subj+_ner_kinds_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from iteration_utilities import unique_everseen\n",
    "\n",
    "\n",
    "def _extract_plain_features(document):\n",
    "    \n",
    "    def _extract(sentence):\n",
    "    \n",
    "        # postag sequences occuring in dataset more than 20 times\n",
    "        cols_postag_subj = ['VBN~IN~NNP~IN~CD', 'VBN~IN~NNP', 'VBN~IN~CD', 'NN~NN~NN~VBN~IN~NNP', 'NNP', 'NNP~NNP~NNP~NNP', 'NN', 'JJ~NN~NN', 'JJ~CD', 'NN~CD', 'NNP~NNPS', 'NNP~NNP~NNS', 'JJ~NNP~NNP', 'RB~VBN', 'PRP', 'CD', 'NNP~NNP~IN~NNP~NNP', 'JJ~NN', 'NN~IN~NN~NN', 'CD~NNS', 'NN~IN~NNP', 'NN~NN', 'NN~NN~NN', 'JJ~NN~NN~NN', 'JJ~NN~VBN~IN~NNP', 'NN~VBN~IN~NNP', 'NNP~CD', 'NN~NN~VBN~IN~NN~NNP', 'NN~NN~VBN~IN~JJ~NN~NNP', 'NNP~NNP~NNP', 'JJ~NNS', 'NNP~NNP', 'NN~NN~CD', 'NNP~NN', 'NN~IN~CD', 'JJ~NN~NN~VBN~IN~CD', 'NN~NN~VBN~IN~CD', 'CD~NNP~CD', 'NN~NN~VBN~IN~NNP~NNP~NNP', 'NN~IN~CD~NNS', 'NNP~NNP~NN~NN', 'JJ', 'NNP~POS~NN', 'PRP$~NN~NN', 'JJ~NN~NN~NN~VBN', 'NN~NN~NN~VBN', 'NNP~NN~NN', 'JJ~NNP', 'NNP~NNP~IN~NNP', 'PRP$~JJ~NN', 'PRP$~NN', 'VBN~IN~CD~IN~NNP~NNP', 'JJ~JJ', 'PRP$~CD~NN', 'NNP~NN~VBN', 'NN~IN~NNP~NNPS', 'NN~IN~NNP~NNP', 'CD~CD~NNS', 'JJ~NN~IN~NN', 'NN~NN~NN~CD', 'RB~NN', 'VBN~IN~NNP~NNP~IN~CD', 'CD~NN', 'NN~IN~NNP~NNP~NNP', 'JJ~NN~VBN~IN~NN', 'JJ~NN~VBN', 'JJ~JJ~NN', 'JJ~JJ~NN~VBN', 'PRP$~NNS', 'CD~NNS~IN~NN', 'CD~JJ~NNS', 'NN~VBN~IN~NNP~NNP~NNP', 'JJ~NN~IN~NNP~NNP', 'NN~NNP~NNP', 'JJ~NN~NNP~NNP', 'NNS', 'NN~NN~NN~NN', 'NN~NN~IN~NN', 'NNP~IN~NNP', 'NNP~NNS', 'JJ~JJ~NN~NN', 'CD~NN~IN~NN', 'NN~CD~CD', 'NN~NN~VBN~IN~NNP', 'JJ~NN~NN~VBN~IN~NNP', 'NNP~NNP~NN', 'JJ~NN~NN~NN~CD', 'NNP~CD~NN', 'JJ~NN~NN~NN~VBN~IN~NNP', 'JJ~JJ~NN~VBN~IN~NNP', 'JJ~NN~NNS', 'NN~VBN~IN~NNP~NNP', 'JJ~NN~VBN~IN~NNP~NNP', 'RB~NNP', 'NNP~NNP~CD', 'NNS~IN~NNP', 'NN~NN~VBN', 'CD~NNS~IN~NNS', 'CD~NN~NNS', 'NN~JJ~NN', 'NNS~IN~NNP~NNP', 'NN~IN~JJ~NN', 'NNP~NNP~NNP~IN~NNP', 'NN~NNS', 'JJ~NNP~NN', 'JJ~NN~IN~NNS', 'NN~NN~IN~NNP', 'NN~IN~PRP$~NN', 'NN~IN~NN', 'JJ~VBN', 'CD~NN~NN', 'PRP$~NN~IN~NN', 'PRP$~NN~JJ~NN', 'JJ~NN~NN~VBN', 'JJ~JJ~JJ~NN', 'VBN~IN~CD~IN~NN', 'CD~TO~CD', 'NNS~NN', 'CD~NNS~VBN', 'NN~VBN', 'NNP~IN~NNP~NNP', 'NN~NNP', 'JJ~NN~NNP', 'VBN~IN~CD~TO~CD', 'JJ~NN~VBN~IN~CD', 'NN~VBN~IN~CD', 'JJ~NN~IN~NNP', 'JJ~NN~IN~JJ~NN', 'JJ~JJ~NN~NN~VBN', 'VBN~IN~NNP~NNP', 'NN~NN~IN~NNP~NNP', 'NNS~IN~NN', 'JJ~NNS~IN~NNP', 'NN~IN~NN~IN~NNP', 'NN~NN~VBN~IN~NNP~NNP', 'JJ~NN~NN~VBN~IN~NNP~NNP', 'NN~IN~JJ~NNS', 'NN~NN~CD~NN', 'NN~IN~CD~JJ~NNS', 'JJ~JJ~NNS', 'CD~JJ~NN', 'PRP$~JJ~NN~IN~NN', 'PRP$~NN~NNS', 'VBN~IN~NNP~IN~CD~TO~CD']\n",
    "        cols_postag_obj = ['VBN~IN~NNP~IN~CD', 'VBN~IN~NNP', 'VBN~IN~CD', 'NN~NN~NN~VBN~IN~NNP', 'NNP', 'NNP~NNP~NNP~NNP', 'NN', 'JJ~NN~NN', 'JJ~CD', 'NN~CD', 'NNP~NNPS', 'NNP~NNP~NNS', 'JJ~NNP~NNP', 'RB~VBN', 'PRP', 'CD', 'NNP~NNP~IN~NNP~NNP', 'JJ~NN', 'NN~IN~NN~NN', 'CD~NNS', 'NN~IN~NNP', 'NN~NN', 'NN~NN~NN', 'JJ~NN~NN~NN', 'JJ~NN~VBN~IN~NNP', 'NN~VBN~IN~NNP', 'NNP~CD', 'NN~NN~VBN~IN~NN~NNP', 'NN~NN~VBN~IN~JJ~NN~NNP', 'NNP~NNP~NNP', 'JJ~NNS', 'NNP~NNP', 'NN~NN~CD', 'NNP~NN', 'NN~IN~CD', 'JJ~NN~NN~VBN~IN~CD', 'NN~NN~VBN~IN~CD', 'CD~NNP~CD', 'NN~NN~VBN~IN~NNP~NNP~NNP', 'NN~IN~CD~NNS', 'NNP~NNP~NN~NN', 'JJ', 'NNP~POS~NN', 'PRP$~NN~NN', 'JJ~NN~NN~NN~VBN', 'NN~NN~NN~VBN', 'NNP~NN~NN', 'JJ~NNP', 'NNP~NNP~IN~NNP', 'PRP$~JJ~NN', 'PRP$~NN', 'VBN~IN~CD~IN~NNP~NNP', 'JJ~JJ', 'PRP$~CD~NN', 'NNP~NN~VBN', 'NN~IN~NNP~NNPS', 'NN~IN~NNP~NNP', 'CD~CD~NNS', 'JJ~NN~IN~NN', 'NN~NN~NN~CD', 'RB~NN', 'VBN~IN~NNP~NNP~IN~CD', 'CD~NN', 'NN~IN~NNP~NNP~NNP', 'JJ~NN~VBN~IN~NN', 'JJ~NN~VBN', 'JJ~JJ~NN', 'JJ~JJ~NN~VBN', 'PRP$~NNS', 'CD~NNS~IN~NN', 'CD~JJ~NNS', 'NN~VBN~IN~NNP~NNP~NNP', 'JJ~NN~IN~NNP~NNP', 'NN~NNP~NNP', 'JJ~NN~NNP~NNP', 'NNS', 'NN~NN~NN~NN', 'NN~NN~IN~NN', 'NNP~IN~NNP', 'NNP~NNS', 'JJ~JJ~NN~NN', 'CD~NN~IN~NN', 'NN~CD~CD', 'NN~NN~VBN~IN~NNP', 'JJ~NN~NN~VBN~IN~NNP', 'NNP~NNP~NN', 'JJ~NN~NN~NN~CD', 'NNP~CD~NN', 'JJ~NN~NN~NN~VBN~IN~NNP', 'JJ~JJ~NN~VBN~IN~NNP', 'JJ~NN~NNS', 'NN~VBN~IN~NNP~NNP', 'JJ~NN~VBN~IN~NNP~NNP', 'RB~NNP', 'NNP~NNP~CD', 'NNS~IN~NNP', 'NN~NN~VBN', 'CD~NNS~IN~NNS', 'CD~NN~NNS', 'NN~JJ~NN', 'NNS~IN~NNP~NNP', 'NN~IN~JJ~NN', 'NNP~NNP~NNP~IN~NNP', 'NN~NNS', 'JJ~NNP~NN', 'JJ~NN~IN~NNS', 'NN~NN~IN~NNP', 'NN~IN~PRP$~NN', 'NN~IN~NN', 'JJ~VBN', 'CD~NN~NN', 'PRP$~NN~IN~NN', 'PRP$~NN~JJ~NN', 'JJ~NN~NN~VBN', 'JJ~JJ~JJ~NN', 'VBN~IN~CD~IN~NN', 'CD~TO~CD', 'NNS~NN', 'CD~NNS~VBN', 'NN~VBN', 'NNP~IN~NNP~NNP', 'NN~NNP', 'JJ~NN~NNP', 'VBN~IN~CD~TO~CD', 'JJ~NN~VBN~IN~CD', 'NN~VBN~IN~CD', 'JJ~NN~IN~NNP', 'JJ~NN~IN~JJ~NN', 'JJ~JJ~NN~NN~VBN', 'VBN~IN~NNP~NNP', 'NN~NN~IN~NNP~NNP', 'NNS~IN~NN', 'JJ~NNS~IN~NNP', 'NN~IN~NN~IN~NNP', 'NN~NN~VBN~IN~NNP~NNP', 'JJ~NN~NN~VBN~IN~NNP~NNP', 'NN~IN~JJ~NNS', 'NN~NN~CD~NN', 'NN~IN~CD~JJ~NNS', 'JJ~JJ~NNS', 'CD~JJ~NN', 'PRP$~JJ~NN~IN~NN', 'PRP$~NN~NNS', 'VBN~IN~NNP~IN~CD~TO~CD']\n",
    "        cols_postag_rel = ['VBZ', 'VBZ~DT~JJ~NN', 'VBD~RB~VBN~IN', 'VBD~VBN~IN', 'MD~VB', 'MD~RB~VB', 'VBD~NNS~IN', 'VBD', 'NN~IN', 'VBN', 'VBN~IN', 'IN', 'VB', 'VBZ~RB~TO', 'VBZ~TO', 'RB~VBN~IN', 'VBD~RB~VBN', 'VBD~VBN', 'VBG~IN', 'VB~NN', 'VBD~RB~VBN~TO', 'VB~NN~IN', 'VBZ~VBN', 'VBD~TO', 'VBZ~IN', 'VBD~IN', 'VBZ~NN~NN', 'VBG', 'POS~NNP', 'NN~NN', 'VBZ~NN', 'VBZ~NN~IN', '', 'VBZ~VBN~IN', 'VBZ~JJ~TO', 'POS', 'VBD~NN', 'VBD~NN~IN', 'RB', 'VBP', 'VBZ~DT~NN~IN', 'VBD~JJ~NN~IN', 'VBZ~JJ~NN', 'VBZ~JJ~NN~IN', 'POS~NN', 'MD', 'CD~IN', 'VB~NNS~IN', 'NN', 'NNP~IN', 'VBD~VBN~TO', 'VBG~NN', 'VBD~NNS', 'RB~VBD', 'VBP~RB~VBN', 'VBP~VBN', 'RB~VBZ', 'NNS~IN', 'JJ~NNS~IN', 'RB~VBZ~IN', 'VBP~JJ~NNS', 'NNS', 'VBZ~RB~VBN', 'VBN~TO', 'JJ~NN', 'VBD~DT~JJ~NN', 'JJ~IN', 'RB~VBN', 'VB~NNS', 'VBZ~NN~NNS', 'VBZ~RB~VBN~IN', 'VBG~TO', 'NN~NN~NN', 'VBZ~JJ~IN', 'VBD~JJ~IN', 'VBD~JJ', 'VB~JJ', 'VBZ~VBG', 'NN~TO', 'MD~VB~IN', 'VBD~DT~NN', 'VBP~IN', 'VBZ~NNS~IN', 'RB~VBP', 'VBD~VBG', 'RB~VBG', 'JJ~NN~IN', 'JJ~JJ~NN~IN', 'VBD~JJ~TO', 'VBN~NN~IN', 'VBD~JJ~NN', 'NN~NNS', 'VBD~NN~NN', 'VBG~NNS~IN', 'VBZ~DT~NN', 'VBD~NN~TO', 'VBD~JJ~VBN~IN', 'VBP~VBN~IN', 'VBP~NNS', 'RB~VB~NNS~IN', 'VBN~NN', 'RB~VB', 'VB~IN', 'VBZ~DT~NN~TO', 'VBN~VBN', 'VBZ~JJ', 'VBZ~NN~TO', 'VBP~VBN~TO', 'TO', 'VBD~DT~NN~NN', 'VB~TO', 'VBD~NN~NNS~IN', 'VBD~DT~NN~IN', 'VBD~NNP~IN', 'VBZ~VBN~TO', 'RB~VBD~NNS~IN', 'RB~VBD~IN', 'VBG~NN~IN', 'VBD~VBN~RP~IN', 'VBD~VBN~RP', 'VBP~JJ', 'VBD~RP', 'VBD~NN~NN~IN', 'VBZ~NN~NNS~IN', 'RB~VBZ~VBN', 'VBD~NNP', 'VBP~RB~VBN~IN', 'VBP~RB', 'VBZ~NNS', 'MD~VB~RP', 'VBP~DT~NN~IN', 'VBD~DT~NN~TO', 'JJ', 'VBD~RB', 'RB~VBP~IN', 'NNP', 'RB~VBG~IN', 'VBD~NNP~NNP', 'VBG~VBN', 'VBZ~NNP~IN', 'RB~VBD~TO', 'VBZ~RB~JJ~IN', 'VBD~NNP~NNP~NNP', 'RB~VBD~NN', 'VBZ~DT~NNP', 'VBZ~JJ~JJ~NN', 'VBP~JJ~IN', 'VBZ~VBN~NN~IN', 'RBS~VBN~IN', 'VBZ~VBN~NN', 'VBZ~NNP', 'VBG~VBN~IN', 'RBS~JJ~NN~IN', 'VBD~RB~TO', 'JJ~JJ~NN', 'MD~VB~NN', 'JJ~VBN~IN', 'VB~VBN~IN', 'VBD~RP~IN', 'VBG~NNS', 'RB~VBD~NN~NN', 'VBP~NN', 'VBZ~JJS~NN', 'VBN~VBN~IN', 'VBG~DT~NN', 'IN~IN', 'RB~IN', 'VB~VBN', 'VBP~VBN~NN', 'JJ~TO', 'VBP~TO', 'VBD~JJ~NNS', 'VBD~RB~IN', 'DT~IN', 'VBD~NN~NNS', 'JJ~CC~JJ~NNS', 'JJR~NN', 'VBD~DT~JJ', 'VBD~RBS~JJ~NN', 'VBP~JJ~NNS~IN', 'VBZ~CD', 'VBZ~CD~IN', 'NN~NN~IN', 'VBD~RP~NN', 'VBP~NNS~IN', 'RB~VBD~RB', 'VBD~JJ~VBN', 'MD~NN~IN', 'RB~VBD~NN~IN', 'RB~NNP', 'VBG~RB~TO', 'VBZ~RBS~JJ~IN', 'VBG~NNP~NNS', 'VBZ~RB~JJ~NN', 'VBP~NN~IN', 'VBP~NNP', 'RB~VBG~RB', 'VBG~RB', 'VBP~RP~TO', 'VB~RB~RBR', 'VB~RBR', 'VBD~NN~NN~NNS', 'VBZ~VB~NN~IN', 'VBZ~VBG~NN~IN', 'RBR', 'NNS~NNP~IN', 'JJ~NNP', 'RB~JJ~NNP', 'NNP~NN~NN~IN', 'VBZ~NNS~RB~IN', 'VBG~CD~NN~IN', 'VBG~PRP$~NNS~TO', 'RBS~RB']\n",
    "\n",
    "        # now we filter obj/subj/rels to be <= 3 in length\n",
    "        cols_postag_subj = [seq for seq in cols_postag_subj if len(seq.split('~')) <= 3]\n",
    "        cols_postag_obj = [seq for seq in cols_postag_obj if len(seq.split('~')) <= 3]\n",
    "        cols_postag_rel = [seq for seq in cols_postag_rel if len(seq.split('~')) <= 3]\n",
    "\n",
    "        def get_postags_sequence(span, words, columns):\n",
    "            sequence = '~'.join([token['pos'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                         if token['originalText'] in words])\n",
    "\n",
    "            result = tuple(int(sequence == column) for column in columns)\n",
    "            return result\n",
    "\n",
    "        def get_before(span):\n",
    "            return sentence['tokens'][span[0]]['before']\n",
    "\n",
    "        def get_after(span):\n",
    "            return sentence['tokens'][span[-1] - 1]['after']\n",
    "\n",
    "        _ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "           'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "           'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "           'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "           'MONEY', 'HANDLE']\n",
    "\n",
    "        _ner_kinds_obj = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "           'LOCATION', 'NATIONALITY', 'ORDINAL',\n",
    "           'DURATION', 'CITY']\n",
    "\n",
    "        _ner_kinds_subj = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "           'LOCATION', 'URL']\n",
    "\n",
    "        def get_ner_occurrences(span, words, obj=True):\n",
    "            mentions = [token['ner'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            if obj:\n",
    "                _ner_kinds = _ner_kinds_obj\n",
    "            else:\n",
    "                _ner_kinds = _ner_kinds_subj\n",
    "\n",
    "            result = tuple(int(_ner_kind in mentions) for _ner_kind in _ner_kinds)\n",
    "            return result\n",
    "\n",
    "        def tag_lemma(span, words, tag=False):\n",
    "            if tag:\n",
    "                return [token['lemma'].lower() + '_' + _penn_tagset[token['pos']]['fPOS'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            else:\n",
    "                return [token['lemma'].lower() for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "\n",
    "        def remove_repetition(words):\n",
    "            if words[:len(words)//2].strip() == words[len(words)//2:].strip():\n",
    "                return words[:len(words)//2].strip()\n",
    "            return words\n",
    "\n",
    "        def _build_dep_path(dependencies, tokens, start: int, end: int):\n",
    "            edges = []\n",
    "            deps = {}\n",
    "\n",
    "            for edge in dependencies:\n",
    "                edges.append((edge['governor'], edge['dependent']))\n",
    "                deps[(min(edge['governor'], edge['dependent']),\n",
    "                      max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "            graph = nx.Graph(edges)\n",
    "            path = nx.shortest_path(graph, source=start, target=end)\n",
    "            return path[:-1]  # exclude right end\n",
    "\n",
    "        def _tokens_by_index(indexes, tokens):\n",
    "            return [token['originalText'] for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        def _lemmas_by_index(indexes, tokens):\n",
    "            return [token['lemma'].lower() for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        result = []\n",
    "        header = ['subject', 'relation', 'object', \n",
    "                  'dep_path',\n",
    "                  'distance_0', 'distance_1', \n",
    "                  'rel_pos_subj', 'rel_pos_rel', 'rel_pos_obj',\n",
    "                  #'postags_subj', 'postags_rel', 'postags_obj',\n",
    "                  'lemma_subj', 'lemma_rel', 'lemma_obj'] +\\\n",
    "                [ner + '_subj' for ner in _ner_kinds_subj] +\\\n",
    "                [ner + '_obj' for ner in _ner_kinds_obj] +\\\n",
    "                [col + '_subj' for col in cols_postag_subj] +\\\n",
    "                [col + '_obj' for col in cols_postag_obj] +\\\n",
    "                [col + '_rel' for col in cols_postag_rel]\n",
    "\n",
    "        deprecated = set(['one', 'he', 'she', 'they', 'his', 'her', 'its', 'our', 'day', 'co.', 'inc.', \n",
    "              'society', 'people', 'inventor', 'head', 'poet', 'doctor', 'teacher', 'inventor', \n",
    "              'thanksgiving day', 'halloween',\n",
    "              'sales person', 'model', 'board', 'technology', 'owner', 'one', 'two'])\n",
    "        \n",
    "        triplets = sentence['openie']\n",
    "        filtered_triplets = filter(lambda obj: obj['object'].lower() not in deprecated and obj['subject'].lower() not in deprecated, triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['object']) > 2 or len(obj['subject']) > 2, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['relation'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = list(filtered_triplets)\n",
    "        \n",
    "        for triplet in filtered_triplets:\n",
    "            result.append((\n",
    "                remove_repetition(triplet['subject']), \n",
    "                remove_repetition(triplet['relation']), \n",
    "                remove_repetition(triplet['object']),\n",
    "                ' '.join(_lemmas_by_index(_build_dep_path(sentence['basicDependencies'], \n",
    "                                                          sentence['tokens'], \n",
    "                                                          triplet['subjectSpan'][0], \n",
    "                                                          triplet['objectSpan'][-1]), sentence['tokens'])),\n",
    "                triplet['relationSpan'][0] - triplet['subjectSpan'][0],\n",
    "                triplet['objectSpan'][0] - triplet['relationSpan'][0],\n",
    "                triplet['subjectSpan'][0] / len(sentence['tokens']),\n",
    "                triplet['relationSpan'][0] / len(sentence['tokens']),\n",
    "                triplet['objectSpan'][0] / len(sentence['tokens']),\n",
    "                tag_lemma(triplet['subjectSpan'], triplet['subject']),\n",
    "                tag_lemma(triplet['relationSpan'], triplet['relation']),\n",
    "                tag_lemma(triplet['objectSpan'], triplet['object']),\n",
    "            ) +\\\n",
    "                get_ner_occurrences(triplet['subjectSpan'], triplet['subject'], obj=False) +\\\n",
    "                get_ner_occurrences(triplet['objectSpan'], triplet['object'], obj=True) +\\\n",
    "                get_postags_sequence(triplet['subjectSpan'], triplet['subject'], cols_postag_subj) +\\\n",
    "                get_postags_sequence(triplet['objectSpan'], triplet['relation'], cols_postag_obj) +\\\n",
    "                get_postags_sequence(triplet['relationSpan'], triplet['object'], cols_postag_rel)\n",
    "            ) \n",
    "\n",
    "        return pd.DataFrame(result, columns=header)\n",
    "    \n",
    "    result = []\n",
    "    for sentence in document:\n",
    "        result.append(_extract(sentence))\n",
    "    \n",
    "    return pd.concat(result)\n",
    "\n",
    "def _embed(placeholder, words):\n",
    "    for j in range(len(words)):\n",
    "        if j == len(placeholder):\n",
    "            break\n",
    "            \n",
    "        word = words[j]\n",
    "        if word and word in word2vec_model:\n",
    "            placeholder[j, :] = word2vec_model[word]\n",
    "    return placeholder\n",
    "\n",
    "def _mark_ner_object(row):\n",
    "    return row['relation'] + (row['DATE_obj'] == 1) * ' date'\\\n",
    "                           + (row['LOCATION_obj'] == 1) * ' location'\n",
    "    \n",
    "def _extract_features(document):\n",
    "    features = _extract_plain_features(document[1])\n",
    "    features.insert(loc=0, column='docid', value=document[0])\n",
    "    max_len = {'obj': 3, 'rel': 3, 'subj': 3}\n",
    "    features['w2v_subj'] = features['lemma_subj'].map(lambda words: _embed(np.zeros((max_len['subj'], word2vec_vector_length)), words))\n",
    "    features['w2v_rel'] = features['lemma_rel'].map(lambda words: _embed(np.zeros((max_len['rel'], word2vec_vector_length)), words))\n",
    "    features['w2v_obj'] = features['lemma_obj'].map(lambda words: _embed(np.zeros((max_len['obj'], word2vec_vector_length)), words))\n",
    "                                                                  \n",
    "    return features\n",
    "\n",
    "def remove_repetitions(annot):\n",
    "    for i in range(len(annot)):\n",
    "        for j in range(len(annot[i])):\n",
    "            annot[i][j]['openie'] = list(unique_everseen(annot[i][j]['openie']))\n",
    "    return annot\n",
    "\n",
    "                                           \n",
    "class FeaturesProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pool = multiprocessing.Pool(processes=4)\n",
    "                 \n",
    "    def __call__(self, data):\n",
    "        features = pd.concat(self.pool.map(_extract_features, data))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "#DATA_PATH = 'data/corenlp_annotations_ner_pairs'  #'data/filtered_annotations'\n",
    "DATA_PATH = 'data/corenlp_annotations_only_ner'\n",
    "RESULT_PATH = 'data/processed'\n",
    "! mkdir $RESULT_PATH \n",
    "result = []\n",
    "extr = FeaturesProcessor()\n",
    "\n",
    "for file in tqdm(glob(DATA_PATH + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp = tmp[tmp.loc[:, 1].map(len) > 0]\n",
    "    tmp[1] = remove_repetitions(tmp[1].values)\n",
    "    result = extr(tmp[[0, 1]].values)\n",
    "    result.to_pickle(file.replace(DATA_PATH, RESULT_PATH).replace('.json', '.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect to one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv processed_ data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/processed'\n",
    "\n",
    "result = []\n",
    "for file in tqdm(glob(DATA_PATH + '/*.pkl')):\n",
    "    result.append(pd.read_pickle(file))\n",
    "    \n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.to_pickle('data/dataset_ner.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW for relation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_rel = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "bow_rel = vectorizer_rel.fit_transform(result['relation'].values).toarray()\n",
    "columns = [column + '_bow_rel' for column in vectorizer_rel.get_feature_names()]\n",
    "features = pd.concat([result.reset_index(drop=True), pd.DataFrame(bow_rel, columns=columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer_rel, open('models/relation_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW for synt dependency path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_path = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "bow_path = vectorizer_path.fit_transform(features['dep_path'].values).toarray()\n",
    "columns = [column + '_bow_path' for column in vectorizer_path.get_feature_names()]\n",
    "features = pd.concat([features.reset_index(drop=True), pd.DataFrame(bow_path, columns=columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer_path, open('models/path_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple clustering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = pd.read_pickle('data/dataset_ner.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "excluding_cols = ['docid', 'subject', 'relation', 'object', 'dep_path', 'lemma_subj', 'lemma_rel', 'lemma_obj']\n",
    "embedding_cols = ['w2v_subj', 'w2v_rel', 'w2v_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=100, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans=KMeans(n_clusters=100)\n",
    "kmeans.fit(X.drop(columns=embedding_cols+excluding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return X[kmeans.labels_ == number][['subject', 'relation', 'object']].sample(frac=1).iloc[:20].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Ontario Telepresence Project', 'Located at',\n",
       "        'University of Toronto'],\n",
       "       ['W. Ford Doolittle', 'However argued in', 'early 1980s'],\n",
       "       ['New York Times', 'is In', 'late November 2005'],\n",
       "       ['Ryan Cooper', 'Writing in', 'Week'],\n",
       "       ['Nakamura', 'Born in', 'Japan'],\n",
       "       ['ACTA', 'is In', 'early 2012 Denis'],\n",
       "       ['Basilidians', 'taught According to', 'Clement'],\n",
       "       ['Manning', 'serving in', 'Iraq'],\n",
       "       ['James P. Hanigan', 'Writing in', 'America'],\n",
       "       ['Apple', 'later announced on', 'July 17'],\n",
       "       ['ENTITY', 'Founded in', '2016'],\n",
       "       ['Ohmae', 'Born in', 'Kitakyūshū'],\n",
       "       ['Horstman', 'living with', 'Portuguese'],\n",
       "       ['first U.S. president', 'is In', '1965'],\n",
       "       ['it', 'Aggregating', 'Metacritic'],\n",
       "       ['Ladopoulos', 'Later worked as', 'consultant'],\n",
       "       ['Tesla', 'Named after', 'engineer Nikola Tesla'],\n",
       "       ['Rintaro', 'Returning to', 'past'],\n",
       "       ['Pokémon Masters', 'Developed by', 'DeNA'],\n",
       "       ['journalist Ben Hammersley', 'Writing for', 'Guardian']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_cluster_sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple_clusters.txt', 'w') as f:\n",
    "    for i in range(19):\n",
    "        f.write(f'{i}-----------------\\n{show_cluster_sample(i)}\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

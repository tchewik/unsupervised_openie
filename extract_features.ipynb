{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature rich dataset ``data/dataset.pkl`` out of corenlp annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (3.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'GoogleNews-vectors-negative300.bin.gz'  # 1.6G\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://s3.amazonaws.com/dl4j-distribution/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                       binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iteration_utilities in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (0.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install iteration_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "       'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "       'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "       'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "       'MONEY', 'HANDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from iteration_utilities import unique_everseen\n",
    "\n",
    "\n",
    "def _extract_plain_features(document):\n",
    "    \n",
    "    def _extract(sentence):\n",
    "        \n",
    "        def get_postags_sequence(span, words):\n",
    "            columns = ['JJ', 'CD', 'VBD', '', 'RB', 'VBN', 'PRP', 'IN', 'VBP', 'TO', 'NNP', 'VB', \n",
    "                       'VBZ', 'VBG', 'POS', 'NNS', 'NN', 'MD']\n",
    "        \n",
    "            sequence = [token['pos'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                         if token['originalText'] in words][:3]\n",
    "\n",
    "            sequence = [[int(column == postag) for column in columns] for postag in sequence]\n",
    "            \n",
    "            result = np.zeros((3, len(columns)))\n",
    "            \n",
    "            if sequence:\n",
    "                result[:len(sequence)] = sequence\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def get_ner_occurrences(span, words, obj=True):\n",
    "            _ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "                           'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "                           'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "                           'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "                           'MONEY', 'HANDLE']\n",
    "            \n",
    "            mentions = [token['ner'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            \n",
    "            mentions = [[int(_ner_kind == mention) for _ner_kind in _ner_kinds] for mention in mentions][:3]\n",
    "            result = np.zeros((3, len(_ner_kinds)))\n",
    "            \n",
    "            if mentions:\n",
    "                result[:len(mentions)] = mentions\n",
    "\n",
    "            return result\n",
    "\n",
    "        def tag_lemma(span, words, tag=False):\n",
    "            if tag:\n",
    "                return [token['lemma'].lower() + '_' + _penn_tagset[token['pos']]['fPOS'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            else:\n",
    "                return [token['lemma'].lower() for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "\n",
    "        def remove_repetition(words):\n",
    "            if words[:len(words)//2] == words[len(words)//2:]:\n",
    "                return words[:len(words)//2]\n",
    "            return words\n",
    "        \n",
    "        def get_tokens(words, span):            \n",
    "            return [token['originalText'].lower() for token in sentence['tokens'][span[0]:span[1]]\n",
    "                                if token['originalText'] in words]\n",
    "            \n",
    "        def _build_dep_path(dependencies, tokens, start: int, end: int):\n",
    "            edges = []\n",
    "            deps = {}\n",
    "\n",
    "            for edge in dependencies:\n",
    "                edges.append((edge['governor'], edge['dependent']))\n",
    "                deps[(min(edge['governor'], edge['dependent']),\n",
    "                      max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "            graph = nx.Graph(edges)\n",
    "            path = nx.shortest_path(graph, source=start, target=end)\n",
    "            return path[:-1]  # exclude right end\n",
    "\n",
    "        def _tokens_by_index(indexes, tokens):\n",
    "            return [token['originalText'] for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        def _lemmas_by_index(indexes, tokens):\n",
    "            return [token['lemma'].lower() for token in tokens if token['index'] in indexes]\n",
    "        \n",
    "        def _embed(placeholder, words):\n",
    "            for j in range(len(words)):\n",
    "                if j == len(placeholder):\n",
    "                    break\n",
    "\n",
    "                word = words[j]\n",
    "                if word and word in word2vec_model:\n",
    "                    placeholder[j, :] = word2vec_model[word]\n",
    "            return placeholder\n",
    "\n",
    "        def _embed_arg(row):\n",
    "            result = []\n",
    "            result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "\n",
    "            return result\n",
    "\n",
    "        deprecated = set(['one', 'he', 'she', 'they', 'his', 'her', 'its', 'our', 'day', 'co.', 'inc.', \n",
    "              'society', 'people', 'inventor', 'head', 'poet', 'doctor', 'teacher', 'inventor', \n",
    "              'thanksgiving day', 'halloween',\n",
    "              'sales person', 'model', 'board', 'technology', 'owner', 'one', 'two'])\n",
    "        \n",
    "        triplets = sentence['openie']\n",
    "        #filtered_triplets = filter(lambda obj: obj['object'].lower() not in deprecated and obj['subject'].lower() not in deprecated, triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['subject'].lower() not in deprecated, triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['object']) > 2 or len(obj['subject']) > 2, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['relation'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = list(filtered_triplets)\n",
    "        \n",
    "        subjects, relations, objects, dep_path = [], [], [], []\n",
    "        \n",
    "        for triplet in filtered_triplets:\n",
    "\n",
    "            _subject = {\n",
    "                'tokens': get_tokens(triplet['subject'], triplet['subjectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['subjectSpan'], triplet['subject']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['subjectSpan'][0],\n",
    "                'rel_pos': triplet['subjectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['subjectSpan'], triplet['subject']),\n",
    "                'postag': get_postags_sequence(triplet['subjectSpan'], triplet['subject']),\n",
    "            }\n",
    "            _subject.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _subject['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _relation = {\n",
    "                'tokens': get_tokens(triplet['relation'], triplet['relationSpan']),\n",
    "                'lemmas': tag_lemma(triplet['relationSpan'], triplet['relation']),\n",
    "                'dist_to_rel': 0,\n",
    "                'rel_pos': triplet['relationSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['relationSpan'], triplet['relation']),\n",
    "                'postag': get_postags_sequence(triplet['relationSpan'], triplet['relation'])\n",
    "            }\n",
    "            _relation.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _relation['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _object = {\n",
    "                'tokens': get_tokens(triplet['object'], triplet['objectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['objectSpan'], triplet['object']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['objectSpan'][0],\n",
    "                'rel_pos': triplet['objectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['objectSpan'], triplet['object']),\n",
    "                'postag': get_postags_sequence(triplet['objectSpan'], triplet['object'])\n",
    "            }\n",
    "            _object.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _object['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _dependency_path = ' '.join(_lemmas_by_index(_build_dep_path(sentence['basicDependencies'], \n",
    "                                                          sentence['tokens'], \n",
    "                                                          triplet['subjectSpan'][0], \n",
    "                                                          triplet['objectSpan'][-1]), sentence['tokens']))\n",
    "            subjects.append(_subject)\n",
    "            relations.append(_relation)\n",
    "            objects.append(_object)\n",
    "            dep_path.append(_dependency_path)\n",
    "            \n",
    "        #return pd.DataFrame(result, columns=header)\n",
    "        return subjects, relations, objects\n",
    "    \n",
    "    subjects, relations, objects = [], [], []\n",
    "    for sentence in document:\n",
    "        _subject, _relation, _object = _extract(sentence)\n",
    "        subjects += _subject\n",
    "        relations += _relation\n",
    "        objects += _object\n",
    "\n",
    "    \n",
    "    return subjects, relations, objects\n",
    "\n",
    "def _mark_ner_object(row):\n",
    "    return row['relation'] + (row['DATE_obj'] == 1) * ' date'\\\n",
    "                           + (row['LOCATION_obj'] == 1) * ' location'\n",
    "    \n",
    "def _extract_features(document):\n",
    "    def _embed_arg(row):\n",
    "        result = []\n",
    "        result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    features = {}\n",
    "    features['subject'], features['relation'], features['object'] = _extract_plain_features(document[1])\n",
    "\n",
    "    #features.insert(loc=0, column='docid', value=document[0])\n",
    "    #max_len = {'obj': 3, 'rel': 3, 'subj': 3}\n",
    "    #features['w2v_subj'] = list(map(_embed_arg, features['subject']))\n",
    "    #features['w2v_rel'] = list(map(_embed_arg, features['relation']))\n",
    "    #features['w2v_obj'] = list(map(_embed_arg, features['object']))\n",
    "    \n",
    "    #print(features['w2v_subj'])\n",
    "    \n",
    "    #features['w2v_subj'] = features['subject'].map(lambda words: _embed(np.zeros((max_len['subj'], word2vec_vector_length)), words.lower().split()))\n",
    "    #features['w2v_rel'] = features['relation'].map(lambda words: _embed(np.zeros((max_len['rel'], word2vec_vector_length)), words.lower().split()))\n",
    "    #features['w2v_obj'] = features['object'].map(lambda words: _embed(np.zeros((max_len['obj'], word2vec_vector_length)), words.lower().split()))\n",
    "       \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def remove_repetitions(annot):\n",
    "    for i in range(len(annot)):\n",
    "        for j in range(len(annot[i])):\n",
    "            annot[i][j]['openie'] = list(unique_everseen(annot[i][j]['openie']))\n",
    "    return annot\n",
    "\n",
    "                                           \n",
    "class FeaturesProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pool = multiprocessing.Pool(processes=1)\n",
    "                 \n",
    "    def __call__(self, data):\n",
    "        features = pd.concat(self.pool.map(_extract_features, data))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/processed_separately’: File exists\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46326c868664aa8869b47f758a3ae11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "#DATA_PATH = 'data/corenlp_annotations_ner_pairs'  #'data/filtered_annotations'\n",
    "DATA_PATH = 'data/corenlp_annotations_only_ner'\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "! mkdir $RESULT_PATH \n",
    "result = []\n",
    "extr = FeaturesProcessor()\n",
    "\n",
    "def extract_matrix(row):\n",
    "    _matrix = np.concatenate([row['ner'], row['postag'], row['w2v'], np.array([[row['dist_to_rel'], row['rel_pos']]] * 3)], axis=1)\n",
    "    return _matrix\n",
    "\n",
    "for file in tqdm(glob(DATA_PATH + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp = tmp[tmp.loc[:, 1].map(len) > 0]\n",
    "    tmp[1] = remove_repetitions(tmp[1].values)\n",
    "    result = extr(tmp[[0, 1]].values)\n",
    "    result['subject_matr'] = result.subject.map(extract_matrix)\n",
    "    result['object_matr'] = result.object.map(extract_matrix)\n",
    "    result['relation_matr'] = result.relation.map(extract_matrix)\n",
    "    result.to_pickle(file.replace(DATA_PATH, RESULT_PATH).replace('.json', '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>subject_matr</th>\n",
       "      <th>object_matr</th>\n",
       "      <th>relation_matr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...</td>\n",
       "      <td>{'tokens': ['is'], 'lemmas': ['be'], 'dist_to_...</td>\n",
       "      <td>{'tokens': ['american'], 'lemmas': ['american'...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...</td>\n",
       "      <td>{'tokens': ['was'], 'lemmas': ['be'], 'dist_to...</td>\n",
       "      <td>{'tokens': ['old'], 'lemmas': ['old'], 'dist_t...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...</td>\n",
       "      <td>{'tokens': ['graduated'], 'lemmas': ['graduate...</td>\n",
       "      <td>{'tokens': ['1985'], 'lemmas': ['1985'], 'dist...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...</td>\n",
       "      <td>{'tokens': ['lived', 'in'], 'lemmas': ['live',...</td>\n",
       "      <td>{'tokens': ['tennessee'], 'lemmas': ['tennesse...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tokens': ['radical', 'entertainment', 'inc.'...</td>\n",
       "      <td>{'tokens': ['is'], 'lemmas': ['be'], 'dist_to_...</td>\n",
       "      <td>{'tokens': ['video', 'game', 'developer'], 'le...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0  {'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...   \n",
       "1  {'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...   \n",
       "2  {'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...   \n",
       "3  {'tokens': ['eisen'], 'lemmas': ['eisen'], 'di...   \n",
       "0  {'tokens': ['radical', 'entertainment', 'inc.'...   \n",
       "\n",
       "                                            relation  \\\n",
       "0  {'tokens': ['is'], 'lemmas': ['be'], 'dist_to_...   \n",
       "1  {'tokens': ['was'], 'lemmas': ['be'], 'dist_to...   \n",
       "2  {'tokens': ['graduated'], 'lemmas': ['graduate...   \n",
       "3  {'tokens': ['lived', 'in'], 'lemmas': ['live',...   \n",
       "0  {'tokens': ['is'], 'lemmas': ['be'], 'dist_to_...   \n",
       "\n",
       "                                              object  \\\n",
       "0  {'tokens': ['american'], 'lemmas': ['american'...   \n",
       "1  {'tokens': ['old'], 'lemmas': ['old'], 'dist_t...   \n",
       "2  {'tokens': ['1985'], 'lemmas': ['1985'], 'dist...   \n",
       "3  {'tokens': ['tennessee'], 'lemmas': ['tennesse...   \n",
       "0  {'tokens': ['video', 'game', 'developer'], 'le...   \n",
       "\n",
       "                                        subject_matr  \\\n",
       "0  [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                         object_matr  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "0  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                       relation_matr  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect to one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'processed_': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! mv processed_ data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 687M\r\n",
      "drwxr-xr-x 10 root root 4.0K Oct 22 14:37 ..\r\n",
      "-rw-r--r--  1 root root 4.1M Oct  1 12:11 it_wiki_part_27.pkl\r\n",
      "-rw-r--r--  1 root root 9.8M Oct  1 12:11 it_wiki_part_72.pkl\r\n",
      "-rw-r--r--  1 root root  24M Oct  1 12:11 it_wiki_part_2.pkl\r\n",
      "-rw-r--r--  1 root root 7.5M Oct  1 12:11 it_wiki_part_57.pkl\r\n",
      "-rw-r--r--  1 root root 6.1M Oct  1 12:11 it_wiki_part_73.pkl\r\n",
      "-rw-r--r--  1 root root 3.4M Oct  1 12:11 it_wiki_part_40.pkl\r\n",
      "-rw-r--r--  1 root root 7.0M Oct  1 12:11 it_wiki_part_37.pkl\r\n",
      "-rw-r--r--  1 root root 9.3M Oct  1 12:11 it_wiki_part_50.pkl\r\n",
      "-rw-r--r--  1 root root 7.5M Oct  1 12:11 it_wiki_part_81.pkl\r\n",
      "-rw-r--r--  1 root root 6.0M Oct  1 12:10 it_wiki_part_79.pkl\r\n",
      "-rw-r--r--  1 root root 7.9M Oct  1 12:10 it_wiki_part_47.pkl\r\n",
      "-rw-r--r--  1 root root 7.0M Oct  1 12:10 it_wiki_part_35.pkl\r\n",
      "-rw-r--r--  1 root root 6.5M Oct  1 12:10 it_wiki_part_70.pkl\r\n",
      "-rw-r--r--  1 root root  14M Oct  1 12:10 it_wiki_part_76.pkl\r\n",
      "-rw-r--r--  1 root root 6.1M Oct  1 12:10 it_wiki_part_25.pkl\r\n",
      "-rw-r--r--  1 root root 5.7M Oct  1 12:10 it_wiki_part_38.pkl\r\n",
      "-rw-r--r--  1 root root 4.1M Oct  1 12:10 it_wiki_part_32.pkl\r\n",
      "-rw-r--r--  1 root root 7.5M Oct  1 12:10 it_wiki_part_69.pkl\r\n",
      "-rw-r--r--  1 root root  12M Oct  1 12:10 it_wiki_part_3.pkl\r\n",
      "-rw-r--r--  1 root root 7.8M Oct  1 12:10 it_wiki_part_49.pkl\r\n",
      "-rw-r--r--  1 root root 7.0M Oct  1 12:10 it_wiki_part_88.pkl\r\n",
      "-rw-r--r--  1 root root 5.5M Oct  1 12:10 it_wiki_part_84.pkl\r\n",
      "-rw-r--r--  1 root root 7.1M Oct  1 12:10 it_wiki_part_29.pkl\r\n",
      "-rw-r--r--  1 root root  11M Oct  1 12:10 it_wiki_part_8.pkl\r\n",
      "-rw-r--r--  1 root root 6.0M Oct  1 12:10 it_wiki_part_36.pkl\r\n",
      "-rw-r--r--  1 root root 6.8M Oct  1 12:10 it_wiki_part_71.pkl\r\n",
      "-rw-r--r--  1 root root 7.8M Oct  1 12:10 it_wiki_part_53.pkl\r\n",
      "-rw-r--r--  1 root root 6.1M Oct  1 12:10 it_wiki_part_18.pkl\r\n",
      "-rw-r--r--  1 root root 7.6M Oct  1 12:10 it_wiki_part_85.pkl\r\n",
      "-rw-r--r--  1 root root 7.1M Oct  1 12:10 it_wiki_part_13.pkl\r\n",
      "-rw-r--r--  1 root root 8.7M Oct  1 12:10 it_wiki_part_15.pkl\r\n",
      "-rw-r--r--  1 root root  23M Oct  1 12:10 it_wiki_part_1.pkl\r\n",
      "-rw-r--r--  1 root root 6.7M Oct  1 12:10 it_wiki_part_61.pkl\r\n",
      "-rw-r--r--  1 root root 9.3M Oct  1 12:10 it_wiki_part_12.pkl\r\n",
      "-rw-r--r--  1 root root 5.7M Oct  1 12:10 it_wiki_part_20.pkl\r\n",
      "-rw-r--r--  1 root root 3.4M Oct  1 12:10 it_wiki_part_43.pkl\r\n",
      "-rw-r--r--  1 root root 5.1M Oct  1 12:10 it_wiki_part_44.pkl\r\n",
      "-rw-r--r--  1 root root 3.8M Oct  1 12:10 it_wiki_part_42.pkl\r\n",
      "-rw-r--r--  1 root root 6.8M Oct  1 12:10 it_wiki_part_67.pkl\r\n",
      "-rw-r--r--  1 root root 8.5M Oct  1 12:10 it_wiki_part_26.pkl\r\n",
      "-rw-r--r--  1 root root 7.4M Oct  1 12:10 it_wiki_part_63.pkl\r\n",
      "-rw-r--r--  1 root root 9.2M Oct  1 12:10 it_wiki_part_77.pkl\r\n",
      "-rw-r--r--  1 root root  15M Oct  1 12:10 it_wiki_part_7.pkl\r\n",
      "-rw-r--r--  1 root root 1.5M Oct  1 12:10 it_wiki_part_0.pkl\r\n",
      "-rw-r--r--  1 root root 8.3M Oct  1 12:10 it_wiki_part_60.pkl\r\n",
      "-rw-r--r--  1 root root 6.8M Oct  1 12:09 it_wiki_part_34.pkl\r\n",
      "-rw-r--r--  1 root root 8.5M Oct  1 12:09 it_wiki_part_66.pkl\r\n",
      "-rw-r--r--  1 root root 4.2M Oct  1 12:09 it_wiki_part_82.pkl\r\n",
      "-rw-r--r--  1 root root 6.6M Oct  1 12:09 it_wiki_part_83.pkl\r\n",
      "-rw-r--r--  1 root root 5.5M Oct  1 12:09 it_wiki_part_23.pkl\r\n",
      "-rw-r--r--  1 root root 7.3M Oct  1 12:09 it_wiki_part_30.pkl\r\n",
      "-rw-r--r--  1 root root 5.5M Oct  1 12:09 it_wiki_part_55.pkl\r\n",
      "-rw-r--r--  1 root root 6.4M Oct  1 12:09 it_wiki_part_62.pkl\r\n",
      "-rw-r--r--  1 root root 4.8M Oct  1 12:09 it_wiki_part_78.pkl\r\n",
      "-rw-r--r--  1 root root 8.1M Oct  1 12:09 it_wiki_part_90.pkl\r\n",
      "-rw-r--r--  1 root root 9.8M Oct  1 12:09 it_wiki_part_10.pkl\r\n",
      "-rw-r--r--  1 root root 5.3M Oct  1 12:09 it_wiki_part_74.pkl\r\n",
      "-rw-r--r--  1 root root 4.9M Oct  1 12:09 it_wiki_part_56.pkl\r\n",
      "-rw-r--r--  1 root root 1.3M Oct  1 12:09 it_wiki_part_91.pkl\r\n",
      "-rw-r--r--  1 root root  12M Oct  1 12:09 it_wiki_part_31.pkl\r\n",
      "-rw-r--r--  1 root root 5.7M Oct  1 12:09 it_wiki_part_54.pkl\r\n",
      "-rw-r--r--  1 root root 7.8M Oct  1 12:09 it_wiki_part_39.pkl\r\n",
      "-rw-r--r--  1 root root 6.2M Oct  1 12:09 it_wiki_part_86.pkl\r\n",
      "-rw-r--r--  1 root root  13M Oct  1 12:09 it_wiki_part_4.pkl\r\n",
      "-rw-r--r--  1 root root 6.4M Oct  1 12:09 it_wiki_part_16.pkl\r\n",
      "-rw-r--r--  1 root root 8.1M Oct  1 12:09 it_wiki_part_11.pkl\r\n",
      "-rw-r--r--  1 root root 9.0M Oct  1 12:09 it_wiki_part_45.pkl\r\n",
      "-rw-r--r--  1 root root 3.8M Oct  1 12:09 it_wiki_part_48.pkl\r\n",
      "-rw-r--r--  1 root root 6.1M Oct  1 12:09 it_wiki_part_87.pkl\r\n",
      "-rw-r--r--  1 root root 5.2M Oct  1 12:09 it_wiki_part_65.pkl\r\n",
      "-rw-r--r--  1 root root 6.2M Oct  1 12:09 it_wiki_part_68.pkl\r\n",
      "-rw-r--r--  1 root root  13M Oct  1 12:09 it_wiki_part_6.pkl\r\n",
      "-rw-r--r--  1 root root 6.0M Oct  1 12:09 it_wiki_part_24.pkl\r\n",
      "-rw-r--r--  1 root root 6.3M Oct  1 12:09 it_wiki_part_46.pkl\r\n",
      "-rw-r--r--  1 root root 7.6M Oct  1 12:09 it_wiki_part_17.pkl\r\n",
      "-rw-r--r--  1 root root 5.2M Oct  1 12:09 it_wiki_part_19.pkl\r\n",
      "-rw-r--r--  1 root root  16M Oct  1 12:09 it_wiki_part_5.pkl\r\n",
      "-rw-r--r--  1 root root 9.2M Oct  1 12:09 it_wiki_part_89.pkl\r\n",
      "-rw-r--r--  1 root root 7.8M Oct  1 12:09 it_wiki_part_52.pkl\r\n",
      "-rw-r--r--  1 root root 7.0M Oct  1 12:09 it_wiki_part_41.pkl\r\n",
      "-rw-r--r--  1 root root 7.7M Oct  1 12:09 it_wiki_part_51.pkl\r\n",
      "-rw-r--r--  1 root root 6.4M Oct  1 12:09 it_wiki_part_64.pkl\r\n",
      "-rw-r--r--  1 root root 5.3M Oct  1 12:09 it_wiki_part_21.pkl\r\n",
      "-rw-r--r--  1 root root 6.0M Oct  1 12:09 it_wiki_part_14.pkl\r\n",
      "-rw-r--r--  1 root root 7.6M Oct  1 12:09 it_wiki_part_58.pkl\r\n",
      "-rw-r--r--  1 root root 9.5M Oct  1 12:09 it_wiki_part_59.pkl\r\n",
      "-rw-r--r--  1 root root 6.3M Oct  1 12:08 it_wiki_part_80.pkl\r\n",
      "-rw-r--r--  1 root root 6.2M Oct  1 12:08 it_wiki_part_75.pkl\r\n",
      "-rw-r--r--  1 root root 7.2M Oct  1 12:08 it_wiki_part_33.pkl\r\n",
      "-rw-r--r--  1 root root 9.8M Oct  1 12:08 it_wiki_part_28.pkl\r\n",
      "-rw-r--r--  1 root root 8.6M Oct  1 12:08 it_wiki_part_9.pkl\r\n",
      "-rw-r--r--  1 root root 5.0M Oct  1 12:08 it_wiki_part_22.pkl\r\n",
      "drwxr-xr-x  2 root root 4.0K Sep 14 17:11 .\r\n"
     ]
    }
   ],
   "source": [
    "! ls -laht data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56988d43c5e4b1f8007ba717984b548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/processed_separately'\n",
    "\n",
    "result = []\n",
    "for file in tqdm(glob(DATA_PATH + '/*.pkl')):\n",
    "    result.append(pd.read_pickle(file))\n",
    "    \n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32419, 6)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

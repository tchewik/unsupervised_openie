{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset cuda\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "data = []\n",
    "\n",
    "for file in tqdm(glob(RESULT_PATH + '/*.pkl')):\n",
    "    data.append(pd.read_pickle(file))\n",
    "    \n",
    "data = pd.concat(data).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)\n",
    "data = data.drop_duplicates(['_subject', '_relation', '_object'])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data._relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def extract_matrix(row, predicate=False):\n",
    "    _matrix = np.concatenate([row['ner'], row['postag']], axis=1)#.flatten()\n",
    "    if predicate:\n",
    "        _matrix = np.concatenate([_matrix, row['w2v'], [row['prep'], row['prep'], row['prep']]], axis=1)#.flatten()\n",
    "    return _matrix\n",
    "\n",
    "data['object_matr'] = data.object.progress_map(extract_matrix)\n",
    "data['subject_matr'] = data.subject.progress_map(extract_matrix)\n",
    "data['relation_matr'] = data.relation.progress_map(lambda row: extract_matrix(row, predicate=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.object_matr.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object, _subject, _relation = data.object_matr.values, data.subject_matr.values, data.relation_matr.values\n",
    "_object = np.stack(_object)\n",
    "_subject = np.stack(_subject)\n",
    "_relation = np.stack(_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from autoencoder_models import Mish, mish\n",
    "\n",
    "get_custom_objects().update({'mish': mish})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape=[_subject.shape[1:], _object.shape[1:], _relation.shape[1:]]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder_models import noised_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = noised_ae(input_shape=input_shape)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(x=[_subject, _object, _relation],\n",
    "          y=[_subject, _object, _relation], epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import deep_clustering\n",
    "\n",
    "save_dir = 'models/idec/noised_ae'\n",
    "directory = os.path.dirname(save_dir)\n",
    "if not Path(directory).is_dir():\n",
    "    ! mkdir $save_dir\n",
    "\n",
    "def train_idec(autoencoder, n_clusters, score_threshold=1e-5, save_dir=save_dir, partial_init=False):\n",
    "    _directory = save_dir + f'/idec_{n_clusters}_partial{str(partial_init)}'\n",
    "    ! mkdir $_directory\n",
    "    idec = deep_clustering.IDEC(input_shape=input_shape,\n",
    "                                autoencoder_ctor=lambda input_shape: autoencoder(input_shape),  # select model here\n",
    "                                n_clusters=n_clusters,\n",
    "                                pretrain_epochs=50,\n",
    "                                max_iter=300,\n",
    "                                partial_init=partial_init,\n",
    "                                save_dir=_directory, \n",
    "                                log_dir=logPath)\n",
    "\n",
    "    idec.compile(optimizer='adam')\n",
    "    plot_model(idec._model, to_file='daec_model.png', show_shapes=True)\n",
    "    idec.fit([_subject, _object, _relation])\n",
    "\n",
    "    # dump data somewhere\n",
    "    y_pred = idec._y_pred\n",
    "    dumb_features = data[['_subject', '_relation', '_object']]\n",
    "    dumb_features['cluster'] = y_pred\n",
    "    scores = idec.score_examples([_subject, _object, _relation])\n",
    "    dumb_features['score'] = scores\n",
    "    dumb_features = dumb_features[dumb_features['score'] > score_threshold]\n",
    "    dumb_features.to_pickle(f'clusterized/idec_clusters_{n_clusters}_partial{str(partial_init)}.pkl')\n",
    "    \n",
    "    return idec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idec32 = train_idec(autoencoder=noised_ae, n_clusters=32, partial_init=False)\n",
    "idec32 = train_idec(autoencoder=noised_ae, n_clusters=32, partial_init=True)\n",
    "idec40 = train_idec(autoencoder=noised_ae, n_clusters=40, partial_init=False)\n",
    "idec40 = train_idec(autoencoder=noised_ae, n_clusters=40, partial_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "idec64 = train_idec(autoencoder=noised_ae, n_clusters=40, partial_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the results data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features = pd.read_pickle('clusterized/idec_clusters_40_partialTrue2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['_subject', '_relation', '_object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ax = sns.countplot(dumb_features.cluster, order=dumb_features.cluster.value_counts().index, color='green')\n",
    "ax.set(xlabel='cluster', ylabel='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features._relation.str.contains('died')]['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sorted(dumb_features.cluster.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 28\n",
    "temp = show_cluster_sample(number, rows=5000)\n",
    "temp._relation.value_counts()\n",
    "#temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply an IDEC model to the QA corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the features collected in ``1_parse_data_SimpleQuestions.ipynb``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "\n",
    "for part in ['train', 'valid', 'test']:\n",
    "    data[part] = dict()\n",
    "    \n",
    "    for name in ['object', 'subject', 'relation']:\n",
    "        path = \"../uopenie_qa/SimpleWikidataQuestions/csv decoded/\"\n",
    "        path += f\"annotated_wd_data_{part}_answerable_decoded_{name}_features.npy\"\n",
    "        \n",
    "        data[part][name] = np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved IDEC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from autoencoder_models import Mish, mish\n",
    "import deep_clustering\n",
    "\n",
    "get_custom_objects().update({'mish': mish})\n",
    "\n",
    "\n",
    "def load_idec(path):\n",
    "    # information about clusters number and partial init option is in the path name\n",
    "    n_clusters = int(re.findall(r'idec_(\\d+)', path)[0])\n",
    "    partial_init = 'partialTrue' in path\n",
    "    \n",
    "    # load pretrained autoencoder\n",
    "    aec = models.load_model(path.replace(\"dcec_model_final\", \"pretrain_cae_model\"))\n",
    "    \n",
    "    # load pretrained IDEC\n",
    "    model = deep_clustering.IDEC(input_shape=input_shape,\n",
    "                                autoencoder_ctor=aec,\n",
    "                                pretrained=True,\n",
    "                                n_clusters=n_clusters,\n",
    "                                pretrain_epochs=20,\n",
    "                                max_iter=300,\n",
    "                                partial_init=partial_init,\n",
    "                                save_dir=path[:path.rfind('/')], \n",
    "                                log_dir=logPath)\n",
    "\n",
    "    model.compile(optimizer='adam')\n",
    "    model.load_weights(path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '40_partialTrue2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_idec = load_idec(f\"models/idec/noised_ae/idec_{model_name}/dcec_model_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "saved_idec.predict([_subject[:n], _object[:n], _relation[:n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict clusters for QA data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_predictions = dict()\n",
    "\n",
    "for part in ['train', 'valid', 'test']:\n",
    "    qa_predictions[part] = saved_idec.predict([\n",
    "        data[part]['subject'],\n",
    "        data[part]['object'],\n",
    "        data[part]['relation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run baseline classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = {}\n",
    "\n",
    "for part in [\"train\", \"valid\", \"test\"]:\n",
    "    path = f\"../uopenie_qa/SimpleWikidataQuestions/csv decoded/annotated_wd_data_{part}_answerable_decoded.csv\"\n",
    "    qa_data[part] = pd.read_csv(path).drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "    qa_data[part]['class'] = qa_predictions[part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_data['test'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # 40_partialTrue2\n",
    "    qa_data['train'].to_csv('simplequestions_train_classified.csv')\n",
    "    qa_data['valid'].to_csv('simplequestions_valid_classified.csv')\n",
    "    qa_data['test'].to_csv('simplequestions_test_classified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from question_classifiers import FastTextClassifier\n",
    "\n",
    "clf = FastTextClassifier()\n",
    "clf.train(qa_data['train']['question'].values, qa_data['train']['class'].values,\n",
    "          qa_data['valid']['question'].values, qa_data['valid']['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save(f\"fasttext_clf_{model_name}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from question_classifiers import FastTextClassifier\n",
    "\n",
    "model_name = '40_partialTrue'\n",
    "clf = FastTextClassifier(path=f\"fasttext_clf_{model_name}.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, proba = clf.predict(qa_data['valid']['question'].values)\n",
    "\n",
    "clf.evaluate(qa_data['valid']['class'].values, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, proba = clf.predict(qa_data['test']['question'].values)\n",
    "\n",
    "clf.evaluate(qa_data['test']['class'].values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data['test'][qa_data['test']['class'] == 4].property_decoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/idec/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dirty_json('dcec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other autoencoder architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DAEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/daec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/daec/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "daec = deep_clustering.DAEC(input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=50, \n",
    "                            pretrain_epochs=10,\n",
    "                            log_dir=logPath,\n",
    "                            save_dir=save_dir, \n",
    "                            )\n",
    "\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'daec_model.png'), show_shapes=True)\n",
    "daec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = daec._y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dcec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.01\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/daec/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = save_dirty_json('daec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC_Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/dc_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/dc_kmeans/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "dckmeans = deep_clustering.DC_Kmeans(\n",
    "                            input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=30,\n",
    "                            pretrain_epochs=50,\n",
    "                            max_epochs=200,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dckmeans._model, to_file=os.path.join(save_dir, 'dckmeans_model.png'), show_shapes=True)\n",
    "dckmeans.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = dckmeans.y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dckmeans.get_scores([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/dc_kmeans/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = save_dirty_json('dc_kmeans_30c_000.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of internal representations generated by autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pae = plain_ae(x_train.shape[1:])\n",
    "pae.compile(optimizer='adam', loss='mse')\n",
    "pae.fit(x_train, x_train, batch_size=256, epochs=10, verbose=0)\n",
    "hidden = pae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=pae.input, outputs=hidden)\n",
    "#embeddings = encoder.predict(x_train)\n",
    "#cluzeriser = KMeans(2, n_jobs=6)\n",
    "#clusters = cluzeriser.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pae.save('models/pae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return features[clusters == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pae_clusters.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

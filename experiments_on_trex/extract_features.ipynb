{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature rich dataset ``data/dataset.pkl`` out of corenlp annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'GoogleNews-vectors-negative300.bin.gz'  # 1.6G\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://s3.amazonaws.com/dl4j-distribution/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                       binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install iteration_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "       'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "       'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "       'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "       'MONEY', 'HANDLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from iteration_utilities import unique_everseen\n",
    "\n",
    "\n",
    "def _extract_plain_features(document):\n",
    "    \n",
    "    def _extract(sentence):\n",
    "        \n",
    "        def get_postags_sequence(span, words, predicate=False):\n",
    "            columns = ['JJ', 'CD', 'VBD', '', 'RB', 'VBN', 'PRP', 'IN', 'VBP', 'TO', 'NNP', 'VB', \n",
    "                       'VBZ', 'VBG', 'POS', 'NNS', 'NN', 'MD']\n",
    "        \n",
    "            sequence = [token['pos'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                         if token['originalText'] in words][:3]\n",
    "            \n",
    "            if predicate or 'NNP' in set(sequence) or 'CD' in set(sequence):\n",
    "                sequence = [[int(column == postag) for column in columns] for postag in sequence]\n",
    "            else:\n",
    "                sequence = []\n",
    "            \n",
    "            result = np.zeros((3, len(columns)))\n",
    "            \n",
    "            if sequence:\n",
    "                result[:len(sequence)] = sequence\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def get_ner_occurrences(span, words, obj=True):\n",
    "            _ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "                           'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "                           'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "                           'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "                           'MONEY', 'HANDLE']\n",
    "            \n",
    "            mentions = [token['ner'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            \n",
    "            mentions = [[int(_ner_kind == mention) for _ner_kind in _ner_kinds] for mention in mentions][:3]\n",
    "            result = np.zeros((3, len(_ner_kinds)))\n",
    "            \n",
    "            if mentions:\n",
    "                result[:len(mentions)] = mentions\n",
    "\n",
    "            return result\n",
    "\n",
    "        def tag_lemma(span, words, tag=False):\n",
    "            if tag:\n",
    "                return [token['lemma'].lower() + '_' + _penn_tagset[token['pos']]['fPOS'] for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "            else:\n",
    "                return [token['lemma'].lower() for token in sentence['tokens'][span[0]:span[1]] \n",
    "                                 if token['originalText'] in words]\n",
    "\n",
    "        def remove_repetition(words):\n",
    "            if words[:len(words)//2] == words[len(words)//2:]:\n",
    "                return words[:len(words)//2]\n",
    "            return words\n",
    "        \n",
    "        def get_tokens(words, span):            \n",
    "            return [token['originalText'].lower() for token in sentence['tokens'][span[0]:span[1]]\n",
    "                                if token['originalText'] in words]\n",
    "            \n",
    "        def _build_dep_path(dependencies, tokens, start: int, end: int):\n",
    "            edges = []\n",
    "            deps = {}\n",
    "\n",
    "            for edge in dependencies:\n",
    "                edges.append((edge['governor'], edge['dependent']))\n",
    "                deps[(min(edge['governor'], edge['dependent']),\n",
    "                      max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "            graph = nx.Graph(edges)\n",
    "            path = nx.shortest_path(graph, source=start, target=end)\n",
    "            return path[:-1]  # exclude right end\n",
    "\n",
    "        def _tokens_by_index(indexes, tokens):\n",
    "            return [token['originalText'] for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        def _lemmas_by_index(indexes, tokens):\n",
    "            return [token['lemma'].lower() for token in tokens if token['index'] in indexes]\n",
    "        \n",
    "        def _embed(placeholder, words):\n",
    "            for j in range(len(words)):\n",
    "                if j == len(placeholder):\n",
    "                    break\n",
    "\n",
    "                word = words[j]\n",
    "                if word and word in word2vec_model:\n",
    "                    placeholder[j, :] = word2vec_model[word]\n",
    "            return placeholder\n",
    "\n",
    "        def _embed_arg(row):\n",
    "            result = []\n",
    "            result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "\n",
    "            return result\n",
    "\n",
    "        deprecated = set(['one', 'he', 'she', 'they', 'his', 'her', 'its', 'our', 'day', 'co.', 'inc.', \n",
    "              'society', 'people', 'inventor', 'head', 'poet', 'doctor', 'teacher', 'inventor', \n",
    "              'thanksgiving day', 'halloween',\n",
    "              'sales person', 'model', 'board', 'technology', 'owner', 'one', 'two', 'university', \n",
    "                          'fbi', 'patricia churchland', 'century', 'association', 'laboratory', 'academy'])\n",
    "        deprec_rels = {'in', 'is', 'was', 'of', \"'s\", 'to', 'for', 'by', 'with', 'also', 'as of',\n",
    "                       'had', 'said', 'said in', 'felt', 'on', 'gave', 'saw', 'found', 'did'}\n",
    "        \n",
    "        triplets = sentence['openie']\n",
    "        #filtered_triplets = filter(lambda obj: obj['object'].lower() not in deprecated and obj['subject'].lower() not in deprecated, triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['subject'].lower().strip() not in deprecated, triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['object'].lower().strip() not in deprecated, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['relation'].lower().strip() not in deprec_rels, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['object']) > 2 and len(obj['subject']) > 2 and len(obj['relation']) > 2, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['relation'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = list(filtered_triplets)\n",
    "        \n",
    "        subjects, relations, objects, dep_path = [], [], [], []\n",
    "        \n",
    "        for triplet in filtered_triplets:\n",
    "\n",
    "            _subject = {\n",
    "                'tokens': get_tokens(triplet['subject'], triplet['subjectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['subjectSpan'], triplet['subject']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['subjectSpan'][0],\n",
    "                'rel_pos': triplet['subjectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['subjectSpan'], triplet['subject']),\n",
    "                'postag': get_postags_sequence(triplet['subjectSpan'], triplet['subject'], predicate=False),\n",
    "            }\n",
    "            _subject.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _subject['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _relation = {\n",
    "                'tokens': get_tokens(triplet['relation'], triplet['relationSpan']),\n",
    "                'lemmas': tag_lemma(triplet['relationSpan'], triplet['relation']),\n",
    "                'dist_to_rel': 0,\n",
    "                'rel_pos': triplet['relationSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['relationSpan'], triplet['relation']),\n",
    "                'postag': get_postags_sequence(triplet['relationSpan'], triplet['relation'], predicate=True),\n",
    "            }\n",
    "            _relation.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _relation['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _object = {\n",
    "                'tokens': get_tokens(triplet['object'], triplet['objectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['objectSpan'], triplet['object']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['objectSpan'][0],\n",
    "                'rel_pos': triplet['objectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['objectSpan'], triplet['object']),\n",
    "                'postag': get_postags_sequence(triplet['objectSpan'], triplet['object'], predicate=False),\n",
    "            }\n",
    "            _object.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _object['lemmas']),\n",
    "            })\n",
    "            \n",
    "            _dependency_path = ' '.join(_lemmas_by_index(_build_dep_path(sentence['basicDependencies'], \n",
    "                                                          sentence['tokens'], \n",
    "                                                          triplet['subjectSpan'][0], \n",
    "                                                          triplet['objectSpan'][-1]), sentence['tokens']))\n",
    "            subjects.append(_subject)\n",
    "            relations.append(_relation)\n",
    "            objects.append(_object)\n",
    "            dep_path.append(_dependency_path)\n",
    "            \n",
    "        #return pd.DataFrame(result, columns=header)\n",
    "        return subjects, relations, objects\n",
    "    \n",
    "    subjects, relations, objects = [], [], []\n",
    "    for sentence in document:\n",
    "        _subject, _relation, _object = _extract(sentence)\n",
    "        subjects += _subject\n",
    "        relations += _relation\n",
    "        objects += _object\n",
    "    \n",
    "    return subjects, relations, objects\n",
    "\n",
    "def _mark_ner_object(row):\n",
    "    return row['relation'] + (row['DATE_obj'] == 1) * ' date'\\\n",
    "                           + (row['LOCATION_obj'] == 1) * ' location'\n",
    "    \n",
    "def _extract_features(document):\n",
    "    def _embed_arg(row):\n",
    "        result = []\n",
    "        result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    features = {}\n",
    "    features['subject'], features['relation'], features['object'] = _extract_plain_features(document[1])\n",
    "\n",
    "    #features.insert(loc=0, column='docid', value=document[0])\n",
    "    #max_len = {'obj': 3, 'rel': 3, 'subj': 3}\n",
    "    #features['w2v_subj'] = list(map(_embed_arg, features['subject']))\n",
    "    #features['w2v_rel'] = list(map(_embed_arg, features['relation']))\n",
    "    #features['w2v_obj'] = list(map(_embed_arg, features['object']))\n",
    "    \n",
    "    #print(features['w2v_subj'])\n",
    "    \n",
    "    #features['w2v_subj'] = features['subject'].map(lambda words: _embed(np.zeros((max_len['subj'], word2vec_vector_length)), words.lower().split()))\n",
    "    #features['w2v_rel'] = features['relation'].map(lambda words: _embed(np.zeros((max_len['rel'], word2vec_vector_length)), words.lower().split()))\n",
    "    #features['w2v_obj'] = features['object'].map(lambda words: _embed(np.zeros((max_len['obj'], word2vec_vector_length)), words.lower().split()))\n",
    "       \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def remove_repetitions(annot):\n",
    "    for i in range(len(annot)):\n",
    "        for j in range(len(annot[i])):\n",
    "            annot[i][j]['openie'] = list(unique_everseen(annot[i][j]['openie']))\n",
    "    return annot\n",
    "\n",
    "                                           \n",
    "class FeaturesProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pool = multiprocessing.Pool(processes=1)\n",
    "                 \n",
    "    def __call__(self, data):\n",
    "        \n",
    "        def mark_garbage(row):\n",
    "            deprec_rels = {'in', 'is', 'was', 'of', \"'s\", 'to', 'for', 'by', 'with', 'also', 'as of',\n",
    "                           'had', 'said', 'said in', 'felt', 'on', 'gave', 'saw', 'found', 'did'}\n",
    "            return ''.join(row['relation']['tokens']) in deprec_rels or np.all(row['subject']['postag'] == np.zeros((3, 18))) or np.all(row['object']['postag'] == np.zeros((3, 18))) or np.all(row['relation']['postag'] == np.zeros((3, 18)))\n",
    "            \n",
    "        features = pd.concat(self.pool.map(_extract_features, data))\n",
    "        features['garbage'] = features.apply(lambda row: mark_garbage(row), axis=1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "#DATA_PATH = 'data/corenlp_annotations_ner_pairs'  #'data/filtered_annotations'\n",
    "DATA_PATH = 'data/corenlp_annotations_only_ner'\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "! mkdir $RESULT_PATH \n",
    "result = []\n",
    "extr = FeaturesProcessor()\n",
    "\n",
    "def extract_matrix(row):\n",
    "    _matrix = np.concatenate([row['ner'], row['postag'], row['w2v'], np.array([[row['dist_to_rel'], row['rel_pos']]] * 3)], axis=1)\n",
    "    return _matrix\n",
    "\n",
    "for file in tqdm(glob(DATA_PATH + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp = tmp[tmp.loc[:, 1].map(len) > 0]\n",
    "    tmp[1] = remove_repetitions(tmp[1].values)\n",
    "    result = extr(tmp[[0, 1]].values)\n",
    "    result['subject_matr'] = result.subject.map(extract_matrix)\n",
    "    result['object_matr'] = result.object.map(extract_matrix)\n",
    "    result['relation_matr'] = result.relation.map(extract_matrix)\n",
    "    #break\n",
    "    result.to_pickle(file.replace(DATA_PATH, RESULT_PATH).replace('.json', '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.iloc[0].object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect to one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv processed_ data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/processed_separately'\n",
    "\n",
    "result = []\n",
    "for file in tqdm(glob(DATA_PATH + '/*.pkl')):\n",
    "    result.append(pd.read_pickle(file))\n",
    "    \n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['garbage'] = result.garbage.map(lambda row: not row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ = result[result.garbage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ = result_.drop(columns=['garbage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_.to_pickle('data/processed_separately_clean.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

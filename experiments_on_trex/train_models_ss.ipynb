{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_pickle('data/dataset_ner.pkl')  # w2v of lemmas\n",
    "#data = pd.read_pickle('data/processed_separately.pkl')\n",
    "data = pd.read_pickle('data/processed_separately_clean.pkl').sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "data['subject'] = data['subject'].map(get_tokens)\n",
    "data['relation'] = data['relation'].map(get_tokens)\n",
    "data['object'] = data['object'].map(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusters_examples import SAME_CLUSTERS\n",
    "\n",
    "def get_labels(row):\n",
    "    counter = 1\n",
    "    for labeled_cluster in SAME_CLUSTERS:\n",
    "        if (row.subject, row.relation, row.object) in labeled_cluster:\n",
    "            row['label'] = counter\n",
    "        counter += 1\n",
    "    return row\n",
    "\n",
    "data = data.apply(get_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)\n",
    "data = data.drop_duplicates(['subject', 'relation', 'object'])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.label == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.label > 0]['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(cluster) for cluster in SAME_CLUSTERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object, _subject, _relation, _labels = data.object_matr.values, data.subject_matr.values, data.relation_matr.values, data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shape = (data.shape[0], \n",
    "               data.subject_matr.iloc[0].shape[0], \n",
    "               data.subject_matr.iloc[0].shape[1])  # (17901, 3, 343)\n",
    "\n",
    "_object = np.concatenate(_object).reshape(final_shape)\n",
    "_subject = np.concatenate(_subject).reshape(final_shape)\n",
    "_relation = np.concatenate(_relation).reshape(final_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalers = [{}, {}, {}]\n",
    "\n",
    "for i in range(_object.shape[1]):\n",
    "    scalers[0][i] = StandardScaler()\n",
    "    _object[:, i, :] = scalers[0][i].fit_transform(_object[:, i, :]) \n",
    "\n",
    "for i in range(_subject.shape[1]):\n",
    "    scalers[1][i] = StandardScaler()\n",
    "    _subject[:, i, :] = scalers[1][i].fit_transform(_subject[:, i, :]) \n",
    "    \n",
    "for i in range(_relation.shape[1]):\n",
    "    scalers[2][i] = StandardScaler()\n",
    "    _relation[:, i, :] = scalers[2][i].fit_transform(_relation[:, i, :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.python.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from tensorflow.python.keras.layers import Reshape\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input, Layer\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Permute, Add\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import GaussianNoise\n",
    "from tensorflow.python.keras.layers import UpSampling1D\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape, Layer, InputSpec\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_ae(input_shape):\n",
    "    \n",
    "    INNER_SIZE = 100\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        input_layer = GaussianNoise(stddev=.1)(input_layer)\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "\n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_object, encode_rel])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[output_subject, output_object, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_ae(input_shape):\n",
    "    \"\"\" mask relation embedding and try to restore it along with the arguments \"\"\"\n",
    "    \n",
    "    INNER_SIZE = 50\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "    \n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_object])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[output_subject, output_object, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_rel(input_shape):\n",
    "    \"\"\" mask relation embedding and try to restore it alone \"\"\"\n",
    "    \n",
    "    INNER_SIZE = 50\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_sub_noised = GaussianNoise(stddev=.001)(input_subject)\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_obj_noised = GaussianNoise(stddev=.001)(input_object)\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "    \n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_object])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[input_sub_noised, input_obj_noised, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_obj(input_shape):\n",
    "    \"\"\" mask object embedding and try to restore it alone \"\"\"\n",
    "    \n",
    "    INNER_SIZE = 50\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_sub_noised = GaussianNoise(stddev=.001)(input_subject)\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "    input_rel_noised = GaussianNoise(stddev=.001)(input_rel)\n",
    "    \n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_rel])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    #output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    #output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[input_sub_noised, output_object, input_rel_noised])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subject.shape[1:], _object.shape[1:], _relation.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = noised_ae((_object.shape[1:]))\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(x=[_subject, _object, _relation],\n",
    "          y=[_subject, _object, _relation], epochs=200, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = restore_rel((_object.shape[1:]))\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(x=[_subject, _object, _relation],\n",
    "          y=[_subject, _object, _relation], epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import deep_clustering_ss as deep_clustering\n",
    "save_dir = 'semi_sup/idec/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "idec = deep_clustering.IDEC(input_shape=(_object.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),#restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=2,\n",
    "                            pretrain_epochs=2,\n",
    "                            maxiter=int(1e2),\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "\n",
    "plot_model(idec._model, to_file=os.path.join(save_dir, 'idec_model.png'), show_shapes=True)\n",
    "idec.compile(gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idec.fit([_subject, _object, _relation], labels=_labels)\n",
    "#idec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = idec._y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = idec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=999):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('announced launch')].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('worked')].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 34\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/idec/restore_obj/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = save_dirty_json('dcec_kmeans_50c_000_1e4.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DAEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/daec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/daec/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "daec = deep_clustering.DAEC(input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=50, \n",
    "                            pretrain_epochs=10,\n",
    "                            log_dir=logPath,\n",
    "                            save_dir=save_dir, \n",
    "                            )\n",
    "\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'daec_model.png'), show_shapes=True)\n",
    "daec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = daec._y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dcec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/daec/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = save_dirty_json('daec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DC_Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/dc_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/dc_kmeans/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "dckmeans = deep_clustering.DC_Kmeans(\n",
    "                            input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=30,\n",
    "                            pretrain_epochs=50,\n",
    "                            max_epochs=200,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dckmeans._model, to_file=os.path.join(save_dir, 'dckmeans_model.png'), show_shapes=True)\n",
    "dckmeans.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = dckmeans.y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dckmeans.get_scores([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/dc_kmeans/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = save_dirty_json('dc_kmeans_30c_000.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of internal representations generated by autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pae = plain_ae(x_train.shape[1:])\n",
    "pae.compile(optimizer='adam', loss='mse')\n",
    "pae.fit(x_train, x_train, batch_size=256, epochs=10, verbose=0)\n",
    "hidden = pae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=pae.input, outputs=hidden)\n",
    "#embeddings = encoder.predict(x_train)\n",
    "#cluzeriser = KMeans(2, n_jobs=6)\n",
    "#clusters = cluzeriser.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pae.save('models/pae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return features[clusters == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pae_clusters.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

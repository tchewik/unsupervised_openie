{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_path = 'trex_data'\n",
    "annot_path = 'corenlp_annotations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get T-Rex dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8760241/TREx.zip\n",
    "mkdir $trex_path\n",
    "unzip TREx.zip $trex_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "container = 'http://' + hostname + ':9000'\n",
    "nlp = StanfordCoreNLP(container)\n",
    "nlp_properties = {\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,ner',\n",
    "  'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def filter_triples(triples):\n",
    "    result = []\n",
    "    for triple in triples:\n",
    "        if triple['predicate']['surfaceform'] and triple['object']['surfaceform'] and triple['subject']['surfaceform']:\n",
    "            result.append(triple)\n",
    "    return result\n",
    "\n",
    "for dataset_file in glob.glob(os.path.join(trex_path, '*.json'))[418:]:\n",
    "    dataset = pd.read_json(dataset_file)\n",
    "    dataset = dataset[dataset['triples'].map(len) > 0]\n",
    "    dataset['triples'] = dataset['triples'].map(filter_triples)\n",
    "    dataset = dataset[dataset['triples'].map(len) > 0]  # filter documents before applying nlp annotation\n",
    "    nlp_annot = {}\n",
    "\n",
    "    for document in tqdm(range(dataset.shape[0])):\n",
    "        docid = dataset.iloc[document].docid.split('/')[-1]\n",
    "        nlp_annot.update({\n",
    "            docid: nlp.annotate(dataset.iloc[document].text, properties=nlp_properties)['sentences']\n",
    "        })\n",
    "        \n",
    "    json.dump(nlp_annot, open(dataset_file.replace(trex_path, annot_path), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "_digits = re.compile('\\d')\n",
    "\n",
    "def extract_tokens(annotation, arg1, arg2):\n",
    "    def find_in_sentence(sentence_annotation, argument_annotation):\n",
    "        start_token = 0        \n",
    "        for token in sentence_annotation['tokens']:\n",
    "            if token.get('characterOffsetBegin') == argument_annotation.get('boundaries')[0]:\n",
    "                start_token = token['index']\n",
    "            if token.get('characterOffsetEnd') == argument_annotation.get('boundaries')[1]:\n",
    "                if start_token == token['index']:\n",
    "                    # entity contains one token\n",
    "                    return [token['index']]  # begin with 1!\n",
    "                if start_token:\n",
    "                    return [ind for ind in range(start_token, token['index'] + 1)]\n",
    "    \n",
    "    for i, sentence in enumerate(annotation):\n",
    "        tok1 = find_in_sentence(sentence, arg1)\n",
    "        if tok1:\n",
    "            tok2 = find_in_sentence(sentence, arg2)\n",
    "            if tok2:\n",
    "                return [i, tok1, tok2]\n",
    "    return [-1, -1, -1]\n",
    "\n",
    "def _get_bow_between(tokens, tok1, tok2):\n",
    "    tmp = []\n",
    "    result = []\n",
    "    tok_left, tok_right = sorted([tok1, tok2])\n",
    "    for word in [tokens[i-1]['originalText'] for i in range(max(tok_left) + 1, min(tok_right))]:\n",
    "        for pun in string.punctuation:\n",
    "            word = word.strip(pun)\n",
    "        if word != '':\n",
    "            tmp.append(word.lower())\n",
    "    for word in tmp:\n",
    "        if word not in stopwords_list and not _digits.search(word) and not word[0].isupper():\n",
    "            result.append(word)\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_pos_between(tokens, tok1, tok2):\n",
    "    result = []\n",
    "    tok_left, tok_right = sorted([tok1, tok2])\n",
    "    for pos in [tokens[i-1]['pos'] for i in range(max(tok_left) + 1, min(tok_right))]:\n",
    "        if pos not in string.punctuation:\n",
    "            result.append(pos)\n",
    "    return '_'.join(result)\n",
    "\n",
    "def _get_dep_path(dependencies, start, end):\n",
    "    \"\"\"\n",
    "    Finds the shortest dependency path between two tokens in a sentence.\n",
    "        Args:\n",
    "            dependencies(list): List of dependencies in Stanford CoreNLP style\n",
    "            start(int): Number of the first token\n",
    "            end(int): Number of the second token\n",
    "        Returns:\n",
    "            list of tokens [start ... end] as they are presented in the shortest dependency path\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    deps = {}\n",
    "\n",
    "    for edge in dependencies:\n",
    "        edges.append((edge['governor'], edge['dependent']))\n",
    "        deps[(min(edge['governor'], edge['dependent']),\n",
    "              max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "    graph = nx.Graph(edges)\n",
    "    path = nx.shortest_path(graph, source=start, target=end)\n",
    "    return [p for p in path]\n",
    "\n",
    "def _get_shortest_path(dependencies, left_set, right_set):\n",
    "    \"\"\"\n",
    "    Finds the shortest dependency path between two sets of tokens in a sentence.\n",
    "    \"\"\"\n",
    "    result = [1] * len(dependencies)\n",
    "    for a in left_set:\n",
    "        for b in right_set:\n",
    "            candidate = _get_dep_path(dependencies, a, b)\n",
    "            if len(candidate) < len(result):\n",
    "                result = candidate\n",
    "    return result    \n",
    "\n",
    "def _get_words_dep(tokens, dependency_path):\n",
    "    result = [tokens[i-1]['word'] for i in dependency_path[1:-1]]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_trigger(tokens, dependency_path):\n",
    "    result = []\n",
    "    for word in [tokens[i-1]['lemma'] for i in dependency_path[1:-1]]:\n",
    "        if word not in stopwords_list:\n",
    "            result.append(word)\n",
    "    return '|'.join(result)\n",
    "\n",
    "def _get_entity_type(tokens, tok):\n",
    "    _replace = {\n",
    "        'PERSON_PERSON': 'PERSON',\n",
    "        'ORGANIZATION_ORGANIZATION': 'ORGANIZATION'\n",
    "    }\n",
    "    result = '_'.join([tokens[token-1].get('ner') for token in tok])\n",
    "    for key, value in _replace.items():\n",
    "        result = result.replace(key, value)\n",
    "    return result\n",
    "\n",
    "def process_document(document, annotation):\n",
    "    docid = document['docid'].split('/')[-1]\n",
    "    #annotation = json.load(open(os.path.join('corenlp_annotations', docid + '.json'), 'r'))['sentences']\n",
    "    result = []\n",
    "    \n",
    "    for triple in document['triples']:\n",
    "        if triple['object']['surfaceform'] and triple['subject']['surfaceform'] and triple['predicate']['surfaceform']:\n",
    "            #  print('>>>', triple)\n",
    "            #  print('<<<', annotation[0])\n",
    "            act_sent, tok1, tok2 = extract_tokens(annotation, \n",
    "                                                  triple['object'],\n",
    "                                                  triple['subject'])\n",
    "            if act_sent > -1:\n",
    "                surface1 = '_'.join(triple['object']['surfaceform'].split())\n",
    "                surface2 = '_'.join(triple['subject']['surfaceform'].split())\n",
    "                surface_pred = '_'.join(triple['predicate']['surfaceform'].split())\n",
    "                bow = _get_bow_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                dependency_path = _get_shortest_path(annotation[act_sent]['enhancedPlusPlusDependencies'], tok1, tok2)\n",
    "                trigger = _get_trigger(annotation[act_sent]['tokens'], dependency_path)\n",
    "                pos = _get_pos_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                ent1 = _get_entity_type(annotation[act_sent]['tokens'], tok1)\n",
    "                ent2 = _get_entity_type(annotation[act_sent]['tokens'], tok2)\n",
    "                path = _get_words_dep(annotation[act_sent]['tokens'], dependency_path)\n",
    "                \n",
    "                result.append({\n",
    "                    '_docid': docid,\n",
    "                    '_tok1': tok1,\n",
    "                    '_tok2': tok2,\n",
    "                    '_pred': surface_pred,\n",
    "                    '_sent_id': triple['sentence_id'],\n",
    "                    '_sentence': act_sent,\n",
    "                    '_dep_path': dependency_path,\n",
    "                    ## Titov features\n",
    "                    'bow': bow,\n",
    "                    'e1': surface1, \n",
    "                    'e2': surface2,\n",
    "                    'trigger': trigger,\n",
    "                    'pos': pos,\n",
    "                    'pairtype': ent1 + '_' + ent2,\n",
    "                    'e1type': ent1,\n",
    "                    'e2type': ent2,\n",
    "                    'path': path\n",
    "                })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(data_chunk, annot_chunk):\n",
    "    result = []\n",
    "\n",
    "    for index, row in data_chunk.iterrows():\n",
    "        annotation = annot_chunk.get(row['docid'].split('/')[-1])\n",
    "        if annotation:\n",
    "            result += process_document(row, annotation)\n",
    "        \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'final_data'\n",
    "! mkdir $data_path\n",
    "\n",
    "for dataset_file in tqdm(glob.glob(os.path.join(trex_path, '*.json'))[219:]):\n",
    "    data_chunk = pd.read_json(dataset_file)\n",
    "    annot_chunk = json.load(open(dataset_file.replace(trex_path, annot_path), 'r'))\n",
    "    features = extract_triples(data_chunk, annot_chunk)\n",
    "    features = features[features['_sentence'] > -1]  # filter entities not given in the same sentence\n",
    "    features.to_pickle(dataset_file.replace(trex_path, data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yao_like(path, of):\n",
    "    result = []\n",
    "    all_files = glob.glob(path + '*.json')\n",
    "    _train = int(len(all_files) * 0.6)\n",
    "    _dev = (len(all_files) - _train) // 2\n",
    "    \n",
    "    for file in all_files:\n",
    "        df = pd.read_pickle(file)\n",
    "        df.replace('PERSON_PERSON', 'PERSON', inplace=True)\n",
    "        df.replace('ORGANIZATION_ORGANIZATION_ORGANIZATION', 'ORGANIZATION', inplace=True)\n",
    "        df.replace('ORGANIZATION_ORGANIZATION', 'ORGANIZATION', inplace=True)\n",
    "        result.append(df[[key for key in df.keys() if key[0] != '_']])\n",
    "\n",
    "    train = pd.concat(result[:_train])\n",
    "    train.to_csv(of+'_train.csv', sep='\\t', index=None, header=False)\n",
    "    dev = pd.concat(result[_train:_train+_dev])\n",
    "    dev.to_csv(of+'_dev.csv', sep='\\t', index=None, header=False)\n",
    "    test = pd.concat(result[_train+_dev:])\n",
    "    test.to_csv(of+'_test.csv', sep='\\t', index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_yao_like('final_data/', 'trex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_pickle('data/dataset_ner.pkl')  # w2v of lemmas\n",
    "#data = pd.read_pickle('data/processed_separately.pkl')\n",
    "data = pd.read_pickle('data/processed_separately_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object, _subject, _relation = data.object_matr.values, data.subject_matr.values, data.relation_matr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shape = (data.shape[0], 3, 343)\n",
    "_object = np.concatenate(_object).reshape(final_shape)\n",
    "_subject = np.concatenate(_subject).reshape(final_shape)\n",
    "_relation = np.concatenate(_relation).reshape(final_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalers = {}\n",
    "for i in range(_object.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    _object[:, i, :] = scalers[i].fit_transform(_object[:, i, :]) \n",
    "\n",
    "for i in range(_subject.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    _subject[:, i, :] = scalers[i].fit_transform(_subject[:, i, :]) \n",
    "    \n",
    "for i in range(_relation.shape[1]):\n",
    "    scalers[i] = StandardScaler()\n",
    "    _relation[:, i, :] = scalers[i].fit_transform(_relation[:, i, :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.python.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from tensorflow.python.keras.layers import Reshape\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input, Layer\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Permute, Add\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import GaussianNoise\n",
    "from tensorflow.python.keras.layers import UpSampling1D\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape, Layer, InputSpec\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_ae(input_shape):\n",
    "    \n",
    "    INNER_SIZE = 80\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        input_layer = GaussianNoise(stddev=.1)(input_layer)\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "\n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_object, encode_rel])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[output_subject, output_object, output_rel])\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "Epoch 200/200\n",
    "32419/32419 [==============================] - 1s 45us/step - loss: 2.3175 - output_subject_conv2_loss: 0.8170 - output_object_conv2_loss: 0.7654 - output_rel_conv2_loss: 0.7352\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_ae(input_shape):\n",
    "    \"\"\" mask relation embedding and try to restore it along with the arguments \"\"\"\n",
    "    \n",
    "    INNER_SIZE = 80\n",
    "    \n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(input_shape[0], name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(input_shape[1], (6,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_subject = Input(shape=input_shape, name='input_subject')\n",
    "    input_object = Input(shape=input_shape, name='input_object')\n",
    "    input_rel = Input(shape=input_shape, name='input_rel')\n",
    "    \n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    \n",
    "    x = concatenate([encode_subject, encode_object])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_subject, input_object, input_rel], \n",
    "                  outputs=[output_subject, output_object, output_rel])\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "Epoch 200/200\n",
    "32419/32419 [==============================] - 1s 44us/step - loss: 2.4200 - output_subject_conv2_loss: 0.7845 - output_object_conv2_loss: 0.7825 - output_rel_conv2_loss: 0.8530\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1 = 200\n",
    "LAYER_2 = 100\n",
    "INNER_SIZE = 80\n",
    "\n",
    "def masked_subj(input_shape):\n",
    "    \"\"\" mask subject embedding and try to restore it \"\"\"\n",
    "    \n",
    "    def encode_plain_input(input_layer):\n",
    "        x = Dense(LAYER_2, activation='relu', name='enc2')(input_layer)\n",
    "        return x\n",
    "\n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        return Flatten()(pool1)\n",
    "    \n",
    "    def decode_plain_input(latent):\n",
    "        x = Dense(LAYER_2, activation='relu', name='dec1')(latent)\n",
    "        output = Dense(input_shape_plain[0], name='output_plain')(x)\n",
    "        return output\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(3, name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(300, (1,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_shape_plain, input_shape_emb = input_shape\n",
    "    \n",
    "    input_plain = Input(shape=input_shape_plain, name='input_plain')\n",
    "    input_subject = Input(shape=input_shape_emb, name='input_subject')\n",
    "    input_object = Input(shape=input_shape_emb, name='input_object')\n",
    "    input_rel = Input(shape=input_shape_emb, name='input_rel')\n",
    "\n",
    "    encode_plain = encode_plain_input(input_plain)\n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_plain, encode_object, encode_rel])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "    \n",
    "    output_plain = decode_plain_input(latent)\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_plain, input_subject, input_object, input_rel], \n",
    "                  outputs=[output_plain, output_subject, output_object, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subject.shape[1:], _object.shape[1:], _relation.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = noised_ae((_object.shape[1:]))\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(x=[_subject, _object, _relation],\n",
    "          y=[_subject, _object, _relation], epochs=200, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DCEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/restore_rel_kmeans'\n",
    "! mkdir $save_dir\n",
    "\n",
    "dcec = deep_clustering.DCEC(input_shape=(_object.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: masked_ae(input_shape),  # select model here\n",
    "                            n_clusters=80,\n",
    "                            pretrain_epochs=10,\n",
    "                            maxiter=int(1e5),\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'dcec_model.png'), show_shapes=True)\n",
    "dcec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dcec._y_pred\n",
    "\n",
    "dumb_features = data[:]\n",
    "scores = dcec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['cluster'] = y_pred\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "dumb_features['subject'] = dumb_features['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = dumb_features['relation'].map(get_tokens)\n",
    "dumb_features['object'] = dumb_features['object'].map(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[y_pred == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]\n",
    "    #return subj#dumb_features[y_pred == number][['docid', 'subject', 'relation', 'object', 'score']].iloc[:rows] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('published')].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = show_cluster_sample(27)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp.subject == 'joseph laycock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dcec._model.get_layer(name='clustering').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp.relation == \"professor\"].sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp.relation == \"was born\"].sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dirty_json('dcec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"94\": {\"rel_type\": \"released\", \"qa_examples\": [[\"When was released Synchronet?\", \"October 2000\"], [\"When was first released Skype?\", \"August 2003\"]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/daec'\n",
    "! mkdir $save_dir\n",
    "\n",
    "daec = deep_clustering.DAEC(input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: masked_ae(input_shape),  # select model here\n",
    "                            n_clusters=80, \n",
    "                            pretrain_epochs=10,\n",
    "                            log_dir=logPath,\n",
    "                            save_dir=save_dir, \n",
    "                            )\n",
    "\n",
    "plot_model(daec._model, to_file=os.path.join(save_dir, 'daec_model.png'), show_shapes=True)\n",
    "daec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = daec._y_pred\n",
    "\n",
    "dumb_features = data[:]\n",
    "scores = daec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['cluster'] = y_pred\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=10):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[y_pred == number]\n",
    "    cluster['subject'] = cluster.subject.map(get_tokens)\n",
    "    cluster['relation'] = cluster.relation.map(get_tokens)\n",
    "    cluster['object'] = cluster.object.map(get_tokens)\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]\n",
    "    #return subj#dumb_features[y_pred == number][['docid', 'subject', 'relation', 'object', 'score']].iloc[:rows] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = show_cluster_sample(1)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = save_dirty_json('daec_kmeans_80c_001.json', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[5]['predicates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DC_Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models'\n",
    "\n",
    "dckmeans = deep_clustering.DC_Kmeans(input_shape=x_train.shape[1:], \n",
    "                            autoencoder_ctor=lambda input_shape: plain_noised_ae(input_shape),\n",
    "                            n_clusters=30,\n",
    "                            max_epochs=200,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dckmeans._model, to_file=os.path.join(save_dir, 'dckmeans_model.png'), show_shapes=True)\n",
    "dckmeans.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dckmeans._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of internal representations generated by autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pae = plain_ae(x_train.shape[1:])\n",
    "pae.compile(optimizer='adam', loss='mse')\n",
    "pae.fit(x_train, x_train, batch_size=256, epochs=10, verbose=0)\n",
    "hidden = pae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=pae.input, outputs=hidden)\n",
    "#embeddings = encoder.predict(x_train)\n",
    "#cluzeriser = KMeans(2, n_jobs=6)\n",
    "#clusters = cluzeriser.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pae.save('models/pae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return features[clusters == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pae_clusters.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../logs/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/dataset_ner.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8308, 351)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprecated = ('one', 'she', 'they', 'his', 'her', 'its', 'our', 'day', 'co.', 'inc.', \n",
    "              'society', 'people', 'inventor', 'head', 'poet', 'doctor', 'teacher', 'inventor', \n",
    "              'thanksgiving day', 'halloween',\n",
    "              'sales person', 'model', 'board', 'technology', 'owner', 'one', 'two', \n",
    "             )\n",
    "\n",
    "data = data[data.object.map(len) > 2]\n",
    "data = data[data.subject.map(len) > 2]\n",
    "\n",
    "for dep_word in deprecated:\n",
    "    data = data[data.subject.str.lower() != dep_word]\n",
    "    data = data[data.object.str.lower() != dep_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.lemma_subj.map(len) < 4]\n",
    "data = data[data.lemma_obj.map(len) < 4]\n",
    "data = data[data.lemma_rel.map(len) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6729, 351)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_rel = pickle.load(open('models/relation_vectorizer.pkl', 'rb'))\n",
    "bow_rel = vectorizer_rel.transform(data['relation'].values).toarray()\n",
    "columns = [column + '_bow_rel' for column in vectorizer_rel.get_feature_names()]\n",
    "features = pd.concat([data.reset_index(drop=True), pd.DataFrame(bow_rel, columns=columns)], axis=1)\n",
    "\n",
    "vectorizer_path = pickle.load(open('models/path_vectorizer.pkl', 'rb'))\n",
    "bow_path = vectorizer_path.transform(features['dep_path'].values).toarray()\n",
    "columns = [column + '_bow_path' for column in vectorizer_path.get_feature_names()]\n",
    "features = pd.concat([features.reset_index(drop=True), pd.DataFrame(bow_path, columns=columns)], axis=1)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6729, 2351)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluding_cols = ['docid', 'subject', 'relation', 'object', 'dep_path', 'lemma_subj', 'lemma_rel', 'lemma_obj']\n",
    "embedding_cols = ['w2v_subj', 'w2v_rel', 'w2v_obj']\n",
    "w2v_subj = np.array([line.flatten() for line in features['w2v_subj'].values])\n",
    "w2v_rel = np.array([line.flatten() for line in features['w2v_rel'].values])\n",
    "w2v_obj = np.array([line.flatten() for line in features['w2v_obj'].values])\n",
    "x_train = np.concatenate([features.drop(columns=excluding_cols+embedding_cols), \n",
    "                          w2v_subj, w2v_rel, w2v_obj], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6729, 5040)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train = StandardScaler().fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_features = features.drop(columns=excluding_cols+embedding_cols).values\n",
    "plain_features = StandardScaler().fit_transform(plain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_subj = np.array([np.array(line) for line in features['w2v_subj']])\n",
    "w2v_obj = np.array([np.array(line) for line in features['w2v_obj']])\n",
    "w2v_rel = np.array([np.array(line) for line in features['w2v_rel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6729, 2340), (6729, 3, 300), (6729, 3, 300), (6729, 3, 300))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_features.shape, w2v_subj.shape, w2v_rel.shape, w2v_obj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.python.keras.layers import Dropout, UpSampling2D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from tensorflow.python.keras.layers import Reshape\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input, Layer\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Permute, Add\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import GaussianNoise\n",
    "from tensorflow.python.keras.layers import UpSampling1D\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape, Layer, InputSpec\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1 = 200\n",
    "LAYER_2 = 50\n",
    "INNER = 10\n",
    "\n",
    "def plain_ae(input_shape):\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Dense(LAYER_1, activation='relu', name='enc1')(input_layer)\n",
    "    x = Dense(LAYER_2, activation='relu', name='enc2')(x)\n",
    "    latent = Dense(INNER, activation='sigmoid', name='embedding')(x)\n",
    "    x = Dense(LAYER_2, activation='relu', name='dec1')(latent)\n",
    "    x = Dense(LAYER_1, activation='relu', name='dec2')(x)\n",
    "    output_layer = Dense(input_shape[0], name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1 = 200\n",
    "LAYER_2 = 50\n",
    "INNER = 10\n",
    "\n",
    "def plain_noised_ae(input_shape):\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = GaussianNoise(0.5)(input_layer)\n",
    "    x = Dense(LAYER_1, activation='relu', name='enc1')(x)\n",
    "    x = Dense(LAYER_2, activation='relu', name='enc2')(x)\n",
    "    latent = Dense(INNER, activation='sigmoid', name='embedding')(x)\n",
    "    x = Dense(LAYER_2, activation='relu', name='dec1')(latent)\n",
    "    x = Dense(LAYER_1, activation='relu', name='dec2')(x)\n",
    "    output_layer = Dense(input_shape[0], name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1 = 200\n",
    "LAYER_2 = 100\n",
    "INNER_SIZE = 50\n",
    "\n",
    "def noised_ae(input_shape):\n",
    "    \n",
    "    def encode_plain_input(input_layer):\n",
    "        x = GaussianNoise(0.5)(input_layer)\n",
    "        x = Dense(LAYER_1, activation='relu', name='enc1')(x)\n",
    "        #x = Dense(LAYER_2, activation='relu', name='enc2')(x)\n",
    "        return x\n",
    "\n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool2)\n",
    "    \n",
    "    def decode_plain_input(latent):\n",
    "        x = Dense(LAYER_2, activation='relu', name='dec1')(latent)\n",
    "        #x = Dense(LAYER_1, activation='relu', name='dec2')(x)\n",
    "        output = Dense(input_shape_plain[0], name='output_plain')(x)\n",
    "        return output\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        up0 = UpSampling1D(3)(latent)\n",
    "        conv1 = Conv1D(3, (3,), activation='relu', padding='same', name=name+'_conv1')(up0)\n",
    "        #up1 = UpSampling1D(3, name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(300, (1,), activation='relu', padding='same', name=name+'_conv2')(conv1)\n",
    "        #up2 = UpSampling1D(1, name=name+'_up2')(conv2)\n",
    "        #r = Conv1D(1, (2,), activation='sigmoid', padding='same', name=name)(up2)\n",
    "        return conv2\n",
    "    \n",
    "    input_shape_plain, input_shape_emb = input_shape\n",
    "    \n",
    "    input_plain = Input(shape=input_shape_plain, name='input_plain')\n",
    "    input_subject = Input(shape=input_shape_emb, name='input_subject')\n",
    "    input_object = Input(shape=input_shape_emb, name='input_object')\n",
    "    input_rel = Input(shape=input_shape_emb, name='input_rel')\n",
    "\n",
    "    encode_plain = encode_plain_input(input_plain)\n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_plain, encode_subject, encode_object, encode_rel])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "    \n",
    "    output_plain = decode_plain_input(latent)\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_plain, input_subject, input_object, input_rel], \n",
    "                  outputs=[output_plain, output_subject, output_object, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1 = 200\n",
    "LAYER_2 = 100\n",
    "INNER_SIZE = 100\n",
    "\n",
    "def masked_ae(input_shape):\n",
    "    \"\"\" mask relation embedding and try to restore it \"\"\"\n",
    "    \n",
    "    def encode_plain_input(input_layer):\n",
    "        #x = GaussianNoise(0.5)(input_layer)\n",
    "        #x = Dense(LAYER_1, activation='relu', name='enc1')(x)\n",
    "        x = Dense(LAYER_2, activation='relu', name='enc2')(input_layer)\n",
    "        return x\n",
    "\n",
    "    def encode_embedding_input(input_layer):\n",
    "        conv1 = Conv1D(128, (2,), activation='relu', padding='same')(input_layer)\n",
    "        pool1 = MaxPooling1D((2,), padding='same')(conv1)\n",
    "        #conv2 = Conv1D(32, (2,), activation='relu', padding='same')(pool1)\n",
    "        #pool2 = MaxPooling1D((2,), padding='same')(conv2)\n",
    "        return Flatten()(pool1)\n",
    "    \n",
    "    def decode_plain_input(latent):\n",
    "        x = Dense(LAYER_2, activation='relu', name='dec1')(latent)\n",
    "        #x = Dense(LAYER_1, activation='relu', name='dec2')(x)\n",
    "        output = Dense(input_shape_plain[0], name='output_plain')(x)\n",
    "        return output\n",
    "    \n",
    "    def decode_embedding_input(latent, name):\n",
    "        latent = Reshape((1, INNER_SIZE))(latent)\n",
    "        conv1 = Conv1D(128, (1,), activation='relu', padding='same', name=name+'_conv1')(latent)\n",
    "        up1 = UpSampling1D(3, name=name+'_up1')(conv1)\n",
    "        conv2 = Conv1D(300, (1,), activation='relu', padding='same', name=name+'_conv2')(up1)\n",
    "        return conv2\n",
    "    \n",
    "    input_shape_plain, input_shape_emb = input_shape\n",
    "    \n",
    "    input_plain = Input(shape=input_shape_plain, name='input_plain')\n",
    "    input_subject = Input(shape=input_shape_emb, name='input_subject')\n",
    "    input_object = Input(shape=input_shape_emb, name='input_object')\n",
    "    input_rel = Input(shape=input_shape_emb, name='input_rel')\n",
    "\n",
    "    encode_plain = encode_plain_input(input_plain)\n",
    "    encode_subject = encode_embedding_input(input_subject)\n",
    "    encode_object = encode_embedding_input(input_object)\n",
    "    encode_rel = encode_embedding_input(input_rel)\n",
    "    \n",
    "    x = concatenate([encode_plain, encode_subject, encode_object])\n",
    "    latent = Dense(INNER_SIZE, activation='sigmoid', name='embedding')(x)\n",
    "    \n",
    "    output_plain = decode_plain_input(latent)\n",
    "    output_subject = decode_embedding_input(latent, 'output_subject')\n",
    "    output_object = decode_embedding_input(latent, 'output_object')\n",
    "    output_rel = decode_embedding_input(latent, 'output_rel')\n",
    "    \n",
    "    model = Model(inputs=[input_plain, input_subject, input_object, input_rel], \n",
    "                  outputs=[output_plain, output_subject, output_object, output_rel])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2340,), (3, 300))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_features.shape[1:], w2v_obj.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = masked_ae((plain_features.shape[1:], w2v_obj.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.3)\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=[plain_features, w2v_obj, w2v_subj, w2v_rel],\n",
    "          y=[plain_features, w2v_obj, w2v_subj, w2v_rel], epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DCEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monoinput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models'\n",
    "\n",
    "dcec = deep_clustering.DCEC(input_shape=x_train.shape[1:], \n",
    "                            autoencoder_ctor=lambda input_shape: plain_ae(input_shape),\n",
    "                            n_clusters=40, \n",
    "                            pretrain_epochs=10,\n",
    "                            maxiter=int(5e4),\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'dcec_model.png'), show_shapes=True)\n",
    "dcec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcec.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models'\n",
    "\n",
    "dcec = deep_clustering.DCEC(input_shape=(plain_features.shape[1:], w2v_obj.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: masked_ae(input_shape),  # select model here\n",
    "                            n_clusters=50, \n",
    "                            pretrain_epochs=10,\n",
    "                            maxiter=int(1e3),\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'dcec_model.png'), show_shapes=True)\n",
    "dcec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_subject (InputLayer)      (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_object (InputLayer)       (None, 3, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3, 128)       76928       input_subject[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3, 128)       76928       input_object[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_plain (InputLayer)        (None, 2340)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 2, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 2, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc2 (Dense)                    (None, 100)          234100      input_plain[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 256)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 612)          0           enc2[0][0]                       \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Dense)               (None, 100)          61300       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 100)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 100)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 100)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_conv1 (Conv1D)   (None, 1, 128)       12928       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_object_conv1 (Conv1D)    (None, 1, 128)       12928       reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_conv1 (Conv1D)       (None, 1, 128)       12928       reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 100)          10100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_up1 (UpSampling1 (None, 3, 128)       0           output_subject_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output_object_up1 (UpSampling1D (None, 3, 128)       0           output_object_conv1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_up1 (UpSampling1D)   (None, 3, 128)       0           output_rel_conv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 50)           5000        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_plain (Dense)            (None, 2340)         236340      dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_conv2 (Conv1D)   (None, 3, 300)       38700       output_subject_up1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "output_object_conv2 (Conv1D)    (None, 3, 300)       38700       output_object_up1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_conv2 (Conv1D)       (None, 3, 300)       38700       output_rel_up1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 855,580\n",
      "Trainable params: 855,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dcec._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcec.fit([plain_features, w2v_obj, w2v_subj, w2v_rel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dcec._y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=10):\n",
    "    return features[y_pred == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:rows] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was acquired by                        14\n",
       "acquired                               12\n",
       "was acquired in                         7\n",
       "purchased                               4\n",
       "was acquired In                         3\n",
       "acquired in                             3\n",
       "acquired agency in                      3\n",
       "was purchased by                        3\n",
       "acquired Mustard Digital At             2\n",
       "acquiring                               2\n",
       "published demonstration In              2\n",
       "acquire                                 2\n",
       "was purchased In                        2\n",
       "purchased NeXT in                       2\n",
       "acquired copy In                        2\n",
       "has purchased                           2\n",
       "acquired agency In                      1\n",
       "must acquire                            1\n",
       "began in                                1\n",
       "Founded in                              1\n",
       "purchased codex In                      1\n",
       "had acquired in                         1\n",
       "purchased program from                  1\n",
       "acquired Ulead Systems On               1\n",
       "acquired content syndication In         1\n",
       "purchased Clearwire Technologies In     1\n",
       "was acquired later by                   1\n",
       "acquired Wyse in                        1\n",
       "was acquired for                        1\n",
       "acquired technology from                1\n",
       "acquired representation In              1\n",
       "acquired Chorus Systèmes In             1\n",
       "was acquired On                         1\n",
       "was acquired By                         1\n",
       "acquired Tandberg on                    1\n",
       "purchased product line in               1\n",
       "acquired Rare in                        1\n",
       "acquired funding In                     1\n",
       "purchased company in                    1\n",
       "was acquired later in                   1\n",
       "acquired Carbine Studios In             1\n",
       "described Content ID In                 1\n",
       "acquired Infonautics In                 1\n",
       "was purchased in                        1\n",
       "acquired Dodgeball in                   1\n",
       "acquired LinkedIn in                    1\n",
       "founded Knowledge Systems In            1\n",
       "was delisted from                       1\n",
       "acquired RMI In                         1\n",
       "acquired Sun Microsystems In            1\n",
       "purchased program in                    1\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_cluster_sample(35, 100).relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('dcec_clusters_masked.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i, 20).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DC_Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models'\n",
    "\n",
    "dckmeans = deep_clustering.DC_Kmeans(input_shape=x_train.shape[1:], \n",
    "                            autoencoder_ctor=lambda input_shape: plain_noised_ae(input_shape),\n",
    "                            n_clusters=30,\n",
    "                            max_epochs=200,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dckmeans._model, to_file=os.path.join(save_dir, 'dckmeans_model.png'), show_shapes=True)\n",
    "dckmeans.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dckmeans._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of internal representations generated by autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pae = plain_ae(x_train.shape[1:])\n",
    "pae.compile(optimizer='adam', loss='mse')\n",
    "pae.fit(x_train, x_train, batch_size=256, epochs=200, verbose=0)\n",
    "hidden = pae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=pae.input, outputs=hidden)\n",
    "embeddings = encoder.predict(x_train)\n",
    "cluzeriser = KMeans(50, n_jobs=6)\n",
    "clusters = cluzeriser.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pae.save('models/pae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return features[clusters == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pae_clusters.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

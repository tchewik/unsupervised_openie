{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check SimpleQuestions dataset path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls ../uopenie_qa/SimpleWikidataQuestions/csv\\ decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and annotate with stanza or CoreNLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from features_extractor import TripletsParserStanza, TripletsParserCoreNLP\n",
    "\n",
    "parser = TripletsParserCoreNLP('', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for part in [\"train\", \"valid\", \"test\"]:\n",
    "    print(f\"Process {part}... \", end=\"\", flush=True)\n",
    "    path = f\"../uopenie_qa/SimpleWikidataQuestions/csv decoded/annotated_wd_data_{part}_answerable_decoded.csv\"\n",
    "    data[part] = pd.read_csv(path)\n",
    "    data[part] = parser.annotate(data[part])\n",
    "    data[part].to_pickle(path.replace('.csv', '_annotated.pkl'))\n",
    "    features = parser.extract_features(data[part])\n",
    "    for i, name in enumerate([\"object\", \"subject\", \"relation\"]):\n",
    "        np.save(open(path.replace('.csv', f'_{name}_features.npy'), 'wb'), features[i])\n",
    "    print('[Done]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(f\"{key} shape:\\t{data[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features_extractor import TripletsParser\n",
    "\n",
    "parser = TripletsParser()\n",
    "\n",
    "for key in data.keys():\n",
    "    data[key] = parser.annotate(data['key'])\n",
    "    data[key] = parser.extract_features()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'wiki-news-300d-1M.vec.zip'  # 1.6G\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://dl.fbaipublicfiles.com/fasttext/vectors-english/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "    \n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                   binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-4:] == '.zip':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME[:-4],\n",
    "                                               binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,mwt,pos,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data['subject_annot'] = data.subject_decoded.map(nlp)\n",
    "data['property_annot'] = data.property_decoded.map(nlp)\n",
    "data['object_annot'] = data.object_decoded.map(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls annotated*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('annotated_test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_plain_features(row):\n",
    "    def _extract(document):\n",
    "        postag_tagtypes = {\n",
    "            'XPOS': ['JJ', 'CD', 'VBD', '', 'RB', 'VBN', 'PRP', 'IN', 'VBP', 'TO', 'NNP', 'VB',\n",
    "                     'VBZ', 'VBG', 'POS', 'NNS', 'NN', 'MD'],\n",
    "            'UPOS': ['ADJ', 'ADP', 'PUNCT', 'ADV', 'AUX', 'SYM', 'INTJ', 'CCONJ', 'X',\n",
    "                     'NOUN', 'DET', 'PROPN', 'NUM', 'VERB', 'PART', 'PRON', 'SCONJ'],\n",
    "        }\n",
    "\n",
    "        ner_tagtypes = {\n",
    "            'ontonotes': ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT',\n",
    "                          'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', \n",
    "                          'ORDINAL', 'CARDINAL'],\n",
    "            'corenlp': ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "                        'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "                        'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "                        'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "                        'MONEY', 'HANDLE'],\n",
    "        }\n",
    "\n",
    "        def get_postags_sequence(sequence, predicate=False, tagtype='UPOS'):\n",
    "\n",
    "            columns = postag_tagtypes[tagtype]\n",
    "\n",
    "            sequence = sequence[:3]\n",
    "\n",
    "            result = np.zeros((3, len(columns)))\n",
    "            sequence = [[int(column == postag) for column in columns] for postag in sequence]\n",
    "\n",
    "            if sequence:\n",
    "                result[:len(sequence)] = sequence\n",
    "                \n",
    "            return result\n",
    "#             return np.max(result, axis=0)\n",
    "\n",
    "        def get_ner_occurrences(ner_annot, tagtype='ontonotes'):\n",
    "\n",
    "            _ner_kinds = ner_tagtypes[tagtype]\n",
    "\n",
    "            ner_annot = ner_annot[:3]\n",
    "\n",
    "            mentions = [entity.type for entity in ner_annot]\n",
    "            mentions = [[int(_ner_kind == mention) for _ner_kind in _ner_kinds] for mention in mentions][:3]\n",
    "            result = np.zeros((3, len(_ner_kinds)))\n",
    "\n",
    "            if mentions:\n",
    "                result[:len(mentions)] = mentions\n",
    "\n",
    "            return result\n",
    "#             return np.max(result, axis=0)\n",
    "\n",
    "        def _embed(placeholder, words):\n",
    "            for j in range(len(words)):\n",
    "                if j == len(placeholder):\n",
    "                    break\n",
    "\n",
    "                word = words[j]\n",
    "                if word and word in word2vec_model:\n",
    "                    placeholder[j, :] = word2vec_model[word]\n",
    "            \n",
    "            return placeholder\n",
    "#             return np.average(placeholder, axis=0)\n",
    "\n",
    "\n",
    "        def _embed_arg(row):\n",
    "            result = []\n",
    "            result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "\n",
    "            return result\n",
    "\n",
    "        deprecated = []\n",
    "        deprec_rels = []\n",
    "\n",
    "        _object = {\n",
    "            'tokens': [token.text for token in document.object_annot.sentences[0].tokens],\n",
    "            'lemmas': [token.lemma for token in document.object_annot.sentences[0].words],\n",
    "            'ner': get_ner_occurrences(document.object_annot.ents),\n",
    "            'postag': get_postags_sequence(\n",
    "                [token.upos for token in document.object_annot.sentences[0].words]),\n",
    "        }\n",
    "        _object.update({\n",
    "            'w2v': _embed(np.zeros((3, word2vec_vector_length)), _object['lemmas']),\n",
    "        })\n",
    "        _relation = {\n",
    "            'tokens': [token.text for token in document.property_annot.sentences[0].tokens],\n",
    "            'lemmas': [token.lemma for token in document.property_annot.sentences[0].words],\n",
    "            'ner': get_ner_occurrences(document.property_annot.ents),\n",
    "            'postag': get_postags_sequence(\n",
    "                [token.upos for token in document.property_annot.sentences[0].words]),\n",
    "        }\n",
    "        _relation.update({\n",
    "            'w2v': _embed(np.zeros((3, word2vec_vector_length)), _relation['lemmas']),\n",
    "        })\n",
    "        _subject = {\n",
    "            'tokens': [token.text for token in document.subject_annot.sentences[0].tokens],\n",
    "            'lemmas': [token.lemma for token in document.subject_annot.sentences[0].words],\n",
    "            'ner': get_ner_occurrences(document.subject_annot.ents),\n",
    "            'postag': get_postags_sequence(\n",
    "                [token.upos for token in document.subject_annot.sentences[0].words]),\n",
    "        }\n",
    "        _subject.update({\n",
    "            'w2v': _embed(np.zeros((3, word2vec_vector_length)), _subject['lemmas']),\n",
    "        })\n",
    "\n",
    "        subjects, relations, objects, dep_path = [], [], [], []\n",
    "        subjects.append(_subject)\n",
    "        relations.append(_relation)\n",
    "        objects.append(_object)\n",
    "\n",
    "        return subjects, relations, objects\n",
    "\n",
    "    _subject, _relation, _object = _extract(row)\n",
    "\n",
    "    return _subject, _relation, _object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matrix(row):\n",
    "    _matrix = np.concatenate([row[0]['ner'], row[0]['postag'], row[0]['w2v']], axis=1).flatten()\n",
    "    return _matrix\n",
    "\n",
    "def extract_one_matrix(row):\n",
    "    _matrix = np.concatenate([extract_matrix(row['subject']), \n",
    "                             extract_matrix(row['relation']), \n",
    "                             extract_matrix(row['object'])], axis=0)\n",
    "    return _matrix\n",
    "\n",
    "def _extract_features(document):    \n",
    "    features = {}\n",
    "    features['subject'], features['relation'], features['object'] = _extract_plain_features(document)\n",
    "    \n",
    "    return features\n",
    "\n",
    "res = data.apply(_extract_features, axis=1)\n",
    "features = res.apply(extract_one_matrix).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[0]['subject'][0]['ner'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = data.apply(_extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = res.apply(extract_one_matrix).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.stack(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_features.pkl', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "features = np.load('test_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=20, n_init=10)\n",
    "kmeans.fit(features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'] = kmeans.predict(features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"question\", \"subject_decoded\", \"property_decoded\", \"object_decoded\", \"class\"]].to_csv(\"annotated_wd_data_test_classified.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['class'] == 5].property_decoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['class'] == 1].property_decoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['class'] == 1].property_decoded.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "318px",
    "left": "1484px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_path = 'trex_data'\n",
    "annot_path = 'corenlp_annotations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get T-Rex dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8760241/TREx.zip\n",
    "mkdir $trex_path\n",
    "unzip TREx.zip $trex_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate with Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "container = 'http://' + hostname + ':9000'\n",
    "nlp = StanfordCoreNLP(container)\n",
    "nlp_properties = {\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,ner',\n",
    "  'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "\n",
    "for value in [1, 2, 3, 38]:\n",
    "    temp.update({value: 'Opa'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def filter_triples(triples):\n",
    "    result = []\n",
    "    for triple in triples:\n",
    "        if triple['predicate']['surfaceform'] and triple['object']['surfaceform'] and triple['subject']['surfaceform']:\n",
    "            result.append(triple)\n",
    "    return result\n",
    "\n",
    "for dataset_file in glob.glob(os.path.join(trex_path, '*.json')):\n",
    "    dataset = pd.read_json(dataset_file)\n",
    "    dataset = dataset[dataset['triples'].map(len) > 0]\n",
    "    dataset['triples'] = dataset['triples'].map(filter_triples)\n",
    "    dataset = dataset[dataset['triples'].map(len) > 0]  # filter documents before applying nlp annotation\n",
    "    nlp_annot = {}\n",
    "\n",
    "    for document in tqdm(range(dataset.shape[0])):\n",
    "        docid = dataset.iloc[document].docid.split('/')[-1]\n",
    "        nlp_annot.update({\n",
    "            docid: nlp.annotate(dataset.iloc[document].text, properties=nlp_properties)['sentences']\n",
    "        })\n",
    "        \n",
    "    json.dump(nlp_annot, dataset_file.replace(trex_path, annot_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "_digits = re.compile('\\d')\n",
    "\n",
    "def extract_tokens(annotation, arg1, arg2):\n",
    "    def find_in_sentence(sentence_annotation, argument_annotation):\n",
    "        for token in sentence_annotation['tokens']:\n",
    "            if token.get('characterOffsetBegin') == argument_annotation.get('boundaries')[0] \\\n",
    "                and token.get('characterOffsetEnd') == argument_annotation.get('boundaries')[1]:\n",
    "                return token['index']  # begin with 1!\n",
    "    \n",
    "    for i, sentence in enumerate(annotation):\n",
    "        tok1 = find_in_sentence(sentence, arg1)\n",
    "        if tok1:\n",
    "            tok2 = find_in_sentence(sentence, arg2)\n",
    "            if tok2:\n",
    "                return [i, tok1, tok2]\n",
    "        else:\n",
    "            return [-1, -1, -1]\n",
    "\n",
    "def _get_bow_between(tokens, tok1, tok2):\n",
    "    tmp = []\n",
    "    result = []\n",
    "    for word in [tokens[i]['originalText'] for i in range(tok1, tok2-1)]:\n",
    "        for pun in string.punctuation:\n",
    "            word = word.strip(pun)\n",
    "        if word != '':\n",
    "            tmp.append(word.lower())\n",
    "    for word in tmp:\n",
    "        if word not in stopwords_list and not _digits.search(word) and not word[0].isupper():\n",
    "            result.append(word)\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_pos_between(tokens, tok1, tok2):\n",
    "    result = []\n",
    "    for pos in [tokens[i]['pos'] for i in range(tok1, tok2-1)]:\n",
    "        if pos not in string.punctuation:\n",
    "            result.append(pos)\n",
    "    return '_'.join(result)\n",
    "\n",
    "def _get_dep_path(dependencies, start, end):\n",
    "    \"\"\"\n",
    "    Finds the shortest dependency path between two tokens in a sentence.\n",
    "        Args:\n",
    "            dependencies(list): List of dependencies in Stanford CoreNLP style\n",
    "            start(int): Number of the first token\n",
    "            end(int): Number of the second token\n",
    "        Returns:\n",
    "            list of tokens [start ... end] as they are presented in the shortest dependency path\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    deps = {}\n",
    "\n",
    "    for edge in dependencies:\n",
    "        edges.append((edge['governor'], edge['dependent']))\n",
    "        deps[(min(edge['governor'], edge['dependent']),\n",
    "              max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "    graph = nx.Graph(edges)\n",
    "    path = nx.shortest_path(graph, source=start, target=end)\n",
    "    return [p for p in path]\n",
    "\n",
    "def _get_words_dep(tokens, dependency_path):\n",
    "    result = [tokens[i-1]['word'] for i in dependency_path[1:-1]]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_trigger(tokens, dependency_path):\n",
    "    result = []\n",
    "    for word in [tokens[i-1]['lemma'] for i in dependency_path[1:-1]]:\n",
    "        if word not in stopwords_list:\n",
    "            result.append(word)\n",
    "    return '|'.join(result)\n",
    "\n",
    "def _get_entity_type(token):\n",
    "    return token.get('ner')\n",
    "\n",
    "def process_document(document):\n",
    "    docid = document['docid'].split('/')[-1]\n",
    "    annotation = json.load(open(os.path.join('corenlp_annotations', docid + '.json'), 'r'))['sentences']\n",
    "    result = []\n",
    "    \n",
    "    for triple in document['triples']:\n",
    "        if triple['object']['surfaceform'] and triple['subject']['surfaceform'] and triple['predicate']['surfaceform']:\n",
    "            act_sent, tok1, tok2 = extract_tokens(annotation, \n",
    "                                                  triple['object'],\n",
    "                                                  triple['subject'])\n",
    "            if act_sent > -1:\n",
    "                surface1 = '_'.join(triple['object']['surfaceform'].split())\n",
    "                surface2 = '_'.join(triple['subject']['surfaceform'].split())\n",
    "                surface_pred = '_'.join(triple['predicate']['surfaceform'].split())\n",
    "                bow = _get_bow_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                dependency_path = _get_dep_path(annotation[act_sent]['enhancedPlusPlusDependencies'], tok1, tok2)\n",
    "                trigger = _get_trigger(annotation[act_sent]['tokens'], dependency_path)\n",
    "                pos = _get_pos_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                ent1 = _get_entity_type(annotation[act_sent]['tokens'][tok1-1])\n",
    "                ent2 = _get_entity_type(annotation[act_sent]['tokens'][tok2-1])\n",
    "                path = _get_words_dep(annotation[act_sent]['tokens'], dependency_path)\n",
    "                \n",
    "                result.append({\n",
    "                    '_docid': docid,\n",
    "                    '_tok1': tok1,\n",
    "                    '_tok2': tok2,\n",
    "                    '_pred': surface_pred,\n",
    "                    '_sent_id': triple['sentence_id'],\n",
    "                    '_sentence': act_sent,\n",
    "                    '_dep_path': dependency_path,\n",
    "                    ## Titov features\n",
    "                    'bow': bow,\n",
    "                    'e1': surface1, \n",
    "                    'e2': surface2,\n",
    "                    'trigger': trigger,\n",
    "                    'pos': pos,\n",
    "                    'pairtype': ent1 + '_' + ent2,\n",
    "                    'e1type': ent1,\n",
    "                    'e2type': ent2,\n",
    "                    'path': path\n",
    "                })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(data_chunk, annot_chunk):\n",
    "    result = []\n",
    "\n",
    "    for document in range(data_chunk.shape[0]):\n",
    "        annotation = annot.get([dataset.iloc[document]['docid']])\n",
    "        if annotation:\n",
    "            result += process_document(dataset.iloc[document], annotation)\n",
    "        \n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'final_data'\n",
    "\n",
    "for dataset_file in tqdm(glob.glob(os.path.join(trex_path, '*.json'))):\n",
    "    data_chunk = pd.read_json(dataset_file)\n",
    "    annot_chunk = dataset_file.replace(trex_path, annot_path)\n",
    "    features = extract_triples(data_chunk, annot_chunk)\n",
    "    features.to_pickle(dataset_file.replace(trex_path, data_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('./pylingtools/src/')\n",
    "sys.path.append('./pyexling/src/')\n",
    "sys.path.append('./syntaxnet_wrapper/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../logs/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logPath = '../logs/'\n",
    "! mkdir $logPath\n",
    "fileName = 'main.log'\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "fileHandler = logging.FileHandler(os.path.join(logPath, fileName))\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b19189716034ece8059cdd306ad96af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=462.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "data = []\n",
    "\n",
    "for file in tqdm(glob(RESULT_PATH + '/*.pkl')):\n",
    "    data.append(pd.read_pickle(file))\n",
    "    \n",
    "data = pd.concat(data).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61630, 6)\n"
     ]
    }
   ],
   "source": [
    "data.fillna(0, inplace=True)\n",
    "data = data.drop_duplicates(['_subject', '_relation', '_object'])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was                         1507\n",
       "directed by                  888\n",
       "is                           815\n",
       "is city in                   722\n",
       "starring                     707\n",
       "                            ... \n",
       "is co-production between       1\n",
       "despised                       1\n",
       "fix                            1\n",
       "had run                        1\n",
       "rule castile                   1\n",
       "Name: _relation, Length: 15014, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data._relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfb624b45af404fa0d6308a9fdac179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61630.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0b8dce7ebe436599cfa6a7fe69b8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61630.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07d6119f5d9434589575d0730292806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61630.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def extract_matrix(row, predicate=False):\n",
    "    _matrix = np.concatenate([row['ner'], row['postag']], axis=1)#.flatten()\n",
    "    if predicate:\n",
    "        _matrix = np.concatenate([_matrix, row['w2v'], [row['prep'], row['prep'], row['prep']]], axis=1)#.flatten()\n",
    "    return _matrix\n",
    "\n",
    "data['object_matr'] = data.object.progress_map(extract_matrix)\n",
    "data['subject_matr'] = data.subject.progress_map(extract_matrix)\n",
    "data['relation_matr'] = data.relation.progress_map(lambda row: extract_matrix(row, predicate=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object, _subject, _relation = data.object_matr.values, data.subject_matr.values, data.relation_matr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61630, 3, 41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(_object).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_object = np.stack(_object)\n",
    "_subject = np.stack(_subject)\n",
    "_relation = np.stack(_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalers = [{}, {}, {}]\n",
    "\n",
    "for i in range(_object.shape[1]):\n",
    "    scalers[0][i] = StandardScaler()\n",
    "    _object[:, i, :] = scalers[0][i].fit_transform(_object[:, i, :]) \n",
    "\n",
    "for i in range(_subject.shape[1]):\n",
    "    scalers[1][i] = StandardScaler()\n",
    "    _subject[:, i, :] = scalers[1][i].fit_transform(_subject[:, i, :]) \n",
    "    \n",
    "for i in range(_relation.shape[1]):\n",
    "    scalers[2][i] = StandardScaler()\n",
    "    _relation[:, i, :] = scalers[2][i].fit_transform(_relation[:, i, :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from autoencoder_models import Mish, mish\n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 41), (3, 41), (3, 382)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape=[_subject.shape[1:], _object.shape[1:], _relation.shape[1:]]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_subject (InputLayer)      [(None, 3, 41)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_object (InputLayer)       [(None, 3, 41)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rel (InputLayer)          [(None, 3, 382)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 3, 41)        0           input_subject[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNoise (None, 3, 41)        0           input_object[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNoise (None, 3, 382)       0           input_rel[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3, 32)        2656        gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3, 32)        2656        gaussian_noise_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3, 128)       97920       gaussian_noise_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 2, 32)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 32)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 2, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2, 8)         520         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2, 8)         520         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2, 32)        8224        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 8)         0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 8)         0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 32)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Concatenate)         (None, 48)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 48)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 48)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 48)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_conv1 (Conv1D)   (None, 1, 64)        3136        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_object_conv1 (Conv1D)    (None, 1, 64)        3136        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_conv1 (Conv1D)       (None, 1, 128)       6272        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_up1 (UpSampling1 (None, 3, 64)        0           output_subject_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output_object_up1 (UpSampling1D (None, 3, 64)        0           output_object_conv1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_up1 (UpSampling1D)   (None, 3, 128)       0           output_rel_conv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_subject_conv2 (Conv1D)   (None, 3, 41)        15785       output_subject_up1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "output_object_conv2 (Conv1D)    (None, 3, 41)        15785       output_object_up1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_rel_conv2 (Conv1D)       (None, 3, 382)       293758      output_rel_up1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 450,368\n",
      "Trainable params: 450,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "241/241 [==============================] - 3s 10ms/step - loss: 2.2120 - output_subject_conv2_loss: 0.7178 - output_object_conv2_loss: 0.7266 - output_rel_conv2_loss: 0.7677\n",
      "Epoch 2/3\n",
      "241/241 [==============================] - 2s 9ms/step - loss: 1.5442 - output_subject_conv2_loss: 0.5154 - output_object_conv2_loss: 0.3964 - output_rel_conv2_loss: 0.6324\n",
      "Epoch 3/3\n",
      "241/241 [==============================] - 2s 9ms/step - loss: 1.2488 - output_subject_conv2_loss: 0.3919 - output_object_conv2_loss: 0.2722 - output_rel_conv2_loss: 0.5847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75e83d25d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autoencoder_models import noised_ae\n",
    "\n",
    "model = noised_ae(input_shape=input_shape)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit(x=[_subject, _object, _relation],\n",
    "          y=[_subject, _object, _relation], epochs=3, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IDEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import deep_clustering\n",
    "\n",
    "save_dir = 'models/idec/noised_ae'\n",
    "directory = os.path.dirname(save_dir)\n",
    "if not Path(directory).is_dir():\n",
    "    ! mkdir $save_dir\n",
    "\n",
    "def train_idec(autoencoder, n_clusters, score_threshold=1e-5, save_dir=save_dir, partial_init=False):\n",
    "    _directory = save_dir + f'/idec_{n_clusters}'\n",
    "    ! mkdir $_directory\n",
    "    idec = deep_clustering.IDEC(input_shape=input_shape,\n",
    "                                autoencoder_ctor=lambda input_shape: autoencoder(input_shape),  # select model here\n",
    "                                n_clusters=n_clusters,\n",
    "                                pretrain_epochs=100,\n",
    "                                max_iter=300,\n",
    "                                partial_init=partial_init,\n",
    "                                save_dir=_directory, \n",
    "                                log_dir=logPath)\n",
    "\n",
    "    idec.compile(optimizer='adam')\n",
    "    idec.fit([_subject, _object, _relation])\n",
    "\n",
    "    # dump data somewhere\n",
    "    y_pred = idec._y_pred\n",
    "    dumb_features = data[['_subject', '_relation', '_object']]\n",
    "    dumb_features['cluster'] = y_pred\n",
    "    scores = idec.score_examples([_subject, _object, _relation])\n",
    "    dumb_features['score'] = scores\n",
    "    dumb_features = dumb_features[dumb_features['score'] > score_threshold]\n",
    "    dumb_features.to_pickle(f'clusterized/idec_clusters_{n_clusters}_partial{str(partial_init)}.pkl')\n",
    "    \n",
    "    return idec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/idec/noised_ae/idec_32’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 12:39:45,099 [MainThread  ] [INFO ]  Pretraining...\n",
      "2020-11-09 12:42:43,917 [MainThread  ] [INFO ]  Pretraining time: 178.8123025894165\n",
      "2020-11-09 12:42:43,982 [MainThread  ] [INFO ]  Pretrained weights are saved to models/idec/noised_ae/idec_32/pretrain_cae_model.h5\n",
      "2020-11-09 12:42:43,982 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "2020-11-09 12:43:02,060 [MainThread  ] [INFO ]  Cluster centers initialized: 18.077159643173218\n",
      "2020-11-09 12:43:02,060 [MainThread  ] [INFO ]  Training model.\n",
      "2020-11-09 12:43:02,061 [MainThread  ] [INFO ]  Update interval 140\n",
      "2020-11-09 12:43:02,061 [MainThread  ] [INFO ]  Save interval 1203.7109375\n",
      "2020-11-09 12:43:02,061 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2020-11-09 12:43:11,731 [MainThread  ] [INFO ]  saving model to: models/idec/noised_ae/idec_32/dcec_model_0.h5\n",
      "2020-11-09 12:43:13,137 [MainThread  ] [INFO ]  Training model. Iteration #140.\n",
      "2020-11-09 12:43:21,980 [MainThread  ] [INFO ]  Loss: [0.6569353938102722, 0.08355055004358292, 0.04604754224419594, 0.06547029316425323, 0.46186700463294983]\n",
      "2020-11-09 12:43:21,981 [MainThread  ] [INFO ]  delta_label: 0.05091676131754016\n",
      "2020-11-09 12:43:23,965 [MainThread  ] [INFO ]  Training model. Iteration #280.\n",
      "2020-11-09 12:43:32,278 [MainThread  ] [INFO ]  Loss: [0.6386744976043701, 0.10655272752046585, 0.04603346064686775, 0.05055905878543854, 0.4355292320251465]\n",
      "2020-11-09 12:43:32,278 [MainThread  ] [INFO ]  delta_label: 0.046584455622261885\n",
      "2020-11-09 12:43:32,465 [MainThread  ] [INFO ]  Done. 30.40467119216919\n",
      "2020-11-09 12:43:32,466 [MainThread  ] [INFO ]  Saving model to: models/idec/noised_ae/idec_32/dcec_model_final.h5\n",
      "2020-11-09 12:43:32,484 [MainThread  ] [INFO ]  Pretrain time: 178.88330078125\n",
      "2020-11-09 12:43:32,484 [MainThread  ] [INFO ]  Clustering time: 48.50094962120056\n",
      "2020-11-09 12:43:32,484 [MainThread  ] [INFO ]  Total time: 227.38425040245056\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/idec/noised_ae/idec_32’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 12:43:46,243 [MainThread  ] [INFO ]  Pretraining...\n",
      "2020-11-09 12:47:44,967 [MainThread  ] [INFO ]  Pretraining time: 238.7174768447876\n",
      "2020-11-09 12:47:45,032 [MainThread  ] [INFO ]  Pretrained weights are saved to models/idec/noised_ae/idec_32/pretrain_cae_model.h5\n",
      "2020-11-09 12:47:45,032 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "2020-11-09 12:48:02,079 [MainThread  ] [INFO ]  Cluster centers initialized: 17.046117305755615\n",
      "2020-11-09 12:48:02,079 [MainThread  ] [INFO ]  Training model.\n",
      "2020-11-09 12:48:02,080 [MainThread  ] [INFO ]  Update interval 140\n",
      "2020-11-09 12:48:02,080 [MainThread  ] [INFO ]  Save interval 1203.7109375\n",
      "2020-11-09 12:48:02,080 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2020-11-09 12:48:10,467 [MainThread  ] [INFO ]  saving model to: models/idec/noised_ae/idec_32/dcec_model_0.h5\n",
      "2020-11-09 12:48:11,876 [MainThread  ] [INFO ]  Training model. Iteration #140.\n",
      "2020-11-09 12:48:18,976 [MainThread  ] [INFO ]  Loss: [0.5339834094047546, 0.0613832026720047, 2.621545900183264e-05, 7.345141057157889e-05, 0.4725005626678467]\n",
      "2020-11-09 12:48:18,977 [MainThread  ] [INFO ]  delta_label: 0.05138731137433068\n",
      "2020-11-09 12:48:21,046 [MainThread  ] [INFO ]  Training model. Iteration #280.\n",
      "2020-11-09 12:48:27,836 [MainThread  ] [INFO ]  Loss: [0.5890268087387085, 0.12076365947723389, 1.154203710029833e-05, 3.71284295397345e-05, 0.4682145118713379]\n",
      "2020-11-09 12:48:27,837 [MainThread  ] [INFO ]  delta_label: 0.047460652279733895\n",
      "2020-11-09 12:48:28,023 [MainThread  ] [INFO ]  Done. 25.943201065063477\n",
      "2020-11-09 12:48:28,023 [MainThread  ] [INFO ]  Saving model to: models/idec/noised_ae/idec_32/dcec_model_final.h5\n",
      "2020-11-09 12:48:28,042 [MainThread  ] [INFO ]  Pretrain time: 238.78984189033508\n",
      "2020-11-09 12:48:28,043 [MainThread  ] [INFO ]  Clustering time: 43.009591579437256\n",
      "2020-11-09 12:48:28,043 [MainThread  ] [INFO ]  Total time: 281.79943346977234\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/idec/noised_ae/idec_40’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 12:48:43,825 [MainThread  ] [INFO ]  Pretraining...\n",
      "2020-11-09 12:52:34,979 [MainThread  ] [INFO ]  Pretraining time: 231.14883375167847\n",
      "2020-11-09 12:52:35,036 [MainThread  ] [INFO ]  Pretrained weights are saved to models/idec/noised_ae/idec_40/pretrain_cae_model.h5\n",
      "2020-11-09 12:52:35,037 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "2020-11-09 12:52:56,003 [MainThread  ] [INFO ]  Cluster centers initialized: 20.965651750564575\n",
      "2020-11-09 12:52:56,003 [MainThread  ] [INFO ]  Training model.\n",
      "2020-11-09 12:52:56,004 [MainThread  ] [INFO ]  Update interval 140\n",
      "2020-11-09 12:52:56,004 [MainThread  ] [INFO ]  Save interval 1203.7109375\n",
      "2020-11-09 12:52:56,004 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2020-11-09 12:53:02,545 [MainThread  ] [INFO ]  saving model to: models/idec/noised_ae/idec_40/dcec_model_0.h5\n",
      "2020-11-09 12:53:03,936 [MainThread  ] [INFO ]  Training model. Iteration #140.\n",
      "2020-11-09 12:53:09,371 [MainThread  ] [INFO ]  Loss: [0.5291867256164551, 0.05888606980443001, 7.081562558575172e-10, 5.746544906592987e-10, 0.47030067443847656]\n",
      "2020-11-09 12:53:09,372 [MainThread  ] [INFO ]  delta_label: 0.07179944832062307\n",
      "2020-11-09 12:53:11,275 [MainThread  ] [INFO ]  Training model. Iteration #280.\n",
      "2020-11-09 12:53:17,217 [MainThread  ] [INFO ]  Loss: [0.5700623989105225, 0.10579746961593628, 9.315114235519673e-10, 1.1996866833285935e-09, 0.46426495909690857]\n",
      "2020-11-09 12:53:17,217 [MainThread  ] [INFO ]  delta_label: 0.06879766347558007\n",
      "2020-11-09 12:53:17,399 [MainThread  ] [INFO ]  Done. 21.395201206207275\n",
      "2020-11-09 12:53:17,399 [MainThread  ] [INFO ]  Saving model to: models/idec/noised_ae/idec_40/dcec_model_final.h5\n",
      "2020-11-09 12:53:17,418 [MainThread  ] [INFO ]  Pretrain time: 231.2126693725586\n",
      "2020-11-09 12:53:17,418 [MainThread  ] [INFO ]  Clustering time: 42.38040113449097\n",
      "2020-11-09 12:53:17,418 [MainThread  ] [INFO ]  Total time: 273.59307050704956\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/idec/noised_ae/idec_40’: File exists\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-09 12:53:33,595 [MainThread  ] [INFO ]  Pretraining...\n",
      "2020-11-09 12:56:03,362 [MainThread  ] [INFO ]  Pretraining time: 149.76065254211426\n",
      "2020-11-09 12:56:03,407 [MainThread  ] [INFO ]  Pretrained weights are saved to models/idec/noised_ae/idec_40/pretrain_cae_model.h5\n",
      "2020-11-09 12:56:03,408 [MainThread  ] [INFO ]  Initializing cluster centers.\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n",
      "2020-11-09 12:56:26,460 [MainThread  ] [INFO ]  Cluster centers initialized: 23.051671028137207\n",
      "2020-11-09 12:56:26,460 [MainThread  ] [INFO ]  Training model.\n",
      "2020-11-09 12:56:26,461 [MainThread  ] [INFO ]  Update interval 140\n",
      "2020-11-09 12:56:26,461 [MainThread  ] [INFO ]  Save interval 1203.7109375\n",
      "2020-11-09 12:56:26,462 [MainThread  ] [INFO ]  Training model. Iteration #0.\n",
      "2020-11-09 12:56:37,555 [MainThread  ] [INFO ]  saving model to: models/idec/noised_ae/idec_40/dcec_model_0.h5\n",
      "2020-11-09 12:56:38,949 [MainThread  ] [INFO ]  Training model. Iteration #140.\n",
      "2020-11-09 12:56:48,703 [MainThread  ] [INFO ]  Loss: [0.5304797291755676, 0.05971388518810272, 3.4431998918726947e-10, 2.514474461978722e-10, 0.4707658290863037]\n",
      "2020-11-09 12:56:48,704 [MainThread  ] [INFO ]  delta_label: 0.05591432743793607\n",
      "2020-11-09 12:56:50,687 [MainThread  ] [INFO ]  Training model. Iteration #280.\n",
      "2020-11-09 12:57:00,804 [MainThread  ] [INFO ]  Loss: [0.5726378560066223, 0.10930308699607849, 3.6547251336394027e-10, 2.9453797734113607e-10, 0.4633347690105438]\n",
      "2020-11-09 12:57:00,805 [MainThread  ] [INFO ]  delta_label: 0.04760668505597923\n",
      "2020-11-09 12:57:01,008 [MainThread  ] [INFO ]  Done. 34.547343015670776\n",
      "2020-11-09 12:57:01,009 [MainThread  ] [INFO ]  Saving model to: models/idec/noised_ae/idec_40/dcec_model_final.h5\n",
      "2020-11-09 12:57:01,027 [MainThread  ] [INFO ]  Pretrain time: 149.81299591064453\n",
      "2020-11-09 12:57:01,028 [MainThread  ] [INFO ]  Clustering time: 57.619155406951904\n",
      "2020-11-09 12:57:01,028 [MainThread  ] [INFO ]  Total time: 207.43215131759644\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "idec32 = train_idec(autoencoder=noised_ae, n_clusters=32, partial_init=False)\n",
    "idec32 = train_idec(autoencoder=noised_ae, n_clusters=32, partial_init=True)\n",
    "idec40 = train_idec(autoencoder=noised_ae, n_clusters=40, partial_init=False)\n",
    "idec40 = train_idec(autoencoder=noised_ae, n_clusters=40, partial_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the results data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features = pd.read_pickle('clusterized/idec_clusters_32.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['_subject', '_relation', '_object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     586\n",
       "3     253\n",
       "28    139\n",
       "20     71\n",
       "19     36\n",
       "30     34\n",
       "15     18\n",
       "14     11\n",
       "12     10\n",
       "8       7\n",
       "5       6\n",
       "23      5\n",
       "7       5\n",
       "25      4\n",
       "21      3\n",
       "2       3\n",
       "26      2\n",
       "4       2\n",
       "16      1\n",
       "13      1\n",
       "10      1\n",
       "27      1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb_features[dumb_features._relation.str.contains('directed')]['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(dumb_features.cluster.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "was               68\n",
       "is                56\n",
       "city in           37\n",
       "served            29\n",
       "is located        22\n",
       "                  ..\n",
       "has expressed      1\n",
       "emigrating         1\n",
       "was scene of       1\n",
       "also remaining     1\n",
       "is having          1\n",
       "Name: _relation, Length: 1140, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = 5\n",
    "temp = show_cluster_sample(number, rows=2000)\n",
    "temp._relation.value_counts()\n",
    "#temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply an IDEC model to the QA corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features.to_pickle('clusterized/partial_idec_clusters.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/idec/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dirty_json('dcec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DAEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/daec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/daec/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "daec = deep_clustering.DAEC(input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=50, \n",
    "                            pretrain_epochs=10,\n",
    "                            log_dir=logPath,\n",
    "                            save_dir=save_dir, \n",
    "                            )\n",
    "\n",
    "plot_model(dcec._model, to_file=os.path.join(save_dir, 'daec_model.png'), show_shapes=True)\n",
    "daec.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daec.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = daec._y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dcec.score_examples([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.01\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/daec/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = save_dirty_json('daec_kmeans_80c_002.json', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DC_Kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/dc_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_clustering\n",
    "save_dir = 'models/dc_kmeans/restore_rel'\n",
    "! mkdir $save_dir\n",
    "\n",
    "dckmeans = deep_clustering.DC_Kmeans(\n",
    "                            input_shape=(_subject.shape[1:]),\n",
    "                            autoencoder_ctor=lambda input_shape: restore_rel(input_shape),  # select model here\n",
    "                            n_clusters=30,\n",
    "                            pretrain_epochs=50,\n",
    "                            max_epochs=200,\n",
    "                            save_dir=save_dir, \n",
    "                            log_dir=logPath)\n",
    "plot_model(dckmeans._model, to_file=os.path.join(save_dir, 'dckmeans_model.png'), show_shapes=True)\n",
    "dckmeans.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dckmeans.fit([_subject, _object, _relation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "y_pred = dckmeans.y_pred\n",
    "dumb_features = pd.DataFrame()\n",
    "dumb_features['subject'] = data['subject'].map(get_tokens)\n",
    "dumb_features['relation'] = data['relation'].map(get_tokens)\n",
    "dumb_features['object'] = data['object'].map(get_tokens)\n",
    "dumb_features['cluster'] = y_pred\n",
    "scores = dckmeans.get_scores([_subject, _object, _relation])\n",
    "dumb_features['score'] = scores\n",
    "threshold = 0.05\n",
    "dumb_features = dumb_features[dumb_features['score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number, rows=100):\n",
    "    def get_tokens(column):\n",
    "        return ' '.join(column['tokens'])\n",
    "    \n",
    "    cluster = dumb_features[dumb_features.cluster == number]\n",
    "    return cluster[['subject', 'relation', 'object', 'score']].iloc[:rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.object == 'eliza'].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_features[dumb_features.relation.str.contains('born')].sort_values('cluster').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 4\n",
    "temp = show_cluster_sample(number)\n",
    "temp.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DIRTY_JSON = 'unfiltered_results/dc_kmeans/restore_rel/'\n",
    "! mkdir $PATH_DIRTY_JSON\n",
    "\n",
    "def save_dirty_json(id, y_pred):\n",
    "    result = {}\n",
    "    number_of_clusters = y_pred.max()\n",
    "    for number in range(number_of_clusters):\n",
    "        sample = show_cluster_sample(number, 999).sort_values('score', ascending=False)\n",
    "        cluster = {\n",
    "            \"data\": list(zip(*[sample[c].values.tolist() for c in sample])),\n",
    "            \"predicates\": {key: int(value) for key, value in dict(sample.relation.value_counts()).items()}\n",
    "        }\n",
    "        result[int(number)] = cluster\n",
    "    \n",
    "    json.dump(result, open(os.path.join(PATH_DIRTY_JSON, id), 'w'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = save_dirty_json('dc_kmeans_30c_000.json', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of internal representations generated by autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pae = plain_ae(x_train.shape[1:])\n",
    "pae.compile(optimizer='adam', loss='mse')\n",
    "pae.fit(x_train, x_train, batch_size=256, epochs=10, verbose=0)\n",
    "hidden = pae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=pae.input, outputs=hidden)\n",
    "#embeddings = encoder.predict(x_train)\n",
    "#cluzeriser = KMeans(2, n_jobs=6)\n",
    "#clusters = cluzeriser.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pae.save('models/pae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_sample(number):\n",
    "    return features[clusters == number][['docid', 'subject', 'relation', 'object']].sample(frac=1).iloc[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pae_clusters.txt', 'w') as f:\n",
    "    for i in range(50):\n",
    "        try:\n",
    "            line = \"\\n\".join(map(str, show_cluster_sample(i).values.tolist()))\n",
    "            f.write(str(i)+'-----------------\\n' + line + '\\n\\n\\n')\n",
    "        except ValueError:\n",
    "            f.write(str(i)+'-----------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

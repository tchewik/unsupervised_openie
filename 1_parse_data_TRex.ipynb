{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U spacy==2.1.0 \n",
    "pip install -U neuralcoref --no-binary neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trex_path = 'trex_data'\n",
    "annot_path = 'trex_corenlp_annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get T-Rex dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/8760241/TREx.zip\n",
    "mkdir $trex_path\n",
    "unzip TREx.zip -d $trex_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize coreference resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "coref = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(coref)\n",
    "\n",
    "def resolve_coreference(text):\n",
    "    doc = coref(text)\n",
    "    return doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_coreference(\"I go to the park. It is beautiful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iniialize Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME = 'vmh2.isa.ru'\n",
    "PORT = 9002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_documents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "aligned_trex_path = 'trex_simpleqa_aligned'\n",
    "aligned_documents = pickle.load(open(f'{aligned_trex_path}.pkl', 'rb'))\n",
    "aligned_documents.update(pickle.load(open(f'{aligned_trex_path}_pt2.pkl', 'rb')))\n",
    "aligned_documents.update(pickle.load(open(f'{aligned_trex_path}_pt3.pkl', 'rb')))\n",
    "aligned_documents.update(pickle.load(open(f'{aligned_trex_path}_pt4.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aligned_documents.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht trex_data/*.json | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import time\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP(f'http://{HOSTNAME}:{PORT}')\n",
    "nlp_properties = {\n",
    "    'annotators': 'tokenize,ssplit,tokenize,pos,ner,depparse,natlog,openie',\n",
    "    'outputFormat': 'json'\n",
    "}\n",
    "\n",
    "deprec_rels = {'to', 'for', 'by', 'with', 'also', 'as of',\n",
    "               'had', 'said', 'said in', 'felt', 'on', 'gave', 'saw', 'found', 'did',\n",
    "               'at', 'as', 'e', 'as', 'de', 'mo', '’s', 'v', 'yr', 'al',\n",
    "               \"'\", 'na', 'v.', \"d'\", 'et', 'mp', 'di', 'y',\n",
    "               'ne', 'c.', 'be', 'ao', 'mi', 'im', 'h',\n",
    "               'has', 'between', 'are', 'returned', 'began', 'became',\n",
    "               'along', 'doors as', 'subsequently terrytoons in',\n",
    "              }\n",
    "\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    # save only triplets with at least one named entity\n",
    "    # and does not contain a deprecated relation\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if not len(triplet['relation']) < 2:\n",
    "            if not triplet['relation'] in deprec_rels: \n",
    "                for entity in sentence['entitymentions']:\n",
    "                    if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                        openie.append(triplet)\n",
    "                        continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "\n",
    "def corenlp_annotate(text):\n",
    "    text = resolve_coreference(text)\n",
    "    try:\n",
    "        return nlp.annotate(text, properties=nlp_properties)['sentences']\n",
    "    except TypeError:\n",
    "        time.sleep(10)\n",
    "        result = nlp.annotate(text, properties=nlp_properties)\n",
    "        if type(result) == str:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "broken_ids = []\n",
    "\n",
    "# for dataset_file in tqdm(glob.glob(os.path.join(trex_path, '*.json'))[205:214]):\n",
    "    # for dataset_file in tqdm(['trex_data/re-nlg_4500000-4510000.json',]):\n",
    "    \n",
    "for dataset_file in tqdm(aligned_documents.keys()):\n",
    "    if not os.path.isfile(dataset_file.replace(trex_path, annot_path)):\n",
    "        dataset = pd.read_json(dataset_file).loc[aligned_documents[dataset_file]]\n",
    "        nlp_annot = {}\n",
    "\n",
    "        for document in tqdm(range(dataset.shape[0])):\n",
    "            docid = dataset.iloc[document].docid.split('/')[-1]\n",
    "\n",
    "            doc_annot = corenlp_annotate(dataset.iloc[document].text)\n",
    "            if not doc_annot:\n",
    "                broken_ids.append(id)\n",
    "                continue\n",
    "\n",
    "            clean_annot = []\n",
    "            for sentence in doc_annot:\n",
    "                new_sentence = sentence\n",
    "                new_sentence['openie'] = filter_ner(sentence)\n",
    "                if new_sentence['openie']:\n",
    "                    clean_annot.append(new_sentence)\n",
    "\n",
    "            nlp_annot.update({\n",
    "                docid: clean_annot,\n",
    "            })\n",
    "\n",
    "        json.dump(nlp_annot, open(dataset_file.replace(trex_path, annot_path), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Who was Homi K. Bhabha especially influenced by?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.annotate(text, properties=nlp_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht trex_corenlp_annotations/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht $annot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import time\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP(f'http://{HOSTNAME}:{PORT}')\n",
    "nlp_properties = {\n",
    "    'annotators': 'tokenize,ssplit,tokenize,pos,ner,depparse,natlog,openie',\n",
    "    'outputFormat': 'json'\n",
    "}\n",
    "\n",
    "deprec_rels = {'in', 'is', 'was', 'of', \"'s\", 'to', 'for', 'by', 'with', 'also', 'as of',\n",
    "               'had', 'said', 'said in', 'felt', 'on', 'gave', 'saw', 'found', 'did',\n",
    "               'at', 'as', 'e', 'as', 'de', 'mo', '’s', 'v', 'yr', 'al',\n",
    "               \"'\", 'na', 'v.', \"d'\", 'et', 'mp', 'di', 'y',\n",
    "               'ne', 'c.', 'be', 'ao', 'mi', 'im', 'h',\n",
    "               'has', 'between', 'are', 'returned', 'began', 'became',\n",
    "               'along', 'doors as', 'subsequently terrytoons in',\n",
    "              }\n",
    "\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    # save only triplets with at least one named entity\n",
    "    # and does not contain a deprecated relation\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if not len(triplet['relation']) < 3:\n",
    "            if not triplet['relation'] in deprec_rels: \n",
    "                for entity in sentence['entitymentions']:\n",
    "                    if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                        openie.append(triplet)\n",
    "                        continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "\n",
    "def corenlp_annotate(text):\n",
    "    text = resolve_coreference(text)\n",
    "    try:\n",
    "        return nlp.annotate(text, properties=nlp_properties)['sentences']\n",
    "    except TypeError:\n",
    "        time.sleep(10)\n",
    "        result = nlp.annotate(text, properties=nlp_properties)\n",
    "        if type(result) == str:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "broken_ids = []\n",
    "\n",
    "for dataset_file in tqdm(glob.glob(os.path.join(trex_path, '*.json'))[196:]):\n",
    "    dataset = pd.read_json(dataset_file).loc[aligned_documents[dataset_file]]\n",
    "    nlp_annot = {}\n",
    "\n",
    "    for document in tqdm(range(dataset.shape[0])):\n",
    "        docid = dataset.iloc[document].docid.split('/')[-1]\n",
    "        \n",
    "        doc_annot = corenlp_annotate(dataset.iloc[document].text)\n",
    "        if not doc_annot:\n",
    "            broken_ids.append(id)\n",
    "            continue\n",
    "            \n",
    "        clean_annot = []\n",
    "        for sentence in doc_annot:\n",
    "            new_sentence = sentence\n",
    "            new_sentence['openie'] = filter_ner(sentence)\n",
    "            if new_sentence['openie']:\n",
    "                clean_annot.append(new_sentence)\n",
    "        \n",
    "        nlp_annot.update({\n",
    "            docid: clean_annot,\n",
    "        })\n",
    "        \n",
    "    json.dump(nlp_annot, open(dataset_file.replace(trex_path, annot_path), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob(os.path.join(trex_path, '*.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht $annot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls -laht $annot_path | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(nlp_annot, open(dataset_file.replace(trex_path, annot_path), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir trex_corenlp_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_annot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_item(item):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    r = requests.get(url, params = {'format': 'json', 'search': item, 'action': 'wbsearchentities', 'type': 'item', 'language': 'en'})\n",
    "    data = r.json()\n",
    "    return data['search'][0]['label'] if len(data['search']) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_item('Q6255339')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht $annot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "_digits = re.compile('\\d')\n",
    "\n",
    "def extract_tokens(annotation, arg1, arg2):\n",
    "    def find_in_sentence(sentence_annotation, argument_annotation):\n",
    "        start_token = 0        \n",
    "        for token in sentence_annotation['tokens']:\n",
    "            #print('>>', argument_annotation)\n",
    "            if argument_annotation.get('boundaries'):\n",
    "                if token.get('characterOffsetBegin') == argument_annotation.get('boundaries')[0]:\n",
    "                    start_token = token['index']\n",
    "                if token.get('characterOffsetEnd') == argument_annotation.get('boundaries')[1]:\n",
    "                    if start_token == token['index']:\n",
    "                        # entity contains one token\n",
    "                        return [token['index']]  # begin with 1!\n",
    "                    if start_token:\n",
    "                        return [ind for ind in range(start_token, token['index'] + 1)]\n",
    "                    \n",
    "            else:\n",
    "                if token['originalText'] == argument_annotation['surfaceform']:\n",
    "                    # entity contains one token\n",
    "                    return [token['index']]\n",
    "                if start_token:\n",
    "                    return [ind for ind in range(start_token, token['index'] + 1)]\n",
    "            \n",
    "    \n",
    "    for i, sentence in enumerate(annotation):\n",
    "        tok1 = find_in_sentence(sentence, arg1)\n",
    "        if tok1:\n",
    "            tok2 = find_in_sentence(sentence, arg2)\n",
    "            if tok2:\n",
    "                return [i, tok1, tok2]\n",
    "    return [-1, -1, -1]\n",
    "\n",
    "def _get_bow_between(tokens, tok1, tok2):\n",
    "    tmp = []\n",
    "    result = []\n",
    "    tok_left, tok_right = sorted([tok1, tok2])\n",
    "    for word in [tokens[i-1]['originalText'] for i in range(max(tok_left) + 1, min(tok_right))]:\n",
    "        for pun in string.punctuation:\n",
    "            word = word.strip(pun)\n",
    "        if word != '':\n",
    "            tmp.append(word.lower())\n",
    "    for word in tmp:\n",
    "        if word not in stopwords_list and not _digits.search(word) and not word[0].isupper():\n",
    "            result.append(word)\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_pos_between(tokens, tok1, tok2):\n",
    "    result = []\n",
    "    tok_left, tok_right = sorted([tok1, tok2])\n",
    "    for pos in [tokens[i-1]['pos'] for i in range(max(tok_left) + 1, min(tok_right))]:\n",
    "        if pos not in string.punctuation:\n",
    "            result.append(pos)\n",
    "    return '_'.join(result)\n",
    "\n",
    "def _get_dep_path(dependencies, start, end):\n",
    "    \"\"\"\n",
    "    Finds the shortest dependency path between two tokens in a sentence.\n",
    "        Args:\n",
    "            dependencies(list): List of dependencies in Stanford CoreNLP style\n",
    "            start(int): Number of the first token\n",
    "            end(int): Number of the second token\n",
    "        Returns:\n",
    "            list of tokens [start ... end] as they are presented in the shortest dependency path\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    deps = {}\n",
    "\n",
    "    for edge in dependencies:\n",
    "        edges.append((edge['governor'], edge['dependent']))\n",
    "        deps[(min(edge['governor'], edge['dependent']),\n",
    "              max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "    graph = nx.Graph(edges)\n",
    "    path = nx.shortest_path(graph, source=start, target=end)\n",
    "    return [p for p in path]\n",
    "\n",
    "def _get_shortest_path(dependencies, left_set, right_set):\n",
    "    \"\"\"\n",
    "    Finds the shortest dependency path between two sets of tokens in a sentence.\n",
    "    \"\"\"\n",
    "    result = [1] * len(dependencies)\n",
    "    for a in left_set:\n",
    "        for b in right_set:\n",
    "            candidate = _get_dep_path(dependencies, a, b)\n",
    "            if len(candidate) < len(result):\n",
    "                result = candidate\n",
    "    return result    \n",
    "\n",
    "def _get_words_dep(tokens, dependency_path):\n",
    "    result = [tokens[i-1]['word'] for i in dependency_path[1:-1]]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def _get_trigger(tokens, dependency_path):\n",
    "    result = []\n",
    "    for word in [tokens[i-1]['lemma'] for i in dependency_path[1:-1]]:\n",
    "        if word not in stopwords_list:\n",
    "            result.append(word)\n",
    "    return '|'.join(result)\n",
    "\n",
    "def _get_entity_type(tokens, tok):\n",
    "    _replace = {\n",
    "        'PERSON_PERSON': 'PERSON',\n",
    "        'ORGANIZATION_ORGANIZATION': 'ORGANIZATION'\n",
    "    }\n",
    "    result = '_'.join([tokens[token-1].get('ner') for token in tok])\n",
    "    for key, value in _replace.items():\n",
    "        result = result.replace(key, value)\n",
    "    return result\n",
    "\n",
    "def process_document(document, annotation):\n",
    "    docid = document['docid'].split('/')[-1]\n",
    "    #annotation = json.load(open(os.path.join('corenlp_annotations', docid + '.json'), 'r'))['sentences']\n",
    "    result = []\n",
    "    \n",
    "    for triple in document['triples']:\n",
    "        if triple['object']['surfaceform'] and triple['subject']['surfaceform']:\n",
    "            #  print('>>>', triple)\n",
    "            #  print('<<<', annotation[0])\n",
    "            act_sent, tok1, tok2 = extract_tokens(annotation, \n",
    "                                                  triple['object'],\n",
    "                                                  triple['subject'])\n",
    "            if act_sent > -1:\n",
    "                surface1 = '_'.join(triple['object']['surfaceform'].split())\n",
    "                surface2 = '_'.join(triple['subject']['surfaceform'].split())\n",
    "#                 surface_pred = '_'.join()\n",
    "                #surface_pred = '_'.join(triple['predicate']['surfaceform'].split())\n",
    "                bow = _get_bow_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                dependency_path = _get_shortest_path(annotation[act_sent]['enhancedPlusPlusDependencies'], tok1, tok2)\n",
    "                trigger = _get_trigger(annotation[act_sent]['tokens'], dependency_path)\n",
    "                pos = _get_pos_between(annotation[act_sent]['tokens'], tok1, tok2)\n",
    "                ent1 = _get_entity_type(annotation[act_sent]['tokens'], tok1)\n",
    "                ent2 = _get_entity_type(annotation[act_sent]['tokens'], tok2)\n",
    "                path = _get_words_dep(annotation[act_sent]['tokens'], dependency_path)\n",
    "                \n",
    "                result.append({\n",
    "                    '_docid': docid,\n",
    "                    '_tok1': tok1,\n",
    "                    '_tok2': tok2,\n",
    "                    #'_pred': surface_pred,\n",
    "                    '_sent_id': triple['sentence_id'],\n",
    "                    '_sentence': act_sent,\n",
    "                    '_dep_path': dependency_path,\n",
    "                    ## Titov features\n",
    "                    'bow': bow,\n",
    "                    'e1': surface1, \n",
    "                    'e2': surface2,\n",
    "                    'trigger': trigger,\n",
    "                    'pos': pos,\n",
    "                    'pairtype': ent1 + '_' + ent2,\n",
    "                    'e1type': ent1,\n",
    "                    'e2type': ent2,\n",
    "                    'path': path,\n",
    "                    'relation': triple['predicate']['uri']\n",
    "                })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(data_chunk, annot_chunk):\n",
    "    result = []\n",
    "\n",
    "    for index, row in data_chunk.iterrows():\n",
    "        annotation = annot_chunk.get(row['docid'].split('/')[-1])\n",
    "        if annotation:\n",
    "            result += process_document(row, annotation)\n",
    "        \n",
    "    return pd.DataFrame(result).drop_duplicates(['e1', 'e2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(dataset_file):\n",
    "    data_chunk = pd.read_json(dataset_file)\n",
    "    annot_chunk = json.load(open(dataset_file.replace(trex_path, annot_path), 'r'))\n",
    "    features = extract_triples(data_chunk, annot_chunk)\n",
    "    features = features[features['_sentence'] > -1]  # filter entities not given in the same sentence\n",
    "    features.to_pickle(dataset_file.replace(trex_path, data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_path = 'final_data_ra'\n",
    "! mkdir $data_path\n",
    "\n",
    "pool = Pool(4)\n",
    "files = glob.glob(os.path.join(trex_path, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "data_path = 'final_data_ra'\n",
    "! mkdir $data_path\n",
    "\n",
    "pool = Pool(4)\n",
    "files = glob.glob(os.path.join(trex_path, '*.json'))\n",
    "pool.map(process_file, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_chunk = json.load(open(files[0].replace(trex_path, annot_path), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_chunk.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_chunk['Q15052'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_chunk['Q15052'][1]['openie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk.iloc[0].triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    data_chunk = pd.read_json(file)\n",
    "    data_chunk = data_chunk[data_chunk.text.str.contains(\"Yves Klein\")]\n",
    "    if data_chunk.shape[0] != 0:\n",
    "        print(file)\n",
    "        print(data_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(path):\n",
    "    result = []\n",
    "    all_files = glob.glob(path + '*.json')\n",
    "    \n",
    "    for file in all_files:\n",
    "        df = pd.read_pickle(file)\n",
    "        result += df['relation'].values.tolist()\n",
    "        \n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'final_data_ra/'\n",
    "result = []\n",
    "\n",
    "all_files = glob.glob(path + '*.json')    \n",
    "for file in all_files:\n",
    "    df = pd.read_pickle(file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = get_relations('final_data_ra/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels.value_counts()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels.value_counts()[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yao_like(path, of):\n",
    "    result = []\n",
    "    all_files = glob.glob(path + '*.json')\n",
    "    _train = int(len(all_files) * 0.6)\n",
    "    _dev = (len(all_files) - _train) // 2\n",
    "    \n",
    "    for file in tqdm(all_files):\n",
    "        df = pd.read_pickle(file)\n",
    "        df.replace('PERSON_PERSON', 'PERSON', inplace=True)\n",
    "        df.replace('ORGANIZATION_ORGANIZATION_ORGANIZATION', 'ORGANIZATION', inplace=True)\n",
    "        df.replace('ORGANIZATION_ORGANIZATION', 'ORGANIZATION', inplace=True)\n",
    "        result.append(df[[key for key in df.keys() if key[0] != '_']])\n",
    "\n",
    "    train = pd.concat(result[:_train])\n",
    "    train.to_csv(of+'_train.csv', sep='\\t', index=None, header=False)\n",
    "    dev = pd.concat(result[_train:_train+_dev])\n",
    "    dev.to_csv(of+'_dev.csv', sep='\\t', index=None, header=False)\n",
    "    test = pd.concat(result[_train+_dev:])\n",
    "    test.to_csv(of+'_test.csv', sep='\\t', index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_yao_like('final_data_ra/', 'trex_ra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -1000 trex_ra_train.csv >> data-sample.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "W2V_MODEL_PATH ='models/'\n",
    "W2V_MODEL_NAME = 'wiki-news-300d-1M.vec.zip'  # 1.6G\n",
    "\n",
    "directory = os.path.dirname(W2V_MODEL_PATH)\n",
    "if not Path(directory).is_dir():\n",
    "    print(f'Creating directory at {directory}',\n",
    "          ' for saving word2vec pre-trained model')\n",
    "    os.makedirs(directory)\n",
    "if not Path(W2V_MODEL_PATH).is_file():\n",
    "    w2v_archive = os.path.join(directory, W2V_MODEL_NAME)\n",
    "    if not Path(w2v_archive).is_file():\n",
    "        url = f'https://dl.fbaipublicfiles.com/fasttext/vectors-english/{W2V_MODEL_NAME}'\n",
    "        print(f'Downloading word2vec pre-trained model to {w2v_archive}')\n",
    "        wget.download(url, os.path.join(directory, W2V_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "\n",
    "if W2V_MODEL_NAME[-4:] in ['.vec', '.bin']:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME,\n",
    "                                                   binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-4:] == '.zip':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME[:-4],\n",
    "                                               binary=W2V_MODEL_NAME[-4:] == '.bin')\n",
    "elif W2V_MODEL_NAME[-7:] == '.bin.gz':\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(W2V_MODEL_PATH + W2V_MODEL_NAME, binary=True)\n",
    "    \n",
    "else:\n",
    "    word2vec_model = Word2Vec.load(W2V_MODEL_PATH + W2V_MODEL_NAME)\n",
    "    \n",
    "word2vec_vector_length = len(word2vec_model.wv.get_vector('tree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature rich dataset ``data/dataset.pkl`` out of corenlp annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplets exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preps = \"above across after against along among around at away before behind below beneath beside between by down during for from in front inside into near next of off on onto out outside over\tthrough till to toward under underneath until up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preps.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from iteration_utilities import unique_everseen\n",
    "\n",
    "\n",
    "def _extract_plain_features(document):\n",
    "    def _extract(sentence):\n",
    "\n",
    "        def get_postags_sequence(span, words, predicate=False):\n",
    "            columns = ['JJ', 'CD', 'VBD', '', 'RB', 'VBN', 'PRP', 'IN', 'VBP', 'TO', 'NNP', 'VB',\n",
    "                       'VBZ', 'VBG', 'POS', 'NNS', 'NN', 'MD']\n",
    "\n",
    "            sequence = [token['pos'] for token in sentence['tokens'][span[0]:span[1]]\n",
    "                        if token['originalText'] in words][:3]\n",
    "\n",
    "#             if predicate or {'NN', 'NNP', 'NNS', 'CD'}.intersection(set(sequence)):\n",
    "            if predicate or 'NNP' in set(sequence) or 'CD' in set(sequence):\n",
    "                sequence = [[int(column == postag) for column in columns] for postag in sequence]\n",
    "            else:\n",
    "                sequence = []\n",
    "\n",
    "            result = np.zeros((3, len(columns)))\n",
    "\n",
    "            if sequence:\n",
    "                result[:len(sequence)] = sequence\n",
    "\n",
    "            return result\n",
    "\n",
    "        def get_ner_occurrences(span, words, obj=True):\n",
    "            _ner_kinds = ['TITLE', 'COUNTRY', 'DATE', 'PERSON', 'ORGANIZATION', 'MISC',\n",
    "                          'LOCATION', 'NUMBER', 'CAUSE_OF_DEATH', 'NATIONALITY', 'ORDINAL',\n",
    "                          'DURATION', 'CRIMINAL_CHARGE', 'CITY', 'RELIGION',\n",
    "                          'STATE_OR_PROVINCE', 'IDEOLOGY', 'SET', 'URL', 'PERCENT', 'TIME',\n",
    "                          'MONEY', 'HANDLE']\n",
    "\n",
    "            mentions = [token['ner'] for token in sentence['tokens'][span[0]:span[1]]\n",
    "                        if token['originalText'] in words]\n",
    "\n",
    "            mentions = [[int(_ner_kind == mention) for _ner_kind in _ner_kinds] for mention in mentions][:3]\n",
    "            result = np.zeros((3, len(_ner_kinds)))\n",
    "\n",
    "            if mentions:\n",
    "                result[:len(mentions)] = mentions\n",
    "\n",
    "            return result\n",
    "        \n",
    "        def get_prep_sequence(words):\n",
    "            _prep_kinds = ['above', 'across', 'after', 'against', 'along', 'among', 'around', 'at', 'away', \n",
    "                           'before', 'behind', 'below', 'beneath', 'beside', 'between', 'by', \n",
    "                           'down', 'during', 'for', 'from', 'in', 'front', 'inside', 'into', \n",
    "                           'near', 'next', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', \n",
    "                           'through', 'till', 'to', 'toward', 'under', 'underneath', 'until', 'up']\n",
    "            \n",
    "            words = words.split(' ')\n",
    "            mentions = [int(prep in words) for prep in _prep_kinds]\n",
    "            result = np.zeros(len(_prep_kinds))\n",
    "            \n",
    "            if mentions:\n",
    "                result[:len(mentions)] = mentions\n",
    "                \n",
    "            return result\n",
    "\n",
    "        def tag_lemma(span, words, tag=False):\n",
    "            if tag:\n",
    "                return [token['lemma'].lower() + '_' + _penn_tagset[token['pos']]['fPOS'] for token in\n",
    "                        sentence['tokens'][span[0]:span[1]]\n",
    "                        if token['originalText'] in words]\n",
    "            else:\n",
    "                return [token['lemma'].lower() for token in sentence['tokens'][span[0]:span[1]]\n",
    "                        if token['originalText'] in words]\n",
    "\n",
    "        def remove_repetition(words):\n",
    "            if words[:len(words) // 2] == words[len(words) // 2:]:\n",
    "                return words[:len(words) // 2]\n",
    "            return words\n",
    "\n",
    "        def get_tokens(words, span):\n",
    "            return [token['originalText'].lower() for token in sentence['tokens'][span[0]:span[1]]\n",
    "                    if token['originalText'] in words]\n",
    "\n",
    "        def _build_dep_path(dependencies, tokens, start: int, end: int):\n",
    "            edges = []\n",
    "            deps = {}\n",
    "\n",
    "            for edge in dependencies:\n",
    "                edges.append((edge['governor'], edge['dependent']))\n",
    "                deps[(min(edge['governor'], edge['dependent']),\n",
    "                      max(edge['governor'], edge['dependent']))] = edge\n",
    "\n",
    "            graph = nx.Graph(edges)\n",
    "            try:\n",
    "                path = nx.shortest_path(graph, source=start, target=end)\n",
    "                return path[:-1]  # exclude right end\n",
    "            except:\n",
    "                return [start, ]\n",
    "\n",
    "        def _tokens_by_index(indexes, tokens):\n",
    "            return [token['originalText'] for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        def _lemmas_by_index(indexes, tokens):\n",
    "            return [token['lemma'].lower() for token in tokens if token['index'] in indexes]\n",
    "\n",
    "        def _embed(placeholder, words):\n",
    "            for j in range(len(words)):\n",
    "                if j == len(placeholder):\n",
    "                    break\n",
    "\n",
    "                word = words[j]\n",
    "                if word and word in word2vec_model:\n",
    "                    placeholder[j, :] = word2vec_model[word]\n",
    "            return placeholder\n",
    "\n",
    "        def _embed_arg(row):\n",
    "            result = []\n",
    "            result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "\n",
    "            return result\n",
    "\n",
    "        #         deprecated = set(['one', 'he', 'she', 'they', 'his', 'her', 'its', 'our', 'day', 'co.', 'inc.', \n",
    "        #               'society', 'people', 'inventor', 'head', 'poet', 'doctor', 'teacher', 'inventor', \n",
    "        #               'thanksgiving day', 'halloween',\n",
    "        #               'sales person', 'model', 'board', 'technology', 'owner', 'one', 'two', 'university', \n",
    "        #                           'fbi', 'patricia churchland', 'century', 'association', 'laboratory', 'academy'])\n",
    "        deprecated = []\n",
    "        deprec_rels = []\n",
    "\n",
    "        triplets = sentence['openie']\n",
    "#         filtered_triplets = filter(\n",
    "#             lambda obj: obj['object'].lower() not in deprecated and obj['subject'].lower() not in deprecated, \n",
    "#             triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['subject'].lower().strip() not in deprecated, \n",
    "                                   triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['object'].lower().strip() not in deprecated, \n",
    "                                   filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: obj['relation'].lower().strip() not in deprec_rels, \n",
    "                                   filtered_triplets)\n",
    "        filtered_triplets = filter(\n",
    "            lambda obj: len(obj['object']) > 2 and len(obj['subject']) > 2 and len(obj['relation']) > 2,\n",
    "            filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['relation'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['subject'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = filter(lambda obj: len(obj['object'].split()) < 4, filtered_triplets)\n",
    "        filtered_triplets = list(filtered_triplets)\n",
    "\n",
    "        subjects, relations, objects, dep_path = [], [], [], []\n",
    "\n",
    "        for triplet in filtered_triplets:\n",
    "            _subject = {\n",
    "                'tokens': get_tokens(triplet['subject'], triplet['subjectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['subjectSpan'], triplet['subject']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['subjectSpan'][0],\n",
    "                'rel_pos': triplet['subjectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['subjectSpan'], triplet['subject']),\n",
    "                'postag': get_postags_sequence(triplet['subjectSpan'], triplet['subject'], predicate=False),\n",
    "            }\n",
    "            _subject.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _subject['lemmas']),\n",
    "            })\n",
    "\n",
    "            _relation = {\n",
    "                'tokens': get_tokens(triplet['relation'], triplet['relationSpan']),\n",
    "                'lemmas': tag_lemma(triplet['relationSpan'], triplet['relation']),\n",
    "#                 'dist_to_rel': 0,\n",
    "                'rel_pos': triplet['relationSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['relationSpan'], triplet['relation']),\n",
    "                'postag': get_postags_sequence(triplet['relationSpan'], triplet['relation'], predicate=True),\n",
    "                'prep': get_prep_sequence(triplet['relation']),\n",
    "            }\n",
    "            _relation.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _relation['lemmas']),\n",
    "            })\n",
    "\n",
    "            _object = {\n",
    "                'tokens': get_tokens(triplet['object'], triplet['objectSpan']),\n",
    "                'lemmas': tag_lemma(triplet['objectSpan'], triplet['object']),\n",
    "                'dist_to_rel': triplet['relationSpan'][0] - triplet['objectSpan'][0],\n",
    "                'rel_pos': triplet['objectSpan'][0] / len(sentence['tokens']),\n",
    "                'ner': get_ner_occurrences(triplet['objectSpan'], triplet['object']),\n",
    "                'postag': get_postags_sequence(triplet['objectSpan'], triplet['object'], predicate=False),\n",
    "            }\n",
    "            _object.update({\n",
    "                'w2v': _embed(np.zeros((3, word2vec_vector_length)), _object['lemmas']),\n",
    "            })\n",
    "\n",
    "            _dependency_path = ' '.join(_lemmas_by_index(_build_dep_path(sentence['basicDependencies'],\n",
    "                                                                         sentence['tokens'],\n",
    "                                                                         triplet['subjectSpan'][0],\n",
    "                                                                         triplet['objectSpan'][-1]),\n",
    "                                                         sentence['tokens']))\n",
    "            subjects.append(_subject)\n",
    "            relations.append(_relation)\n",
    "            objects.append(_object)\n",
    "            dep_path.append(_dependency_path)\n",
    "\n",
    "        # return pd.DataFrame(result, columns=header)\n",
    "        return subjects, relations, objects\n",
    "\n",
    "    subjects, relations, objects = [], [], []\n",
    "    for sentence in document:\n",
    "        _subject, _relation, _object = _extract(sentence)\n",
    "        subjects += _subject\n",
    "        relations += _relation\n",
    "        objects += _object\n",
    "\n",
    "    return subjects, relations, objects\n",
    "\n",
    "\n",
    "def _mark_ner_object(row):\n",
    "    return row['relation'] + (row['DATE_obj'] == 1) * ' date' \\\n",
    "           + (row['LOCATION_obj'] == 1) * ' location'\n",
    "\n",
    "\n",
    "def _extract_features(document):\n",
    "    def _embed_arg(row):\n",
    "        result = []\n",
    "        result.append(_embed(np.zeros((3, word2vec_vector_length)), row['lemmas']))\n",
    "\n",
    "        return result\n",
    "\n",
    "    features = {}\n",
    "    features['subject'], features['relation'], features['object'] = _extract_plain_features(document)\n",
    "\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def remove_repetitions(annot):\n",
    "    for i in range(len(annot)):\n",
    "        for j in range(len(annot[i])):\n",
    "            annot[i][j]['openie'] = list(unique_everseen(annot[i][j]['openie']))\n",
    "    return annot\n",
    "\n",
    "\n",
    "class FeaturesProcessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pool = multiprocessing.Pool(processes=1)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        data: list of lists: [['wiki_id', 'data'], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        def mark_garbage(row):\n",
    "            \"\"\" Remove from the set some uninformative relations as well as triplets which do not contain \n",
    "                any noun in the object or subject\n",
    "            \"\"\"\n",
    "            \n",
    "            deprec_rels = {'in', \n",
    "#                            'is', 'was', \n",
    "                           'of', \"'s\", 'to', 'for', 'by', 'with', 'also', 'as of',\n",
    "#                            'had', \n",
    "                           'said', 'said in', 'felt', 'on', 'gave', 'saw', 'found', 'did',\n",
    "                           'at', 'as', 'e', 'as', 'de', 'mo', '’s', 'v', 'yr', 'al',\n",
    "                           \"'\", 'na', 'v.', \"d'\", 'et', 'mp', 'di', 'y',\n",
    "                           'ne', 'c.', 'be', 'ao', 'mi', 'im', 'h',\n",
    "                           'has', 'between', 'are', 'returned', 'began', 'became',\n",
    "                           'along', 'doors as', 'subsequently terrytoons in',\n",
    "                          }\n",
    "\n",
    "            def is_relation_deprecated():\n",
    "                return row._relation.isdigit() or row._relation in deprec_rels\n",
    "\n",
    "            def is_postag_undefined():\n",
    "                return np.all(row['subject']['postag'] == np.zeros((3, 18))) or np.all(\n",
    "                    row['object']['postag'] == np.zeros((3, 18))) or np.all(\n",
    "                    row['relation']['postag'] == np.zeros((3, 18)))\n",
    "\n",
    "            return is_relation_deprecated() or is_postag_undefined()\n",
    "\n",
    "        features = pd.concat(self.pool.map(_extract_features, data))\n",
    "        features['_subject'] = features['subject'].map(get_tokens)\n",
    "        features['_relation'] = features['relation'].map(get_tokens)\n",
    "        features['_object'] = features['object'].map(get_tokens)\n",
    "        features['garbage'] = features.apply(lambda row: mark_garbage(row), axis=1)\n",
    "        features = features[features.garbage == False]\n",
    "        features = features.drop(columns=[\"garbage\"])\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#DATA_PATH = 'data/corenlp_annotations_ner_pairs'  #'data/filtered_annotations'\n",
    "trex_path = 'trex_data'\n",
    "DATA_PATH = 'trex_corenlp_annotations'\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "! mkdir $RESULT_PATH \n",
    "result = []\n",
    "extr = FeaturesProcessor()\n",
    "\n",
    "def extract_matrix(row):\n",
    "    _matrix = np.concatenate(\n",
    "        [row['ner'], row['postag'], row['w2v'], np.array([[row['dist_to_rel'], row['rel_pos']]] * 3)], axis=1)\n",
    "    return _matrix\n",
    "\n",
    "def get_tokens(column):\n",
    "    return ' '.join(column['tokens'])\n",
    "\n",
    "for file in tqdm(glob(DATA_PATH + '/*.json')):\n",
    "    \n",
    "    tmp = json.load(open(file, \"r\"))\n",
    "    \n",
    "    if tmp.values():\n",
    "    \n",
    "        try:\n",
    "            result = extr(tmp.values())\n",
    "            result = result.drop_duplicates(['_subject', '_relation', '_object'])\n",
    "\n",
    "            result.to_pickle(file.replace(DATA_PATH, RESULT_PATH).replace('.json', '.pkl'))\n",
    "        except ValueError:\n",
    "            print('No examples in file:', file)\n",
    "        \n",
    "    else:\n",
    "        print('Unable to load examples from file:', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result._relation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "RESULT_PATH = 'data/processed_separately'\n",
    "data = []\n",
    "\n",
    "for file in tqdm(glob(RESULT_PATH + '/*.pkl')):\n",
    "    data.append(pd.read_pickle(file))\n",
    "    \n",
    "data = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_matrix(row, predicate=False):\n",
    "    _matrix = np.concatenate([row['ner'], row['postag']], axis=1)#.flatten()\n",
    "    if predicate:\n",
    "        _matrix = np.concatenate([_matrix, row['w2v'], [row['prep'], row['prep'], row['prep']]], axis=1)#.flatten()\n",
    "    return _matrix.flatten()\n",
    "\n",
    "def extract_one_matrix(row):\n",
    "    _matrix = np.concatenate([extract_matrix(row['subject']), \n",
    "                             extract_matrix(row['relation'], predicate=True), \n",
    "                             extract_matrix(row['object'])], axis=0)\n",
    "    return _matrix\n",
    "\n",
    "\n",
    "features = data.apply(extract_one_matrix, axis=1).values\n",
    "features = np.stack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_features_plain.pkl', 'wb') as f:\n",
    "    np.save(f, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = np.load('train_features_plain.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train simple kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=15, n_init=10)\n",
    "kmeans.fit(features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(kmeans, open('simple_kmeans.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'] = kmeans.predict(features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"_subject\", \"_relation\", \"_object\", \"class\"]].to_csv(\"trex_data_classified.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 3\n",
    "data[data['class'] == number]._relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data['class'] == 14]._relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['class'] == 3].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data['class'] == 3]._relation.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "318px",
    "left": "1484px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

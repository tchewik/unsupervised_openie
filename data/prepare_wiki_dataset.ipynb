{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset of wiki IT-related articles with stanford corenlp annotation.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<ol>\n",
    "  <li>Load IT-related articles from the wikipedia dump;</li>\n",
    "    <ol>\n",
    "      <li>Make the list of IT categories;</li>\n",
    "      <li>Collect the corresponding pages.</li>\n",
    "    </ol>\n",
    "  <li>Process them with corenlp;</li>\n",
    "    <ol>\n",
    "      <li>Run the container: <code>docker run --restart=unless-stopped -ti -p 9000:9000 -d tchewik/corenlp</code>;</li>\n",
    "      <li>Process the collected file;</li>\n",
    "      <li>Save the result into chunks;</li>\n",
    "      <li>(Optional) filter the triplets by named entities occurrence.</li>\n",
    "    </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IT-related articles from the wikipedia dump\n",
    "### Make the list of useful categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_categories = ['Programming_languages', 'Computer_science', 'Information_technology',\n",
    "                   'Algorithms', 'Formal_systems', 'Areas_of_computer_science', 'Software_development']\n",
    "\n",
    "params = {\n",
    "    'categories': base_categories,  # process the categories separately if it doesn't respond!\n",
    "    'depth': 2,\n",
    "    'negcats': ['Information_technology_by_country'],\n",
    "    'ns[14]': 1,  # namespace=14 is for categories, 0 for pages\n",
    "    'language': 'en',\n",
    "    'project': 'wikipedia',\n",
    "    'format': 'json',\n",
    "    'doit': 'Do it!'}\n",
    "\n",
    "r = requests.get('https://petscan.wmflabs.org/', params=params)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [word.strip() for word in open('stopwords.filter', 'r').readlines()] # should not appear in the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "def process_title(title):\n",
    "    for word in stopwords:\n",
    "        if word in title.lower():\n",
    "            return None\n",
    "    return title\n",
    "\n",
    "for item in data['*'][0]['a']['*']:\n",
    "    title = process_title(item.get('title'))\n",
    "    if title:\n",
    "        categories.append(title)\n",
    "        \n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.filter', 'w') as f:\n",
    "    categories.sort()\n",
    "    for category in categories:\n",
    "        f.write(category + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sigh, update the stopword list and repeat infinitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the corresponding pages into ``it_wiki_articles.json``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'enwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "wget http://download.wikimedia.org/enwiki/latest/$1\n",
    "git clone https://github.com/attardi/wikiextractor.git\n",
    "\n",
    "python wikiextractor/WikiExtractor.py $1 \\\n",
    "       --json \\\n",
    "       --processes 2 \\\n",
    "       --output extracted \\\n",
    "       --bytes 1M \\\n",
    "       --compress \\\n",
    "       --filter_category categories.filter \\\n",
    "       --min_text_length 0 \\\n",
    "       --filter_disambig_pages\n",
    "       \n",
    "find extracted -name '*bz2' -exec bzip2 -dkc {} \\; > it_wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "titles = []\n",
    "\n",
    "with open('it_wiki_articles.json', 'w') as fo:\n",
    "    fo.write('[')\n",
    "    with open('it_wiki_articles', 'r') as f:\n",
    "        file = f.readlines()\n",
    "        for i, line in enumerate(file):\n",
    "            if i < len(file) - 1:\n",
    "                fo.write(line[:-1] + ',')\n",
    "            else:\n",
    "                fo.write(line + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_json('it_wiki_articles.json').sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    data = s.get_data().replace('\\n\\n', '\\n')\\\n",
    "                        .replace('Section::::', '')\\\n",
    "                        .replace('BULLET::::', '')\\\n",
    "                        .replace('(Archive)', '')\\\n",
    "                        .replace('( )', '')\n",
    "    data = re.sub(r'style=\".*\"', '', data)\n",
    "    return data.strip()\n",
    "\n",
    "file.text = file.text.map(strip_tags)\n",
    "#file = file[file.text.map(len) < 60000]\n",
    "file.to_json('it_wiki_articles.json', orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file[file.text.map(len) < 84000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for line in file:\n",
    "    flag = False\n",
    "    for word in stopwords:\n",
    "        if word in line['title'].lower():\n",
    "            flag = True\n",
    "            continue\n",
    "    if not flag:\n",
    "        result.append(line)\n",
    "        \n",
    "print(len(result), 'articles were loaded.')\n",
    "json.dump(result, open('it_wiki_articles.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del file, result\n",
    "! rm -r wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the articles using corenlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already up-to-date: pycorenlp in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pycorenlp) (2.20.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests->pycorenlp) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests->pycorenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests->pycorenlp) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests->pycorenlp) (2018.11.29)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "container = 'http://hostname:9000'\n",
    "nlp = StanfordCoreNLP(container)\n",
    "nlp_properties = {\n",
    "  'annotators': 'tokenize,ssplit,tokenize,ssplit,pos,depparse,natlog,openie,ner',\n",
    "  'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘corenlp_annotations_new’: File exists\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a54cbd1edc043d587d19cc254bef33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13145), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "file = json.load(open('it_wiki_articles.json', 'r'))\n",
    "\n",
    "! mkdir corenlp_annotations\n",
    "\n",
    "new_file = []\n",
    "filename_base = 'corenlp_annotations/it_wiki'\n",
    "counter_0, counter_1 = 1, 2\n",
    "save_every = 100\n",
    "\n",
    "for line in tqdm(file[136:]):\n",
    "    id, text, name, url = line\n",
    "    if id not in broken_ids:\n",
    "        try:\n",
    "            result = nlp.annotate(text, properties=nlp_properties)\n",
    "        except Exception:\n",
    "            import time\n",
    "            time.sleep(10)  # wait until container surely restarts after OOM\n",
    "            result = nlp.annotate(text, properties=nlp_properties)\n",
    "        \n",
    "        if type(result) == str:\n",
    "            broken_ids.append(id)  # in case of an internal error, corenlp returns a string\n",
    "            continue\n",
    "            \n",
    "        result['id'] = id\n",
    "        new_file.append(result)\n",
    "\n",
    "        if not counter_0 % save_every:\n",
    "            json.dump(new_file, open(f'{filename_base}_part_{counter_1}.json', 'w'))\n",
    "            counter_1 += 1\n",
    "            new_file = []\n",
    "\n",
    "        counter_0 += 1\n",
    "\n",
    "json.dump(new_file, open(filename_base + f'_part_{counter_1}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of pages, annotation of which caused errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = file[file[0] == remove_ids[0]][1].values[0]\n",
    "# txt = ...\n",
    "result = nlp.annotate(txt, properties=nlp_properties)\n",
    "result['id'] = remove_ids[0]\n",
    "result['text'] = txt\n",
    "new_file.append(result)\n",
    "# then save new_file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file, new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the triplets by named entities occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity in sentence['entitymentions']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "                continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "def filter_ner_both(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity1 in sentence['entitymentions']:\n",
    "            if entity1['text'] in triplet['subject']:\n",
    "                for entity2 in sentence['entitymentions']:\n",
    "                    if entity2['text'] in triplet['object']:\n",
    "                        openie.append(triplet)\n",
    "                        counter += 1\n",
    "            elif entity1['text'] in triplet['object']:\n",
    "                for entity2 in sentence['entitymentions']:\n",
    "                    if entity2['text'] in triplet['subject']:\n",
    "                        openie.append(triplet)\n",
    "                        counter += 1\n",
    "\n",
    "    return openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['openie'] = filter_ner_both(sentence)\n",
    "        if new_sentence['openie']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31051a880c04977b408efb56dde2343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=132), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1555299 triplets were extracted applying NER filtering\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'corenlp_annotations_new'\n",
    "result_path = 'corenlp_annotations_ner_pairs'\n",
    "! mkdir $result_path\n",
    "\n",
    "for file in tqdm(glob(data_path + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    tmp.to_json(file.replace(data_path, result_path), orient='values')\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "! echo contains only triplets with named entities in object and subject \\($counter triplets\\) > $result_path/readme.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

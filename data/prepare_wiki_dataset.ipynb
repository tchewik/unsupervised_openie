{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data/prepare_wiki_dataset.ipynb\n",
    "data/request_categories_list.py\n",
    "data/base_categories.txt\n",
    "data/collect_pages.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset of wiki IT-related articles with stanford corenlp annotation.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<ol>\n",
    "  <li>Load IT-related articles from the wikipedia dump;</li>\n",
    "    <ol>\n",
    "      <li>Make the list of IT categories;</li>\n",
    "      <li>Collect the corresponding pages.</li>\n",
    "    </ol>\n",
    "  <li>Process them with corenlp;</li>\n",
    "    <ol>\n",
    "      <li>Run the container: <code>docker run --restart=unless-stopped -ti -p 9000:9000 -d tchewik/corenlp</code>;</li>\n",
    "      <li>Process the collected file;</li>\n",
    "      <li>Save the result into chunks;</li>\n",
    "      <li>(Optional) filter the triplets by named entities occurrence.</li>\n",
    "    </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IT-related articles from the wikipedia dump\n",
    "### 1. Make the list of useful categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python request_categories_list.py -i base_categories.txt -o categories.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect the corresponding pages into ``it_wiki_articles.json``\n",
    "\n",
    "\n",
    "```sh\n",
    "sh collect_pages.sh \"wiki dump name\" \"extended list of categories\" \"output directory\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! sh collect_pages.sh enwiki-latest-pages-articles.xml.bz2 categories.txt it_wiki_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python path2json.py \"directory with collected wiki pages\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python path2json.py -i it_wiki_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process the articles using corenlp \n",
    "\n",
    "```sh\n",
    "python corenlp_parsing.py -h \"hostname\" -p 9000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME = ''\n",
    "PORT = 9001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python corenlp_parsing.py -i \"it_wiki_articles.json\" -n $HOSTNAME -p $PORT -o \"corenlp_annotations/it_wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(f'https://{HOSTNAME}:9001')\n",
    "nlp_properties = {\n",
    "    'annotators': 'tokenize,ssplit,tokenize,ssplit,pos,depparse,natlog,openie,ner',\n",
    "    'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp.annotate(\"I walk through the valley of the shadows of death.\", \n",
    "                      properties=nlp_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of pages, annotation of which caused errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = file[file[0] == remove_ids[0]][1].values[0]\n",
    "# txt = ...\n",
    "result = nlp.annotate(txt, properties=nlp_properties)\n",
    "result['id'] = remove_ids[0]\n",
    "result['text'] = txt\n",
    "new_file.append(result)\n",
    "# then save new_file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file, new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the triplets by named entities occurrence & length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_BY_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity in sentence['entitymentions']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "                continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "def filter_ner_both(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if triplet['subjectSpan'][1] - triplet['subjectSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['relationSpan'][1] - triplet['relationSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['objectSpan'][1] - triplet['objectSpan'][0] <= FILTER_BY_LENGTH:\n",
    "            for entity1 in sentence['entitymentions']:\n",
    "                if entity1['text'] in triplet['subject']:\n",
    "                    for entity2 in sentence['entitymentions']:\n",
    "                        if entity2['text'] in triplet['object']:\n",
    "                            if not triplet in openie:\n",
    "                                openie.append(triplet)\n",
    "                                counter += 1\n",
    "                elif entity1['text'] in triplet['object']:\n",
    "                    for entity2 in sentence['entitymentions']:\n",
    "                        if entity2['text'] in triplet['subject']:\n",
    "                            if not triplet in openie:\n",
    "                                openie.append(triplet)\n",
    "                                counter += 1\n",
    "\n",
    "    return openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['openie'] = filter_ner_both(sentence)\n",
    "        if new_sentence['openie']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'corenlp_annotations'\n",
    "result_path = 'corenlp_annotations_ner_pairs'\n",
    "! mkdir $result_path\n",
    "\n",
    "for file in tqdm(glob(data_path + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    tmp.to_json(file.replace(data_path, result_path), orient='values')\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "! echo contains only triplets with named entities in object and subject \\($counter triplets\\) > $result_path/readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave only named entities on both ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_BY_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity in sentence['entitymentions']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "                continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "def filter_ner_both(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if triplet['subjectSpan'][1] - triplet['subjectSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['relationSpan'][1] - triplet['relationSpan'][0] <= FILTER_BY_LENGTH + 10 \\\n",
    "            and triplet['objectSpan'][1] - triplet['objectSpan'][0] <= FILTER_BY_LENGTH:\n",
    "            \n",
    "            entitymentions = ' '.join([entity['text'] for entity in sentence['entitymentions']])\n",
    "            if triplet['subject'] in ' '.join(entitymentions) and triplet['object'] in entitymentions:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "\n",
    "    return openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['openie'] = filter_ner_both(sentence)\n",
    "        if new_sentence['openie']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'corenlp_annotations'\n",
    "result_path = 'corenlp_annotations_only_ner'\n",
    "! mkdir $result_path\n",
    "\n",
    "for file in tqdm(glob(data_path + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    tmp.to_json(file.replace(data_path, result_path), orient='values')\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "! echo contains only triplets with named entities in object and subject \\($counter triplets\\) > $result_path/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"sentences\"].iloc[4][1][\"openie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"sentences\"].iloc[4][1][\"entitymentions\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

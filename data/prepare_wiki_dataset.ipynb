{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset of wiki IT-related articles with stanford corenlp annotation.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<ol>\n",
    "  <li>Load IT-related articles from the wikipedia dump;</li>\n",
    "    <ol>\n",
    "      <li>Make the list of IT categories;</li>\n",
    "      <li>Collect the corresponding pages.</li>\n",
    "    </ol>\n",
    "  <li>Process them with corenlp;</li>\n",
    "    <ol>\n",
    "      <li>Run the container: <code>docker run --restart=unless-stopped -ti -p 9000:9000 -d tchewik/corenlp</code>;</li>\n",
    "      <li>Process the collected file;</li>\n",
    "      <li>Save the result into chunks;</li>\n",
    "      <li>(Optional) filter the triplets by named entities occurrence.</li>\n",
    "    </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IT-related articles from the wikipedia dump\n",
    "### Make the list of useful categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make separate lists at first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_categories = ['Programming_languages', 'Computer_science', 'Information_technology',\n",
    "                   'Algorithms', 'Formal_systems', 'Areas_of_computer_science', \n",
    "                   'Software_development', 'Software_engineering', 'Windows_software', \n",
    "                   'Linux', 'MacOS', 'Data_structures', 'Data_analysis', 'Big_data', 'Machine_learning', \n",
    "                   'Google', 'Microsoft', 'IBM', 'Silicon_Valley',\n",
    "                   'Natural_language_processing', 'Computational_linguistics', 'Parsing']\n",
    "\n",
    "params = {\n",
    "    'categories': base_categories[-1],  # process the categories separately\n",
    "    'depth': 3,\n",
    "    'ns[14]': 1,  # namespace=14 is for categories, 0 for pages\n",
    "    'language': 'en',\n",
    "    'project': 'wikipedia',\n",
    "    'format': 'json',\n",
    "    'doit': 'Do it!'}\n",
    "\n",
    "r = requests.get('https://petscan.wmflabs.org/', params=params)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of subcategories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['*'][0]['a']['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [word.strip() for word in open('stopwords.filter', 'r').readlines()] # should not appear in the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "def process_title(title):\n",
    "    for word in stopwords:\n",
    "        if word in title.lower():\n",
    "            return None\n",
    "    return title\n",
    "\n",
    "for item in data['*'][0]['a']['*']:\n",
    "    title = process_title(item.get('title'))\n",
    "    if title:\n",
    "        categories.append(title)\n",
    "        \n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of subcategories after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parsing.categories', 'w') as f:\n",
    "    categories.sort()\n",
    "    for category in categories:\n",
    "        f.write(category + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the contribution of each base category in the final list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('areas_cs.categories', 'r') as f:\n",
    "    all_the_categories = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "print(len(all_the_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp_science.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('it.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('engineering.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('algorithms.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comptat_science.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('languages.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('formal_systems.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('winsoft.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('linux.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('macos.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ai.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('da.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('google.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('microsoft.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ibm.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('silicon_valley.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cl.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parsing.categories', 'r') as f:\n",
    "    temp = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print('contribution:', len(list(set(all_the_categories + temp))) - len(all_the_categories))\n",
    "all_the_categories = list(set(all_the_categories + temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_the_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.filter', 'w') as f:\n",
    "    all_the_categories.sort()\n",
    "    for category in all_the_categories:\n",
    "        f.write(category + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sigh, update the stopword list and repeat infinitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the corresponding pages into ``it_wiki_articles.json``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'enwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "#wget http://download.wikimedia.org/enwiki/latest/$1\n",
    "git clone https://github.com/attardi/wikiextractor.git\n",
    "\n",
    "python wikiextractor/WikiExtractor.py $1 \\\n",
    "       --json \\\n",
    "       --processes 4 \\\n",
    "       --output extracted \\\n",
    "       --bytes 4M \\\n",
    "       --compress \\\n",
    "       --filter_category categories.filter \\\n",
    "       --min_text_length 0\n",
    "       \n",
    "find extracted -name '*bz2' -exec bzip2 -dkc {} \\; > it_wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "titles = []\n",
    "\n",
    "with open('it_wiki_articles.json', 'w') as fo:\n",
    "    fo.write('[')\n",
    "    with open('it_wiki_articles', 'r') as f:\n",
    "        file = f.readlines()\n",
    "        for i, line in enumerate(file):\n",
    "            if i < len(file) - 1:\n",
    "                fo.write(line[:-1] + ',')\n",
    "            else:\n",
    "                fo.write(line + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_json('it_wiki_articles.json').sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    data = s.get_data().replace('\\n\\n', '\\n')\\\n",
    "                        .replace('Section::::', '')\\\n",
    "                        .replace('BULLET::::', '')\\\n",
    "                        .replace('(Archive)', '')\\\n",
    "                        .replace('( )', '')\n",
    "    data = re.sub(r'style=\".*\"', '', data)\n",
    "    return data.strip()\n",
    "\n",
    "file.text = file.text.map(strip_tags)\n",
    "#file = file[file.text.map(len) < 60000]\n",
    "file.to_json('it_wiki_articles.json', orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file[file.text.map(len) < 84000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in file.head(n=2).iterrows():\n",
    "     print(index, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for index, line in file.iterrows():\n",
    "    flag = False\n",
    "    for word in stopwords:\n",
    "        if word in line['title'].lower():\n",
    "            flag = True\n",
    "            continue\n",
    "    if not flag:\n",
    "        result.append(line.values.tolist())\n",
    "        \n",
    "print(len(result), 'articles were loaded.')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open('it_wiki_articles.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del file, result\n",
    "! rm -r wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the articles using corenlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "container = 'http://' + hostname + ':9000'\n",
    "nlp = StanfordCoreNLP(container)\n",
    "nlp_properties = {\n",
    "  'annotators': 'tokenize,ssplit,tokenize,ssplit,pos,depparse,natlog,openie,ner',\n",
    "  'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘corenlp_annotations’: File exists\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30314618785f40579c4001c1750ab881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9033), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "file = json.load(open('it_wiki_articles.json', 'r'))\n",
    "\n",
    "! mkdir corenlp_annotations\n",
    "\n",
    "new_file = []\n",
    "filename_base = 'corenlp_annotations/it_wiki'\n",
    "counter_0, counter_1 = 0, 0\n",
    "save_every = 100\n",
    "\n",
    "def remove_title(text, title):\n",
    "    return text[len(title)+1:]\n",
    "\n",
    "for line in tqdm(file):\n",
    "    id, text, name, url = line\n",
    "    if id not in broken_ids:\n",
    "        try:\n",
    "            result = nlp.annotate(remove_title(text, name), properties=nlp_properties)\n",
    "        except Exception:\n",
    "            import time\n",
    "            time.sleep(10)  # wait until container surely restarts after OOM\n",
    "            result = nlp.annotate(remove_title(text, name), properties=nlp_properties)\n",
    "        \n",
    "        if type(result) == str:\n",
    "            broken_ids.append(id)  # in case of an internal error, corenlp returns a string\n",
    "            continue\n",
    "            \n",
    "        result['id'] = id\n",
    "        new_file.append(result)\n",
    "\n",
    "        if not counter_0 % save_every:\n",
    "            json.dump(new_file, open(f'{filename_base}_part_{counter_1}.json', 'w'))\n",
    "            counter_1 += 1\n",
    "            new_file = []\n",
    "\n",
    "        counter_0 += 1\n",
    "\n",
    "json.dump(new_file, open(filename_base + f'_part_{counter_1}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of pages, annotation of which caused errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = file[file[0] == remove_ids[0]][1].values[0]\n",
    "# txt = ...\n",
    "result = nlp.annotate(txt, properties=nlp_properties)\n",
    "result['id'] = remove_ids[0]\n",
    "result['text'] = txt\n",
    "new_file.append(result)\n",
    "# then save new_file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file, new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the triplets by named entities occurrence & length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_BY_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity in sentence['entitymentions']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "                continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "def filter_ner_both(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if triplet['subjectSpan'][1] - triplet['subjectSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['relationSpan'][1] - triplet['relationSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['objectSpan'][1] - triplet['objectSpan'][0] <= FILTER_BY_LENGTH:\n",
    "            for entity1 in sentence['entitymentions']:\n",
    "                if entity1['text'] in triplet['subject']:\n",
    "                    for entity2 in sentence['entitymentions']:\n",
    "                        if entity2['text'] in triplet['object']:\n",
    "                            if not triplet in openie:\n",
    "                                openie.append(triplet)\n",
    "                                counter += 1\n",
    "                elif entity1['text'] in triplet['object']:\n",
    "                    for entity2 in sentence['entitymentions']:\n",
    "                        if entity2['text'] in triplet['subject']:\n",
    "                            if not triplet in openie:\n",
    "                                openie.append(triplet)\n",
    "                                counter += 1\n",
    "\n",
    "    return openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['openie'] = filter_ner_both(sentence)\n",
    "        if new_sentence['openie']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘corenlp_annotations_ner_pairs’: File exists\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182d0a1c96ae4ce8a03bcfd5c957be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75335 triplets were extracted applying NER filtering\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'corenlp_annotations'\n",
    "result_path = 'corenlp_annotations_ner_pairs'\n",
    "! mkdir $result_path\n",
    "\n",
    "for file in tqdm(glob(data_path + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    tmp.to_json(file.replace(data_path, result_path), orient='values')\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "! echo contains only triplets with named entities in object and subject \\($counter triplets\\) > $result_path/readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave only named entities on both ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_BY_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        for entity in sentence['entitymentions']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "                continue\n",
    "                \n",
    "    return openie\n",
    "\n",
    "def filter_ner_both(sentence):\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for triplet in sentence['openie']:\n",
    "        if triplet['subjectSpan'][1] - triplet['subjectSpan'][0] <= FILTER_BY_LENGTH \\\n",
    "            and triplet['relationSpan'][1] - triplet['relationSpan'][0] <= FILTER_BY_LENGTH + 10 \\\n",
    "            and triplet['objectSpan'][1] - triplet['objectSpan'][0] <= FILTER_BY_LENGTH:\n",
    "            \n",
    "            entitymentions = ' '.join([entity['text'] for entity in sentence['entitymentions']])\n",
    "            if triplet['subject'] in ' '.join(entitymentions) and triplet['object'] in entitymentions:\n",
    "                openie.append(triplet)\n",
    "                counter += 1\n",
    "\n",
    "    return openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['openie'] = filter_ner_both(sentence)\n",
    "        if new_sentence['openie']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘corenlp_annotations_only_ner’: File exists\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9accf7acb4f44aee8a8c025642ef661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45949 triplets were extracted applying NER filtering\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'corenlp_annotations'\n",
    "result_path = 'corenlp_annotations_only_ner'\n",
    "! mkdir $result_path\n",
    "\n",
    "for file in tqdm(glob(data_path + '/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    tmp.to_json(file.replace(data_path, result_path), orient='values')\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "! echo contains only triplets with named entities in object and subject \\($counter triplets\\) > $result_path/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject': 'He',\n",
       "  'subjectSpan': [0, 1],\n",
       "  'relation': 'is professor at',\n",
       "  'relationSpan': [1, 4],\n",
       "  'object': 'University of California',\n",
       "  'objectSpan': [11, 14]},\n",
       " {'subject': 'He',\n",
       "  'subjectSpan': [0, 1],\n",
       "  'relation': 'is professor at',\n",
       "  'relationSpan': [1, 4],\n",
       "  'object': 'University',\n",
       "  'objectSpan': [11, 12]},\n",
       " {'subject': 'He',\n",
       "  'subjectSpan': [0, 1],\n",
       "  'relation': 'is professor at',\n",
       "  'relationSpan': [1, 4],\n",
       "  'object': 'Berkeley',\n",
       "  'objectSpan': [15, 16]},\n",
       " {'subject': 'He',\n",
       "  'subjectSpan': [0, 1],\n",
       "  'relation': 'is',\n",
       "  'relationSpan': [1, 2],\n",
       "  'object': 'professor',\n",
       "  'objectSpan': [3, 4]}]"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"sentences\"].iloc[4][1][\"openie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'docTokenBegin': 19,\n",
       "  'docTokenEnd': 20,\n",
       "  'tokenBegin': 3,\n",
       "  'tokenEnd': 4,\n",
       "  'text': 'professor',\n",
       "  'characterOffsetBegin': 90,\n",
       "  'characterOffsetEnd': 99,\n",
       "  'ner': 'TITLE'},\n",
       " {'docTokenBegin': 27,\n",
       "  'docTokenEnd': 32,\n",
       "  'tokenBegin': 11,\n",
       "  'tokenEnd': 16,\n",
       "  'text': 'University of California, Berkeley',\n",
       "  'characterOffsetBegin': 141,\n",
       "  'characterOffsetEnd': 175,\n",
       "  'ner': 'ORGANIZATION'},\n",
       " {'docTokenBegin': 16,\n",
       "  'docTokenEnd': 17,\n",
       "  'tokenBegin': 0,\n",
       "  'tokenEnd': 1,\n",
       "  'text': 'He',\n",
       "  'characterOffsetBegin': 82,\n",
       "  'characterOffsetEnd': 84,\n",
       "  'ner': 'PERSON'}]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[\"sentences\"].iloc[4][1][\"entitymentions\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

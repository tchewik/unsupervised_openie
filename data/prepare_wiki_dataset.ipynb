{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset of wiki IT-related articles with stanford corenlp annotation.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<ol>\n",
    "  <li>Load IT-related articles from the wikipedia dump;</li>\n",
    "    <ol>\n",
    "      <li>Make the list of IT categories;</li>\n",
    "      <li>Collect the corresponding pages.</li>\n",
    "    </ol>\n",
    "  <li>Process them with corenlp;</li>\n",
    "    <ol>\n",
    "      <li>Run the container: <code>docker run --rm -ti -p 9000:9000 -d tchewik/corenlp</code>;</li>\n",
    "      <li>Process the collected file;</li>\n",
    "      <li>Save the result into chunks;</li>\n",
    "      <li>(Optional) filter the triplets by named entities occurrence.</li>\n",
    "    </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IT-related articles from the wikipedia dump\n",
    "### Make the list of useful categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_categories = ['Programming_languages', 'Computer_science', 'Information_technology',\n",
    "                   'Algorithms', 'Formal_systems', 'Areas_of_computer_science', 'Software_development']\n",
    "\n",
    "params = {\n",
    "    'categories': base_categories,  # process the categories separately if it doesn't respond!\n",
    "    'depth': 2,\n",
    "    'negcats': ['Information_technology_by_country'],\n",
    "    'ns[14]': 1,  # namespace=14 is for categories, 0 for pages\n",
    "    'language': 'en',\n",
    "    'project': 'wikipedia',\n",
    "    'format': 'json',\n",
    "    'doit': 'Do it!'}\n",
    "\n",
    "r = requests.get('https://petscan.wmflabs.org/', params=params)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [word.strip() for word in open('stopwords.filter', 'r').readlines()] # should not appear in the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "\n",
    "def process_title(title):\n",
    "    for word in stopwords:\n",
    "        if word in title.lower():\n",
    "            return None\n",
    "    return title\n",
    "\n",
    "for item in data['*'][0]['a']['*']:\n",
    "    title = process_title(item.get('title'))\n",
    "    if title:\n",
    "        categories.append(title)\n",
    "        \n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/categories.filter', 'w') as f:\n",
    "    categories.sort()\n",
    "    for category in categories:\n",
    "        f.write(category + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sigh, update the stopword list and repeat infinitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the corresponding pages into ``it_wiki_articles.json``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'enwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "wget http://download.wikimedia.org/enwiki/latest/$1\n",
    "git clone https://github.com/attardi/wikiextractor.git\n",
    "\n",
    "python wikiextractor/WikiExtractor.py $1 \\\n",
    "       --json \\\n",
    "       --processes 2 \\\n",
    "       --output extracted_2782 \\\n",
    "       --bytes 1M \\\n",
    "       --compress \\\n",
    "       --filter_category categories.filter \\\n",
    "       --links \\\n",
    "       --sections \\\n",
    "       --lists \\\n",
    "       --keep_tables \\\n",
    "       --min_text_length 0 \\\n",
    "       --filter_disambig_pages\n",
    "       \n",
    "find extracted_2782 -name '*bz2' -exec bzip2 -dkc {} \\; > it_wiki_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "titles = []\n",
    "\n",
    "with open('it_wiki_articles.json', 'w') as fo:\n",
    "    fo.write('[')\n",
    "    with open('it_wiki_articles', 'r') as f:\n",
    "        file = f.readlines()\n",
    "        for i, line in enumerate(file):\n",
    "            if i < len(file) - 1:\n",
    "                fo.write(line[:-1] + ',')\n",
    "            else:\n",
    "                fo.write(line + ']')\n",
    "                \n",
    "file = json.load(open('it_wiki_articles.json', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for line in file:\n",
    "    flag = False\n",
    "    for word in stopwords:\n",
    "        if word in line['title'].lower():\n",
    "            flag = True\n",
    "            continue\n",
    "    if not flag:\n",
    "        result.append(line)\n",
    "        \n",
    "print(len(result), 'articles were loaded.')\n",
    "json.dump(result, open('it_wiki_articles.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file, result\n",
    "! rm -r wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the articles using corenlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U pip\n",
    "pip install pycorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "container = # '0.0.0.0:9000'\n",
    "nlp = StanfordCoreNLP(container)\n",
    "nlp_properties = {\n",
    "  'annotators': 'tokenize,ssplit,tokenize,ssplit,pos,depparse,natlog,openie,ner',\n",
    "  'outputFormat': 'json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = pd.read_json('it_wiki_articles.json').sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    data = s.get_data().replace('\\n\\n', '\\n')\\\n",
    "                        .replace('Section::::', '')\\\n",
    "                        .replace('BULLET::::', '')\\\n",
    "                        .replace('(Archive)', '')\n",
    "    data = re.sub(r'style=\".*\"', '', data)\n",
    "    return data\n",
    "\n",
    "file.text = file.text.map(strip_tags)\n",
    "file = file[file.text.map(len) < 60000]\n",
    "file.to_json('it_wiki_articles.json', orient='values')\n",
    "del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "file = json.load(open('it_wiki_articles.json', 'r'))\n",
    "broken_ids = []\n",
    "! mkdir corenlp_annotations\n",
    "\n",
    "new_file = []\n",
    "filename_base = 'corenlp_annotations/it_wiki'\n",
    "counter_0, counter_1 = 1, 0\n",
    "save_every = 300\n",
    "\n",
    "for line in tqdm(file):\n",
    "    id, text, name, url = line\n",
    "    if id not in remove_ids:\n",
    "        result = nlp.annotate(strip_tags(text), properties=nlp_properties)\n",
    "        if type(result) == str:\n",
    "            broken_ids.append(id)  # in case of an error, corenlp returns a string\n",
    "            continue\n",
    "            \n",
    "        result['id'] = id\n",
    "        result['text'] = text\n",
    "        new_file.append(result)\n",
    "\n",
    "        if not counter_0 % save_every:\n",
    "            json.dump(new_file, open(f'{filename_base}_part_{counter_1}.json', 'w'))\n",
    "            counter_1 += 1\n",
    "            new_file = []\n",
    "\n",
    "        counter_0 += 1\n",
    "\n",
    "json.dump(new_file, open(filename_base + f'_part_{counter_1}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of pages, annotation of which caused errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = file[file[0] == remove_ids[0]][1].values[0]\n",
    "# txt = ...\n",
    "result = nlp.annotate(txt, properties=nlp_properties)\n",
    "result['id'] = remove_ids[0]\n",
    "result['text'] = txt\n",
    "new_file.append(result)\n",
    "# then save new_file somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file, new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the triplets by named entities occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "def filter_ner(sentence):\n",
    "    entitymentions = []\n",
    "    openie = []\n",
    "    global counter\n",
    "    \n",
    "    for entity in sentence['entitymentions']:\n",
    "        for triplet in sentence['openie']:\n",
    "            if entity['text'] in [triplet['subject'], triplet['object']]:\n",
    "                openie.append(triplet)\n",
    "                entitymentions.append(entity)\n",
    "                counter += 1\n",
    "                \n",
    "    return entitymentions, openie\n",
    "\n",
    "def process_page(page):\n",
    "    sentences = []\n",
    "    for sentence in page:\n",
    "        new_sentence = sentence\n",
    "        new_sentence['entitymentions'], new_sentence['openie'] = filter_ner(sentence)\n",
    "        if new_sentence['entitymentions']:\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "for file in tqdm(glob('corenlp_annotations/*.json')):\n",
    "    tmp = pd.read_json(file)\n",
    "    tmp.sentences = tmp.sentences.map(process_page)\n",
    "    result.append(tmp)\n",
    "    \n",
    "print(counter, 'triplets were extracted applying NER filtering')\n",
    "result = pd.concat(result).sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "directory = 'filtered_annotations'\n",
    "! mkdir $directory\n",
    "result = np.array_split(tmp, 4)\n",
    "for i in range(len(result)):\n",
    "    result[i].to_json(f'{directory}/it_wiki_annots_filtered_part_{i}.json', orient='records')\n",
    "! echo contains only triplets with named entities \\($counter triplets\\) > filtered_annotations/readme.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
